---
title: "My R CookBook"
description: |
  My simple working R Code cookbook.
author: "Matthew Son"
output: 
  distill::distill_article:
    toc: yes
    toc_float: yes
    # theme: yeti
    number_sections: yes
    toc_depth: 3
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```

# MACHINE LEARNING

## H2O

### Init and Shutdown

```{r}
h2o.init() # initialize, use all core and 25% of memory
h2o.shutdown() # close connection

h2o.init(max_mem_size = '800G')

```



### h2o.jar location installed by R
```{r}
jar_path = system.file('java', 'h2o.jar', package='h2o')
```


Copy to current working directory
```{r}
file.copy(jar_path, './h2o.jar')
```




### Run java instances
```{r}
require(stringr)
system(str_glue('java -Xmx3g -jar h2o.jar -name n1 -nthreads 3  -port 54321'), intern = F, wait =F)
system(str_glue('java -Xmx3g -jar h2o.jar -name n2 -nthreads 3  -port 54322'), intern = F, wait =F)


library(h2o)
h2o.init(startH2O = F)
h2o.shutdown(prompt = F)

h2o.init()
```






## Supervised ML: caret

`caret` package

```{r message=F}
library(caret)
library(mlbench)
data(Sonar)
```

## train-test split

`createDataPartition()` function, p=0.7 stands for 70% of train set. The output is the rownames(index number).

To create an index with 70% of the breast_cancer_data to create a training set and stratify the partitions by the response variable diagnosis:

```{r}
index <- createDataPartition(breast_cancer_data$diagnosis, p = 0.7, list = FALSE)
```

## train()

### 1 preProcess

#### missing values

Generate missing values for demo

```{r}
set.seed(42)
mtcars[sample(1:nrow(mtcars), 10), 'hp'] = NA
Y = mtcars$mpg
X = mtcars[,2:4]
```

-   If the data is missing at random, `medianImpute` is good option

-   It missing is non-random, `knnImpute` should be considered

    -   it imputes with observations that are similar

```{r}
# medianImpute
median_model <- train(
  x = X, 
  y = Y,
  method = "glm",
  preProcess = "medianImpute" # knnImpute
)

```

#### center and scale

```{r}
model_prep = train(
  x = X,
  y = Y,
  method = 'glm',
  preProcess = c('medianImpute', 'center','scale') # combination of preprocessing
)
print(min(model_prep$results$RMSE))
```

#### low-information variable

Low-variance variables don't contain much information

They cause computational problems and bugs, and makes fitting longer - good reason to drop them - `preProcess = zv` to drop constant columns - `preProcess = nzv` to drop nearly constant columns - Also `caret` provides function `nearZeroVar()` to detect those columns - nearZeroVar() takes in data x, then looks at the ratio of the most common value to the second most common value, freqCut, and the percentage of distinct values out of the number of total samples, uniqueCut.

Example: constant column

```{r}
X$bad = 1
train(X,Y,method='glm', preProcess=c('medianImpute','center','scale','pca')) # PCA goes wrong because of constant variable
train(X,Y,method='glm', preProcess=c('zv','medianImpute','center','scale','pca')) # now it works fine
```

```{r}
nearZeroVar(X, names=T, freqCut = 2, uniqueCut = 20) # gives the names of columns
```

```{r}
?setdiff
```

#### PCA

PCA combines low-variance and correlated variables into single set of high-variance perpendicular predictors. It also prevents collenearity. First component has highest variance, and so on.

Best practice : drop only zero-variance, and use PCA to capture low-variance variables.

### 2 trControl

To choose the best model with the best fit, train-test split must be the same, even in Cross-validation.

Can do this by predefining `trainControl` object that is reusable for multiple models.

```{r}
# Create custom indices: myFolds
# By saving the indexes in the train control, we can fit many models using the same CV folds.
myFolds <- createFolds(churn_y, k = 5)

# Create reusable trainControl object: myControl
myControl <- trainControl(
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE,
  savePredictions = TRUE,
  index = myFolds
)
```

#### Cross Validation

To do cross validation, need to modify `trainContol()`

-   number : how many slices of train/test split. `number = 5` means slice the data randomly into 5 chunks and do 4:1 train/test split = 5 possible scenarios
-   repeats : how many repeats it will perform. Do another random slice and repeat the operation according to above number.

10 fold CV

```{r results='hide'}
tenfold_cv = trainControl(
    method = "cv", 
    number = 10,
    verboseIter = TRUE
  )
# Fit lm model using 10-fold CV: model
model = train(
  price ~ ., 
  diamonds,
  method = "lm",
  trControl = tenfold_cv
)
model
```

5\*5 fold CV

```{r}
repeated_cv = trainControl(
    method = "repeatedcv", 
    number = 5,
    repeats = 5,
    verboseIter = TRUE
  )

# Fit lm model using 5 * 5 fold CV
model <- train(
  price ~ ., 
  diamonds,
  method = "lm",
  trControl = repeated_cv
)
# Print model to console
model
```

#### search

search = 'grid' or 'random'

Since searching all grid is computationally expensive, Randomly choose hyperparameters within the specified grid. Random search is used especailly when hyperparameter is given in continuous range variable.

```{r}
# Train control with random search
fitControl <- trainControl(method = "repeatedcv",
                           number = 3,
                           repeats = 5,
                           search = "random")

# Test 6 random hyperparameter combinations
tic()
nn_model_voters_big_grid <- train(turnout16_2016 ~ ., 
                   data = voters_train_data, 
                   method = "nnet", 
                   trControl = fitControl,
                   verbose = FALSE,
                   tuneLength = 6 # need to be specified in random search
                   )
toc()
```

#### Adaptive resampling

Grid search : all hyperparameter combinations are searched Random search : random subsets of hyperparameter combinations are searched

=\> All of the combinations will be tested

Adaptive resampling : hyperparameter combinations are resampled with values near combinations that performed well.

=\> combinations that are sub-optimal will not be tested - faster and efficient.

```{r}
trainControl(
  method = 'adaptive_cv',
  search = 'random',
  adaptive = list(
    min = 2,
    alpha = 0.05,
    method = 'gls',
    complete = TRUE
    
  )
)
```

-   `min` : minimum number of resamples per hyperparameter
-   `alpha` : confidence level for removing hyperparameters. Doesn't change that much
-   `method` : 'gls' for linear model and 'BT' for Bradley-Terry (for large number of hyperparameters)
-   `complete` : `TRUE` generates full resampling set / `FALSE` saves time and still get the optimal combination

### 3 tuneLength

Should be specified especially with random search.

Automatically tries different values of hyperparameters E.g. `tuneLength = 5` in SVM, it tries degree = 1,2,3,4,5, scale = 0.001,0.01,0.1,1,10, c = 0, 0.25, 0.5, 1, 2, 4

Which argument do you need to set in combination with trainControl(search = "random") in order to perform random search? [Correct! Tune length defines the number of (randomly sampled) tuning parameter combinations to compare.]

### 4 tuneGrid

Manual hyperparameter tuning : `tuneGrid` + `expand.Grid`

```{r}
# Define Cartesian grid
man_grid <- expand.grid(degree = c(1, 2, 3), 
                        scale = c(0.1, 0.01, 0.001), 
                        C = 0.5)

svm_model_voters_grid <- train(turnout16_2016 ~ ., 
                   data = voters_train_data, 
                   method = "svmPoly", 
                   trControl = fitControl,
                   verbose= FALSE,
                   tuneGrid = man_grid)

```

## model comparison

-   Highest average AUC
-   Lowest std in AUC
-   `resamples()` function

Now that you have fit two models to the churn dataset, it's time to compare their out-of-sample predictions and choose which one is the best model for your dataset.

You can compare models in caret using the resamples() function, provided they have the same training data and use the same trainControl object with preset cross-validation folds. resamples() takes as input a list of models and can be used to compare dozens of models at once (though in this case you are only comparing two models).

```{r}
# model comparison example
model_list = list(
  glmnet = model_glmnet,
  rf = model_rf
)
# collect resamples from CV folds
resamps = resamples(model_list)
resamps 
summary(resamps)
```

You can make this plot using the bwplot() function, which makes a box and whisker plot of the model's out of sample scores. Box and whisker plots show the median of each distribution as a line and the interquartile range of each distribution as a box around the median line. You can pass the metric = "ROC" argument to the bwplot() function to show a plot of the model's out-of-sample ROC scores and choose the model with the highest median ROC.

```{r}
# Create bwplot
bwplot(resamples, metric = "ROC")
# Create xyplot
xyplot(resamples, metric = "ROC")
```

Ensembling models

That concludes the course! As a teaser for a future course on making ensembles of caret models, I'll show you how to fit a stacked ensemble of models using the caretEnsemble package.

```{r}
# Create ensemble model: stack
stack <- caretStack(model_list, method = "glm")

# Look at summary
summary(stack)
```

### Make prediction

```{r}
iris_caret = iris[!(Species %in% 'versicolor')]
# Get the number of observations
n_obs <- nrow(iris_caret)

# Shuffle row indices: permuted_rows
permuted_rows <- sample(n_obs)

# Randomly order data: iris_caret
iris_caret_shuffled <- iris_caret[permuted_rows, ]

# Identify row to split on: split
split <- round(n_obs * 0.6)

# Create train
train <- iris_caret_shuffled[1:split, ]

# Create test
test <- iris_caret_shuffled[(split + 1):n_obs, ]


# Fit glm model: model
model <- glm(Species ~ ., family = "binomial", train)

# Predict on test: p
p <- predict(model, test, type = "response")

# If p exceeds threshold of 0.5, M else R: m_or_r
m_or_r <- ifelse(p > 0.5, "versicolor", "virginica")

# Convert to factor: p_class
p_class <- factor(m_or_r, levels = levels(test[["Species"]]))


```

### Confusion Matrix

```{r}

# Create confusion matrix
confusionMatrix(p_class, test[["Species"]])

```

### ROC curve

`Receiver Operating Characteristic` curve.

About classification threshold : manually calculating and comparing dozens of confusion matrices is a tedious work.

ROC curve : plot true / false `positive` rate at `EVERY POSSIBLE threshold`.

-   Visualize tradeoffs between two extremes : `100% true positive` vs `0% false positive`

    -   Suppose a binary classification problem : which probability should be used for classification? (50 pos 50 neg)

        -   If 0% : then it classifies always positive. `100% true positive` but also 100% false positive.
        -   If 100% : always classify negative. 100% false negative but also 100% true negative (`0% false positive`)

-   X axis : false positive (left better off)

-   Y axis : true positive (up better off)

Use package `caTools`

```{r}
library(caTools)

# Predict on test: p
p <- predict(model, test, type = "response")

# Make ROC curve
colAUC(p, test[["Class"]], plotROC = TRUE)
```

### AUC

`Area Under the Curve` : gives one statistic that summarizes the model's accuracy without having to specify all the details of the model!

-   0.5 is random guessing
-   1 is perfect model
-   0 is model always wrong

Most models lies between 0.5 and 1

Rule of thumb :

-   0.9 is `A`
-   0.8 is `B`
-   and so forth

You can use the trainControl() function in caret to use AUC (instead of acccuracy), to tune the parameters of your models. The twoClassSummary() convenience function allows you to do this easily.

When using twoClassSummary(), be sure to always include the argument classProbs = TRUE or your model will throw an error! (You cannot calculate AUC with just class predictions. You need to have class probabilities as well.)

```{r}
# Create trainControl object: myControl
myControl <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)

# Train glm with custom trainControl: model
model <- train(
  Class ~ ., 
  Sonar, 
  method = "glm",
  trControl = myControl
)

# Print model to console
model
```

### glmnet Model

Extension of glm models with built-in variable selection. It is great baseline model for any type of prediction, with several advantages: - quick fit - ignores noisy varibles - provides interpretable coefficients

Helps handle collinerity of regressors / small sample errors

-   Lasso regression : penalize number of non-zero coefficients
-   Ridge regression : penalize absolute magnitude of coefficients

It attempts to find parsimonious models, and pairs well with `random forest` models (it tends to yield different results!)

glmnet model has two tuning parameters :

-   `alpha [0, 1]` : 0 is pure Ridge, and 1 is pure Lasso regression
-   `lambda (0, inf)` : size of penalty, high being simpler models (penalize for complex)

Classification problems are a little more complicated than regression problems because you have to provide a custom summaryFunction to the train() function to use the AUC metric to rank your models. Start by making a custom trainControl, as you did in the previous chapter. Be sure to set classProbs = TRUE, otherwise the twoClassSummary for summaryFunction will break.

```{r}
# Create custom trainControl: myControl
myControl <- trainControl(
  method = "cv", 
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)
# Fit glmnet model: model
model <- train(
  y ~ ., 
  overfit,
  method = "glmnet",
  trControl = myControl
)

# Print model to console
model

# Print maximum ROC statistic
max(model[["results"]][["ROC"]])
```

#### glmnet tuning

```{r}
# make a custom tuning grid
myGrid  = expand.grid(
  alpha=0:1,
  lambda = seq(0.0001, 0.1, length=10)
)
# fit a model
set.seed(42)
model = train(
  y ~ .,
  data = overfit,
  metric = 'ROC',
  method = 'glmnet',
  tuneGrid = myGrid,
  trControl = myControl
)
# plot results
plot(model)
```

`caret` automatically chooses best alpha and lambda coefficient here, so no need to do further.

### random forest Model

Instead of `randomForest`, `ranger` is faster, stable and use less memory.

Pros - Yield very accurate, non-linear models with no extra work - Robust against over-fitting - Requires little preprocessing (no log-transform, normalize..) - Captures threshould effects and variable interactions well

Cons - A bit slower than glmnet - Less interpretable - Needs hyperparameters before fitting the model, which might be a bit arbitrary

```{r}
library(caret)
library(mlbench)
data(Sonar)
set.seed(42)
```

```{r}
# Fit random forest: model
model <- train(
  quality ~ .,
  tuneLength = 1,
  data = wine, 
  method = 'ranger',
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model
```

`mtry` is the most important hyperparameter : number of randomly selected variables used at each split - lower mtry : more random - highter mtry : less random

High value : potentially more accurate model, but at the expense of waiting much longer for it to run.

`caret` does this hyperparameter selection ::`tuneLength()` argument in `train()` function.

```{r}
# fit a model with deeper tuning grid
model = train(
  Class ~ .,
  data = Sonar,
  method = 'ranger',
  tuneLength = 10 # hyperparameter selection 
)
plot(model)
```

### custom tuning

Custom tuning grids to `tuneGrid()` argument

Pros : - Most flexible - Complete control Cons : - Knowledge in the model - Can dramatically increase the runtime

```{r}
# From previous step
tuneGrid <- data.frame(
  .mtry = c(2, 3, 7),
  .splitrule = "variance",
  .min.node.size = 5
)

# Fit random forest: model
model <- train(
  quality ~.,
  tuneGrid = tuneGrid,
  data = wine, 
  method = 'ranger',
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model

# Plot model
plot(model)
```

with custom trainControl

```{r}
# Fit random forest: model_rf
model_rf <- train(
  x = churn_x, 
  y = churn_y,
  metric = "ROC",
  method = "ranger",
  trControl = myControl
)
```

## Unsupervised ML

Data

```{r}
set.seed(1)
x = data.frame( a= rnorm(300, mean=0,sd=1), b= rnorm(300, mean=1, sd=1))
x
```

### kmeans

`stats` package

k-means has random component : - randomly assign each points into groups - calculate centers of groups - each point is assigned to cluster of nearest center (iteration 1) - calculate centers of groups - each point is assigned to cluster of nearest center (iteration 2) - ... - until it no longer changes any assignments of each point

Model selection

-   in R, kmeans uses total within cluster sum of squares

    -   which is the squared distance from point to center

-   Best is minimum one

-   `nstart` does multiple random assignment and give the best(minimum SS) one

```{r}
# Fit a k-means model to x using 3 centers and run the k-means algorithm 20 times. Store the result in km.out.
km.out = kmeans(x , centers = 3, nstart =20) # 20 initial random assignemnts and give the best one
# Inspect the result
summary(km.out)

```

```{r}
km.out
```

```{r}
km.out$cluster
```

```{r}
plot(x, col = km.out$cluster,
     main = "k-means with 3 clusters", 
     xlab = "", ylab = "")
```

Model selection

```{r}
# Set up 2 x 3 plotting grid
par(mfrow = c(2, 3))

# Set seed
set.seed(1)

for(i in 1:6) {
  # Run kmeans() on x with three clusters and one start
  km.out <- kmeans(x, centers = 3, nstart = 1)
  
  # Plot clusters
  plot(x, col = km.out$cluster, 
       main = km.out$tot.withinss, 
       xlab = "", ylab = "")
}
```

How many clusters to choose?

Choose one with highest elbow(kink)

```{r}
# Initialize total within sum of squares error: wss
wss <- 0

# For 1 to 15 cluster centers
for (i in 1:15) {
  km.out <- kmeans(x, centers = i, nstart = 20)
  # Save total within sum of squares to wss variable
  wss[i] <- km.out$tot.withinss
}

# Plot total within sum of squares vs. number of clusters
plot(1:15, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")

# Set k equal to the number of clusters corresponding to the elbow location
k <- 2  # 3 is probably OK, too
```

### hierarchical clustering

Number of cluster is not known ahead of time

Two approaches : bottom-up and top-down - bottom-up approach: `hclust()` function 1) assign all different cluster N 2) join `closest` one pair to one group, now N-1 clusters - here to find closest one, similarity measure should be used. Simple one is euclidean distance. 3) join next closest pair to one group, N-2 - cluster to clusters, 4 methods to determine. 1) and 3) makes balanced one and popular. 1) complete : pairwise similarity between all observations in cluster 1 and 2, pick largest similarity to gauge distance between clusters - `method = 'complete'` 2) single : pairwise but pick smallest one to gauge distance - `method = 'single'` 3) average : pairwise and pick average of distances - `method = 'average'` 4) centroid : find center of each clusters and check distance between those centers - it can make inversion, undesirable and much less used 4) ... 5) until all of them are clustered into one cluster

Preprocessing

Difference in units makes problem, standardizing is needed before clustering Use `scale(x)` for this step.

```{r}
# distance matrix, euclidean distance
#  dist(x) 
hclust.out = hclust(dist(x))
summary(hclust.out)
# dentrogram
plot(hclust.out)
abline(h = 6, col='red') # cut at height 6, two clusters
```

Generate vector of clustering outcome

```{r}
# Cut by height
cutree(hclust.out, h = 7)
# Cut by number of clusters
cutree(hclust.out, k = 3)
```

Cluster linkage methods

```{r}
# Cluster using complete linkage: hclust.complete
hclust.complete <- hclust(dist(x), method = "complete")

# Cluster using average linkage: hclust.average
hclust.average <- hclust(dist(x), method = "average")

# Cluster using single linkage: hclust.single
hclust.single <- hclust(dist(x), method = "single")

# Plot dendrogram of hclust.complete
plot(hclust.complete, main = "Complete")

# Plot dendrogram of hclust.average
plot(hclust.average, main = "Average")

# Plot dendrogram of hclust.single
plot(hclust.single, main = "Single")
```

### Dimensionality reduction

Dimensionality reduction is needed for: - to find structure in features - aid in visualization

principal component analysis - linear combination of features - find lower dimensional representation that maintains and describes maximum amount of variance from original data - maintain most variance in the data - components are orthgonal (uncorrelated) by construction

```{r}
setDT(iris)
pr.iris = prcomp(x = iris[,-5],
                 scale = FALSE, # divide by std to set standard deviation to 1
                 center = TRUE # demean the data, centered around zero
                 )
summary(pr.iris) # variance explained by each component
# here, PC1 explains 92% of variability of the data
```

```{r}
pr.iris

pr.iris$center
pr.iris$scale
pr.iris$rotation
# Rotation : the directions of the principal component vectors in terms of the original features/variables. This information allows you to define new data in terms of the original principal components
```

#### Visualizing

See the factor loadings : `biplot()`

```{r}
biplot(pr.iris) # Petal.Width and Petal.Length has similar loadings

```

Screeplot

A scree plot shows the variance explained as the number of principal components increases. Sometimes the cumulative variance explained is plotted as well.

```{r}
pr.var = pr.iris$sdev^2 # make it variance
pve = pr.var / sum(pr.var) # proportion of variance explained
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")

# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")
```

## Deep Learning

















# NETWORK ANALYSIS

## My Functions


### Remove zero weight edges

```{r}
delete.edges(test_graph, which(E(test_graph)$weight == 0))
```






## Concepts

### Centrality measures 
: to see important and influential vertices

-   degree centrality

    -   out-degree : how many edges does a vertex send out
    -   in-degree : how mane edges does a vertex receives
    -   total-degree : sum of out and in degree (undirected network's degree centrality)
    -   `degree` function

-   strength

    -   used in weighted networks (connection strength)
    -   summation of weights of edges connected to each node
    -   `strength` function

-   betweenness

    -   a measure that quantifies how often a node lies on the shortest path between other nodes.
    -   In other words, searching for `hub`
    -   Both directed / non-directed
    -   `betweenness()`

-   weighted betweenness

    -   when edges are weighted, then shortest path is the MINUMUM of sum of weights.
    -   Usually weights need to be inversed to calculate correctly
    -   `edge_betweenness(g, weight =dist_weight)`


-   closeness

    -   a measure that quantifies how close a node is to all other nodes in the network in terms of shortest path distance.
    -   `closeness()`

-   eigenvector centrality (undirected network)

    -   How well connected a vertex is 
    -   Highest eigenvector centrality : those that are connected to many others but especially who themselves are highly connected to others

-   closeness centrality

    -   Counting how many steps are required to get to every other node
    -   `closeness()`

-   transitivity (clustering coefficient)

    -   the extent to which the nodes in the network are connected

$\frac{number\ of\ triangles}{number\ of\ triads}$ 
at each node

    -   type global

        -   computed for the whole network

    -   type local

        -   computed for each node separately

-   pagerank centrality

    -   `page.rank()`



-   dyads : two connected vertices(nodes)

-   triads : three connected verticies

    -   triangle



### Overall structure of network

-   density

    -   Proportion of edges that actually do exist in the network out of all potentially could exist between every pair of vertices
    -   Measure of how interconnected a network is
    -   `edge_density()` funciton
    -   Network connectance(density)
    -   Number of edges in a fully connected network : $_nC_2 = \frac{nodes(nodes-1)}2$
    -   Network connectance : $p = \frac{number\ of\ edges\ connected}{_nC_2}$

-   average path length

    -   Mean of length of shortest path between all pairs of vertices in the network
    -   the more connected overall, the shorter the shortest-length
    -   `mean_distance()` function


### Connectivity

:   Measures how densely connected the vertices of a graph are.

Vertex / Edge connectivity : how man vertices or edges need to be removed to disconnect graph, creating two distinct graphs. `vertex_connectivity()`, `edge_connectivity()` : returns integer

More details about - how many, which are cut, what are the partitions after the cut? `min_cut( value.only =FALSE)`, and `stMincuts(graph, 'node1','node2')`

connectivity randomizations








## igraph package

Setup

```{r message=F}
library(igraph)
# sample data
friends = structure(list(name1 = c("Jessie", "Jessie", "Sidney", "Sidney", 
"Karl", "Sidney", "Britt", "Shayne", "Sidney", "Sidney", "Jessie", 
"Donnie", "Sidney", "Rene", "Shayne", "Jessie", "Rene", "Elisha", 
"Eugene", "Berry", "Odell", "Odell", "Britt", "Elisha", "Lacy", 
"Britt", "Karl"), name2 = c("Sidney", "Britt", "Britt", "Donnie", 
"Berry", "Rene", "Rene", "Sidney", "Elisha", "Whitney", "Whitney", 
"Odell", "Odell", "Whitney", "Donnie", "Lacy", "Lacy", "Eugene", 
"Jude", "Odell", "Rickie", "Karl", "Lacy", "Jude", "Whitney", 
"Whitney", "Tommy")), .Names = c("name1", "name2"), class = "data.frame", row.names = c(NA, 
-27L))
print(friends) # unordered network
friends.mat = as.matrix(friends)
```


### Conversion

#### Matrix to graph

```{r}
# edgelist accepts matrix, not dataframe
g <- graph.edgelist(friends.mat, directed = FALSE)
```



#### Dataframe to graph

`graph_from_data_frame`

```{r}
friends1_edges = structure(list(name1 = c("Joe", "Joe", "Joe", "Erin", "Kelley", 
"Ronald", "Ronald", "Ronald", "Michael", "Michael", "Michael", 
"Valentine", "Troy", "Troy", "Jasmine", "Jasmine", "Juan", "Carey", 
"Frankie", "Frankie", "Micheal", "Micheal", "Keith", "Keith", 
"Gregory"), name2 = c("Ronald", "Michael", "Troy", "Kelley", 
"Valentine", "Troy", "Perry", "Jasmine", "Troy", "Jasmine", "Juan", 
"Perry", "Jasmine", "Juan", "Juan", "Carey", "Demetrius", "Frankie", 
"Micheal", "Merle", "Merle", "Alex", "Gregory", "Marion", "Marion"
), hours = c(1L, 3L, 2L, 3L, 5L, 1L, 3L, 5L, 2L, 1L, 3L, 5L, 
3L, 2L, 6L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 3L, 2L)), .Names = c("name1", 
"name2", "hours"), class = "data.frame", row.names = c(NA, -25L
))

friends1_nodes = structure(list(name = c("Joe", "Erin", "Kelley", "Ronald", "Michael", 
"Valentine", "Troy", "Jasmine", "Juan", "Carey", "Frankie", "Micheal", 
"Keith", "Gregory", "Perry", "Demetrius", "Merle", "Alex", "Marion"
), gender = c("M", "F", "F", "M", "M", "F", "M", "F", "M", "F", 
"F", "M", "M", "M", "M", "M", "M", "F", "F")), .Names = c("name", 
"gender"), class = "data.frame", row.names = c(NA, -19L))

friends1_edges
```


```{r}
# Create an igraph object with attributes directly from dataframes
g1 <- graph_from_data_frame(d = friends1_edges, vertices = friends1_nodes, directed = FALSE)
# d : symbolic edge list in first two columns
```


```{r}
# Subset edges greater than or equal to 5 hours
E(g1)[[hours >= 5]] 

# Set vertex color by gender
V(g1)$color <- ifelse(V(g1)$gender == "F", "orange", "dodgerblue")


# Plot the graph
plot(g1, vertex.label.color = "black")

E(g1)[[hours]]

as_adjacency_matrix(g1, attr = 'hours' )
```

### Basic features

Vertices and Edges

```{r}
# Subset vertices and edges
V(g)
E(g)

# number of nodes ( 2 commands)
vcount(g) # old
gorder(g)
# Count number of edges
ecount(g) # old
gsize(g)

```


### add attributes

```{r}
genders = c("M", "F", "F", "M", "M", "M", "F", "M", "M", "F", "M", "F", 
"M", "F", "M", "M")
ages = c(18, 19, 21, 20, 22, 18, 23, 21, 22, 20, 20, 22, 21, 18, 19, 
20)
hours = c(1, 2, 2, 1, 2, 5, 5, 1, 1, 3, 2, 1, 1, 5, 1, 2, 4, 1, 3, 1, 
1, 1, 4, 1, 3, 3, 4)
```


#### Vertex attributes

```{r}
# Create new vertex attribute called 'gender'
g <- set_vertex_attr(g, "gender", value = genders)

# Create new vertex attribute called 'age'
g <- set_vertex_attr(g, "age", value = ages)

# View all vertex attributes in a list
vertex_attr(g)

# View attributes of first five vertices in a dataframe
V(g)[[1:5]] 
```

#### Edge attributes

```{r}
# Create new edge attribute called 'hours'
g <- set_edge_attr(g, "hours", value = hours)

# View edge attributes of graph object
edge_attr(g)


# Find all edges that include "Britt"
E(g)[[inc('Britt')]]

# Find all pairs that spend 4 or more hours together per week
E(g)[[hours>=4]]  

```


## directed network

Check if the graph object is directed / weighted

```{r}
is.directed(g)
is.weighted(g)
```

```{r}
# Is there an edge going from vertex 184 to vertex 178?
g['184', '178'] # 1

# Is there an edge going from vertex 18 to vertex 20?
g['178', '184'] # 0

# Show all edges going to or from vertex 184
incident(g, '184', mode = c("all"))

# Show all edges going out from vertex 184
incident(g, '184', mode = c("out"))
```

Relationships between vertices

```{r}
# Identify all neighbors of vertex 12 regardless of direction
neighbors(g, '12', mode = c('all'))

# Identify other vertices that direct edges towards vertex 12
neighbors(g, '12', mode = c('in'))

# Identify any vertices that receive an edge from vertex 42 and direct an edge to vertex 124
n1 <- neighbors(g, '42', mode = c('out'))
n2 <- neighbors(g, '124', mode = c('in'))
intersection(n1, n2)

```

Distances between vertices

```{r}
#- Which two vertices are the furthest apart in the graph ?
farthest_vertices(g) 

#- Shows the path sequence between two furthest apart vertices.(it is called a diameter)
get_diameter(g)  

# Identify vertices that are reachable within two connections from vertex 42
ego(g, 2, '42', mode = c('out'))

# Identify vertices that can reach vertex 42 within two connections
ego(g, 2, '42', mode = c('in'))
```

```{r}
# Calculate the out-degree of each vertex
g.outd <- degree(g, mode = c("out"))

# View a summary of out-degree
table(g.outd)

# Make a histogram of out-degrees
hist(g.outd, breaks = 30)

# Find the vertex that has the maximum out-degree
which.max(g.outd)
```

```{r}
# Calculate betweenness of each vertex
g.b <- betweenness(g, directed = TRUE)

# Show histogram of vertex betweenness
hist(g.b, breaks = 80)

# Create plot with vertex size determined by betweenness score
plot(g, 
     vertex.label = NA,
     edge.color = 'black',
     vertex.size = sqrt(g.b)+1,
     edge.arrow.size = 0.05,
     layout = layout_nicely(g))

```

One issue with the measles dataset is that there are three individuals for whom no information is known about who infected them. One of these individuals (vertex 184) appears ultimately responsible for spreading the disease to many other individuals even though they did not directly infect too many individuals. However, because vertex 184 has no incoming edge in the network they appear to have low betweenness. One way to explore the importance of this vertex is by visualizing the geodesic distances of connections going out from this individual. In this exercise you shall create a plot of these distances from this patient zero.



```{r}
# Make an ego graph
g184 <- make_ego_graph(g, diameter(g), nodes = '184', mode = c("all"))[[1]]

# Get a vector of geodesic distances of all vertices from vertex 184 
dists <- distances(g184, "184")

# Create a color palette of length equal to the maximal geodesic distance plus one.
colors <- c("black", "red", "orange", "blue", "dodgerblue", "cyan")

# Set color attribute to vertices of network g184.
V(g184)$color <- colors[dists+1]

# Visualize the network based on geodesic distance from vertex 184 (patient zero).
plot(g184, 
     vertex.label = dists, 
     vertex.label.color = "white",
     vertex.label.cex = .6,
     edge.color = 'black',
     vertex.size = 7,
     edge.arrow.size = .05,
     main = "Geodesic Distances from Patient Zero"
     )
```

### eigenvector centrality

```{r}
# Make an undirected network
g <- graph_from_data_frame(gump, directed = FALSE)

# Identify key nodes using eigenvector centrality
g.ec <- eigen_centrality(g)
which.max(g.ec$vector)

# Plot Forrest Gump Network
plot(g,
vertex.label.color = "black", 
vertex.label.cex = 0.6,
vertex.size = 25*(g.ec$vector),
edge.color = 'gray88',
main = "Forrest Gump Network"
)
```



## census

Dyads (two connected vertices)

3 types

-   Null : no edges
-   Asymettric : one directional edge
-   Mutual : two directed edge back and forth

Triads (three connected vertices)

16 types

```{r}
# Perform dyad census
dyad_census(amzn_g)

# Perform triad census
triad_census(amzn_g)

# Find the edge density
edge_density(amzn_g)
```




## Network Structure

```{r}
# Get density of a graph
gd <- edge_density(g)

#Get the diameter of the graph g
diameter(g, directed = FALSE)

# average path length

#Get the average path length of the graph g
g.apl <- mean_distance(g, directed = FALSE)
g.apl

```

Generating random network with given parameters

Random graph & randomization tests - To determine if the network is particularly different from those random networks

1.  Generate 1000 graphs based on original network, with same number of vertices and similar density
2.  Calculate average path length of original network
3.  Calculate average path length of 1000 random networks
4.  determine how many random networks have length greater or less than original average length path

Generate 1000 random graphs

```{r}
gl = vector('list', 1000)

for (i in 1:1000){
  gl[[i]] = erdos.renyi.game(
    n = gorder(g), # same number of vertices
    p.or.m = edge_density(g), # similar density
    type = 'gnp'
  )
}


# calculate average mean distance of 1000 random graphs

gl.apls = unlist(
  lapply(glm ,mean_distance, directed = F)
)

# histogram of random mean distances
hist(gl.apls, breaks =20)



```

Generate a random graph using the function erdos.renyi.game(). 
The first argument n should be the number of nodes of the graph g which can be calculated using gorder(), 
the second argument p.or.m should be the density of the graph g which you previously stored as the object gd. 
The final argument is set as type='gnp' to tell the function that you are using the density of the graph to generate a random graph.

```{r}
# Create one random graph with the same number of nodes and edges as g
g.random <- erdos.renyi.game(n = gorder(g), p.or.m = gd, type = "gnp")

g.random

plot(g.random)

# Get density of new random graph `g.random`
edge_density(g.random)

#Get the average path length of the random graph g.random
mean_distance(g.random, directed = FALSE)


```

```{r}

# Generate 1000 random graphs
gl <- vector('list',1000)
  
for(i in 1:1000){
  gl[[i]] <- erdos.renyi.game(n = gorder(g), p.or.m = gd, type = "gnp")
}

# Calculate average path length of 1000 random graphs
gl.apls <- unlist(lapply(gl, mean_distance, directed = FALSE))

# Plot the distribution of average path lengths
hist(gl.apls, xlim = range(c(1.5, 6)))
abline(v = g.apl, col = "red", lty = 3, lwd = 2)

# Calculate the proportion of graphs with an average path length lower than our observed
mean(gl.apls < g.apl)
```



### network substructure

#### closed triangles

For every three vertices there exists three potential edges. If all edges exist, then the triad is said to be `closed`.

To identify all closed triangles : `triangles()` function.

Global Transitivity : probability that the adjacent verticies of a given vertex are connected. `transitivity()` function.

Local Transitivity : the proportion of closed triangles that a vertex is a part of, out of theoretical number of closed triangles it could be a part of given its connections.

`count_triangles()` function yields the number of closed triangle of given vertex.


#### clique

In a clique, every vertex is connected to every other vertex. `largest_cliques()`

`max_cliques()` : identify cliques of any size, 2+

```{r}
# Show all triangles in the network.
matrix(triangles(g), nrow = 3)

# Count the number of triangles that vertex "BUBBA" is in.
count_triangles(g, vids = 'BUBBA')

# Calculate  the global transitivity of the network.
g.tr <- transitivity(g)
g.tr

# Calculate the local transitivity for vertex BUBBA.
transitivity(g, vids = 'BUBBA', type = "local")
```

```{r}
# Calculate average transitivity of 1000 random graphs
gl.tr <- lapply(gl, transitivity)
gl.trs <- unlist(gl.tr)

# Get summary statistics of transitivity scores
summary(gl.trs)

# Calculate the proportion of graphs with a transitivity score higher than Forrest Gump's network
mean(gl.trs > g.tr) # 0 - randomized networks gave lower transitivity score than Gump's network

```

```{r}
# Identify the largest cliques in the network
largest_cliques(g)

# Determine all maximal cliques in the network and assign to object 'clq'
clq <- max_cliques(g)

# Calculate the size of each maximal clique.
table(unlist(lapply(clq, length)))
```

Visualize largest cliques

```{r}

# Assign largest cliques output to object 'lc'
lc <- largest_cliques(g)

# Create two new undirected subgraphs, each containing only the vertices of each largest clique.
gs1 <- as.undirected(subgraph(g, lc[[1]]))
gs2 <- as.undirected(subgraph(g, lc[[2]]))


# Plot the two largest cliques side-by-side

par(mfrow=c(1,2)) # To plot two plots side-by-side

plot(gs1,
     vertex.label.color = "black", 
     vertex.label.cex = 0.9,
     vertex.size = 0,
     edge.color = 'gray28',
     main = "Largest Clique 1",
     layout = layout.circle(gs1)
)

plot(gs2,
     vertex.label.color = "black", 
     vertex.label.cex = 0.9,
     vertex.size = 0,
     edge.color = 'gray28',
     main = "Largest Clique 2",
     layout = layout.circle(gs2)
)
```

## Relationship measures

Assortativity 

- The preferential attachement of vertices to other verticies that are similar in numerical or categorical attributes 
- E.g. political party, gender, age 
- Network with high assortativity? how high is high? => by `randomization test`

Reciprocity (directed network) 

- equal to proportion of edges that are symmetrical. (A -> B) and (B -> A) 
- Proportion of outgoing edges that also have an incoming edge.

```{r}
# Convert the gender attribute into a numeric value
values <- as.numeric(factor(V(g1)$gender))

# Calculate the assortativity of the network based on gender
assortativity(g1, values)

# Calculate the assortativity degree of the network
assortativity.degree(g1, directed = FALSE)
```

Using randomizations to assess assortativity

```{r}
# Calculate the observed assortativity
observed.assortativity <- assortativity(g1, values)

# Calculate the assortativity of the network randomizing the gender attribute 1000 times
results <- vector('list', 1000)
for(i in 1:1000){
  results[[i]] <- assortativity(g1, sample(values))
}

# Plot the distribution of assortativity values and add a red vertical line at the original observed value
hist(unlist(results))
abline(v = observed.assortativity, col = "red", lty = 3, lwd=2)

```

Reciprocity

```{r}
# Make a plot of the chimp grooming network
plot(g,
     edge.color = "black",
     edge.arrow.size = 0.3,
     edge.arrow.width = 0.5)


# Calculate the reciprocity of the graph
reciprocity(g)

```

Dyacity(Homophily measure) 

- Connectedness between nodes with the `same label` compared to what is expected in a random configuration of the network 
- Expected number of same label edges : All possible number of edges within the same label
\* connectance(density) of network $p$ 
- \$n\_{green}C_2 \* p = \$ 
- e.g. 9 white nodes 6 green nodes, 21 edges and p =0.2 - $_6C_2 * 0.2 = 3$ - $D = \frac{number\ of\ same\ label\ edges}{expected\ number\ of\ same\ label\ edges}$ 
- Dyadic : D >1 D ~ 1 random , D < 1 anti-dyadic

Heterophilicity 

- Connectedness between nodes with different labels compared to what is expected for a random configuration of the network 
- Expected number of cross label edges : $n_wn_gp$
- e.g. 9 white nodes 6 green nodes 21 edges and connectance $p=0.2$
- expected number of cross label edges is 11 ($9*6*0.2$) 
- $H = \frac{number\ of\ cross\ label\ edges}{expected\ number\ of\ cross\ label\ edges}$ 
- H > 1 : heterophilic / H = 1 : random / H < 1 : Heterophobic

Network connectance



### community detection

Several algorithms are out there, and they have pros and cons, so all should be tried out to determine the best fit.

-   fastgreedy detection

    -   modularity score based
    -   works by tyring to build larger and larger commnunities by adding vertices to each community one by one and assessing modularity score
    -   modularity score : an index of how inter-connected edges are within vs between communities.
    -   `cluster_fastgreedy`

-   edge-betweenness

    -   works by dividing the network into smaller and smaller pieces until it finds edges that it perceives to be bridges between communities.
    -   `cluster_edge_betweenness`

-   leading eigenvector

    -   `cluster_leading_eigen`

-   label propergation

    -   `cluster_label_prop`

-   infomap

-   louvain (multilevel)




```{r}
# Perform fast-greedy community detection on network graph
kc = fastgreedy.community(g)

# Determine sizes of each community
sizes(kc)

# Determine which individuals belong to which community
membership(kc)

# Plot the community structure of the network
plot(kc, g)
```

```{r}
# Perform edge-betweenness community detection on network graph
gc = edge.betweenness.community(g)

# Determine sizes of each community
sizes(gc)

# Plot community networks determined by fast-greedy and edge-betweenness methods side-by-side
par(mfrow = c(1, 2)) 
plot(kc, g)
plot(gc, g)
```




### community comparison

How to compare the communities found by each detection methods?

`compare(method ='vi')` to find similarity. 
-One possible use : `vi` variation information metric. 
- How much variation is there in community membership for each vertex 
- The closer to zero, the more likely any two vertices to be found in the same community

```{r}
# From previous step
retweet_graph_undir <- as.undirected(retweet_graph)
communities_fast_greedy <- cluster_fast_greedy(retweet_graph_undir)
communities_infomap <- cluster_infomap(retweet_graph_undir)
communities_louvain <- cluster_louvain(retweet_graph_undir)

two_users <- c("bass_analytics", "big_data_flow")

# Subset membership of communities_fast_greedy by two_users
membership(communities_fast_greedy)[two_users]
```

```{r}
data()
```

Collective Inferencing

Collective inferencing is a procedure to simultaneously label nodes in interconnected data to reduce classification error.

In this exercise you will perform collective inferencing and see the effect it has on the churn prediction using the AUC performance measure. AUC, or area under the ROC curve, is commonly used to assess the performance of classification techniques.

AUC = probability that a randomly chosen churner is ranked higher by the model than a randomly chosen non-churner AUC = number between 0.5 and 1, where a higher number means a better model




## Network features

`degree()` function

number of nodes that are connected to each node (first order)

`neighborhood.size(g, order=2)`

2nd order neighberhood (connected within 2, including itself)

`count_triangles()`

How many triangles does a node form?

Centrality measures - Betweenness - Closeness






## Network Visualization package

```{r}
library(ggnetwork)
library(ggraph)
```


### igraph base

```{r}
# See a list of possible layouts
ls("package:igraph", pattern = "^layout_.")
```



```{r}
# Plot the graph object g1 in a circle layout
plot(g1, vertex.label.color = "black", layout = layout_in_circle(g1))
```


```{r}
# Plot the graph object g in a Fruchterman-Reingold layout 
plot(g1, vertex.label.color = "black", layout = layout_with_fr(g1))
```


```{r}
# Plot the graph object g in a Tree layout 
plot(g1, vertex.label.color = "black", layout = layout_as_tree(g1))
```


```{r}
# Plot the graph object g using igraph's chosen layout 
m1 <- layout_nicely(g1)
plot(g1, vertex.label.color = "black", layout = m1)

```

```{r}
# Create a vector of weights based on the number of hours each pair spend together
w1 <- E(g1)$hours

# Plot the network varying edges by weights
m1 <- layout_nicely(g1)
plot(g1, 
        vertex.label.color = "black", 
        edge.color = 'black',
        edge.width = w1,
        layout = m1)
```

####  Network Filtering

```{r}
# Create a new igraph object by deleting edges that are less than 2 hours long 
g2 <- delete_edges(g1, E(g1)[hours < 2])


# Plot the new graph 
w2 <- E(g2)$hours
m2 <- layout_nicely(g2)

plot(g2, 
     vertex.label.color = "black", 
     edge.color = 'black',
     edge.width = w2,
     layout = m2)

```



```{r}
# Add a node attribute called churn
V(network)$churn <- customers$churn

# Add a node attribute called color
V(network)$color <- V(network)$churn

# Change the color of churners to red and non-churners to white
V(network)$color <- gsub("1","red",V(network)$color)
V(network)$color <- gsub("0","white",V(network)$color)

# Plot the network
plot(network, vertex.label = NA, edge.label = NA,
     edge.color = "black", vertex.size = 2)
```

Subgraph with conditions

```{r}
# Create a subgraph with only churners
churnerNetwork <- induced_subgraph(network, 
                    v = V(network)[which(V(network)$churn == 1)])
                    
# Plot the churner network                     
plot(churnerNetwork, vertex.label = NA, vertex.size = 2)
```

### ggraph

Regular graph(Karmada-Kawai layout)

```{r}
ggraph(g, layout = "with_kk") +
  geom_edge_link(aes(alpha = weight)) +
  geom_node_point()  +
  # Add a node text geometry, mapping label to id and repelling
  geom_node_text(aes(label = id), repel = TRUE)
```

circular graph

```{r}
# Visualize the network in a circular layout
ggraph(g, layout = "in_circle") + 
  # Map tie transparency to its weight
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point()
```

Grid graph

```{r}
# Change the layout so points are on a grid
ggraph(g, layout = "on_grid") + 
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point()
```

Visualize betweennesss

```{r}
# Plot with the Kamada-Kawai layout 
ggraph(g, layout = "with_kk") + 
  # Add an edge link geom, mapping alpha to weight
  geom_edge_link(aes(alpha = weight)) + 
  # Add a node point geom, mapping size to degree
  geom_node_point(aes(size = degree))
```

Highlight edges

```{r}
# Make is_weak TRUE whenever the tie is weak
is_weak <- E(g)$weight == 1

ggraph(g, layout = "with_kk") +
  # Add an edge link geom, mapping color to is_weak
  geom_edge_link(aes(color = is_weak))
```

Subgraph

```{r}
ggraph(g, layout = "with_kk") + 
  # Map filter to is_weak
  geom_edge_link(aes(filter = is_weak), alpha = 0.5) 
```


### ggnetwork

```{r}
# Call ggplot

ggplot(
  # Convert retweet_samp to a ggnetwork
  ggnetwork(retweet_samp), 
  # Specify x, y, xend, yend
  aes(x = x, y = y, xend = xend, yend = yend)) +
  # Add a node layer
  geom_nodes() +
  # Change the edges to arrows of length 6 pts
  geom_edges(arrow = arrow(length = unit(6, "pt"))) +
  # Use the blank theme
  theme_blank()
```


### visNetwork

Interactive visualization package

```{r}
# convert igraph to visNetwork
data <- toVisNetworkData(g)

# Visualize the network
visNetwork(
  nodes = data$nodes, 
  edges = data$edges, 
  width = 300, 
  height = 300
)
```

It is possible to change the layout of the visualization using the visNetwork() and visIgraphLayout() function calls. The igraph package contains several functions that provide algorithms to lay out the nodes. You can pass the function name as a string to the layout argument of visIgraphLayout() to use it.

```{r}
# Update the plot
visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  # Change the layout to be in a circle
  visIgraphLayout(layout = "layout_with_kk")
# on grid format
visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  # Change the layout to be on a grid
  visIgraphLayout(layout = "layout_on_grid")
```

Here, we will highlight the nearest nodes and ties when a node is selected.

```{r}
# Add to the plot
visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  # Choose an operator
  visIgraphLayout(layout = "layout_with_kk") %>%
  # Change the options to highlight the nearest nodes and ties
  visOptions(highlightNearest = TRUE)
```

Nodes by ID

```{r}
# Update the plot
visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visIgraphLayout(layout = "layout_with_kk") %>%
  # Change the options to allow selection of nodes by ID
  visOptions(nodesIdSelection = TRUE)
```

Nodes by group (cluster)

```{r}
# Update the plot
visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visIgraphLayout(layout = "layout_with_kk") %>%
  # Change options to select by group
  visOptions(selectedBy = "group")
```




## Classifier

### relational neighber classifier

```{r}
# You are given two vectors: ChurnNeighbors and NonChurnNeighbors with each customer's number of neighbors that have churned and not churned, respectively.
# Compute the churn probabilities
churnProb <- ChurnNeighbors / (ChurnNeighbors + NonChurnNeighbors)

# Find who is most likely to churn
mostLikelyChurners <- which(churnProb == max(churnProb))

# Extract the IDs of the most likely churners
customers$id[mostLikelyChurners]
```

probabilistic relational neighbor classifier

## Adjacency Matrix

`as_adjacency_matrix(g)`

With weighted network,

`as_adjacency_matrix(g, attr = 'weight')`




## Hierarchical clustering

The basic idea behind hierarchical clustering is to define a measure of similarity between groups of nodes and then incrementally merge together the most similar groups of nodes until all nodes belongs to a unique cluster. The result of this process is called a dendrogram.

Similarity measure

pearson similarity is used

-   single linkage : the similarity between groups is the max of similarities between nodes of different groups
-   complete linkage : the similarity between groups is the min of similarities between nodes of different groups
-   average linkage : the similarity between groups is the average of similarities between nodes of different groups

Hierarchical clustering algorithm

1)  evaluate the similarity measures for all node pairs
2)  assign each node to a group of its own
3)  find a pair of groups with the highest similarity and join them together into single group
4)  calculate similarity between new composite groups and all others
5)  repeat 3),4) until all nodes are grouped into one.

in R, `hclust` package

```{r}
# compute a distance matrix
D <- 1-S

# obtain a distance object 
d <- as.dist(D)

# run average-linkage clustering method and plot the dendrogram 
cc <- hclust(d, method = "average")
plot(cc)
```

```{r}
# Cut the dendrogram tree into 4 clusters
cls <- cutree(cc, k = 4) # factor vector that classifies each into 1,2,3,4

# Add cluster information to nodes
nodes_with_clusters <- nodes %>%
  mutate(cluster = cls)

# See the result
nodes_with_clusters
```

Visualize cluster

```{r}
# From previous step
V(g)$cluster <- nodes$cluster

# Update the plot
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = weight), show.legend=FALSE) +  
  geom_node_point(aes(color = factor(cluster))) + 
  labs(color = "cluster")  +
  # Facet the nodes by cluster, with a free scale
  facet_nodes(~ cluster, scales="free")
```

## Predictive models

from graph object -\> data.frame -\> run ML

Logistic regression : white-box, comprehensible Random forest : black-box, powerful

```{r}
# Set the seed
set.seed(7)

# Create the index vector
index_train <- sample(1:nrow(studentnetworkdata), 2 / 3 * nrow(studentnetworkdata))

# Make the training set
training_set <- studentnetworkdata[index_train, ]

# Make the test set
test_set <- studentnetworkdata[-index_train, ]
```

Logit model

```{r}
# Make firstModel
firstModel <- glm(Future ~ degree + degree2 + triangles + betweenness + closeness + transitivity, family = 'binomial', data = training_set)
# Build the model
secondModel <- glm(Future ~ ChurnNeighbors + RelationalNeighbor + ChurnNeighborsSecond + RelationalNeighborSecond + averageDegree + averageTriangles + averageTransitivity + averageBetweenness, 
                   family = 'binomial', data = training_set)
# Build the model
thirdModel <- glm(Future~., family = 'binomial', data=training_set)

```

Random forest model

```{r}
# Load package
library(randomForest) # slower than ranger package

# Set seed
set.seed(863)

# Build model
rfModel <- randomForest(as.factor(Future)~. ,data=training_set)

# Plot variable importance
varImpPlot(rfModel)
```

## Examples

Generate a column with degree / strength from graph object

```{r}
nodes_with_centrality <- nodes %>%
  mutate(
    degree = degree(g),
    # Add a column containing the strength of each node
    strength = strength(g)
  ) %>%
  # Arrange rows by descending strength
  arrange(desc(strength))

# See the result
nodes_with_centrality
```


# DATABASE & SERVER COMPUTING (HPC)

## SSH

### Hypergator conn

```
ssh gunsu.son@hpg.rc.ufl.edu
```


### WRDS conn

```
ssh mgson633@wrds-cloud.wharton.upenn.edu
```



## Filezilla

### Site Manager

Go to 

#### Hypergator

    - host : hpg.rc.ufl.edu
    - port : 22

#### WRDS

    - host : wrds-cloud.wharton.upenn.edu
    - port : 22




## SCP

Upload and download files from server

```{zsh}
# Downloading
scp mgson633@wrds-cloud.wharton.upenn.edu:/home/ufl/mgson633/SAS/ibes/suecars.sas7bdat /Users/matthewson/Desktop # download
# scp user@server:/path/to/remotefile.zip /Local/Target/Destination
```


```{r}
rstudioapi::terminalExecute('scp ~/Dropbox\ \(UFL\)/Nimal_Matthew/Projects/Options_ML/outputs/alldays_c.fst gunsu.son@hpg.rc.ufl.edu:/blue/nimalendran/matthewson/projects/optm/data')
```



## Sparklyr

### Installation 

Install local spark
```{r}
library(sparklyr)
spark_install() #2.4.3 and 2.7 as default
```


### Config

For SAS reading package : use saurfang's Spark package for SAS reading

```{r}
config <- spark_config()
config["sparklyr.shell.packages"] = "saurfang:spark-sas7bdat:2.0.0-s_2.11"
config["sparklyr.shell.repositories"] = "https://repos.spark-packages.org/"

sc = spark_connect(master = "local", config = config)
```





## WRDS

### Setup for WRDS

For more details, check wrds website\
<https://wrds-www.wharton.upenn.edu/pages/support/programming-wrds/programming-r/r-from-your-computer/>

### Install package : Rpostgres

```{r}
install.packages("RPostgres")
# install.packages("RPostgres", repos = "https://r-dbi.r-universe.dev")
```

### .pgpass file

For local and WRDS cloud
use `option(alt)` + `cmd` + `enter` to run in terminal\

    nano ~/.pgpass

Save below inside .pgpass file

    wrds-pgdata.wharton.upenn.edu:9737:wrds:wrds_username:wrds_password

Run below to secure file

    chmod 600 ~/.pgpass



For Windows

    %APPDATA%\postgresql\pgpass.conf


On CMD:

    echo wrds-pgdata.wharton.upenn.edu:9737:wrds:wrds_username:wrds_password > %APPDATA%\postgresql\pgpass.conf

### DBPLYR ON WRDS

```{r}
library(tidyverse)
library(dbplyr)
library(RPostgres)
con <- dbConnect(Postgres(),
                 host='wrds-pgdata.wharton.upenn.edu',
                 port=9737,
                 dbname='wrds',
                 sslmode='require',
                 user='mgson633'
                 )

```


WRDS table names

https://wrds-www.wharton.upenn.edu/users/products/



The solution is to either by explicit with:

crsp_a_stock.dsf
crsp_m_stock.dsf
crsp_q_stock.dsf
 

Dsf
 = tbl(con, in_schema('tr_ibes','act_epsus'))
 
OR
 
dbExecute(con, "SET search_path='crsp_a_stock'")
Dsf = tbl(con, 'dsf')
 



#### CRSP

##### S&P500 Identifiers

Join dsf and dsenames (for ticker symbol) then query

```{r}
dsf %>% 
  select(cusip, permno, date, ret, retx) %>% 
  filter(year(date) >= 2018,) %>% 
  right_join(dsenames %>% select(permno,namedt,nameendt,permno,ticker), 
                   sql_on = '"LHS".date >= "RHS".namedt AND "LHS".date <= "RHS".nameendt AND "LHS".permno = "RHS".permno') %>% 
  filter(ticker %in% local(special_tickers$Ticker))
```


S&P500 Constituent
```{r}
# crsp.dsp500list / crsp.msp500list
dsp500list = tbl(con, in_schema('crsp_a_indexes', 'dsp500list')) # daily stock file S&P500 list
snp500 = dsp500list %>% filter(start <= '2019-01-01',  ending >= '2021-03-31') %>% collect %>% setDT 
# samples to be included during the sample period

snp500[,uniqueN(permno)]
```

Permno to TICKER : use Company identifiers

```{r}
# crsp.dsenames / crsp.msenames
dsenames = tbl(con, in_schema('crsp_a_stock', 'dsenames')) # daily names
dsenames = dsenames %>% collect %>% setDT
```


Merge 

```{r}
# names that are valid at 2019-01-01, and registered as snp throughout the sample period (2019 ~ 2021 Mar)
snp500_tickers = dsenames[,.(permno, namedt, nameendt, ncusip, ticker, tsymbol)] %>%
  .[nameendt >= '2019-01-01'] %>%
  .[snp500, on = .(permno)] %>%
  .[start <= '2019-01-01' & ending >= '2021-03-31'] %>% 
  .[,unique(ticker)]


```


##### DSF,MSF

Daily / monthly stock files

```{r}
dsf = tbl(con, in_schema('crsp_a_stock', 'dsf'))
dsfhdr = tbl(con,'dsfhdr')
```


Merge identifier (permno to cusip and ticker)

```{r}
dsenames = tbl(con, in_schema('crsp_a_stock', 'dsenames')) %>% collect %>% setDT


dsf2[,date_copy := date]
dsf2 = dsenames[,.(permno, ticker, namedt,nameendt)][dsf2, on = .(permno = permno, namedt <= date_copy, nameendt >= date_copy)][,.(permno,ticker,date, ret,retx)]
setnames(dsf2, c('PERMNO','TICKER','date','RET','RETX'))

```




```{r}


query = dbSendQuery(con,
            "
            select a.*, b.date, b.ret
            from crsp.msp500list as a,
            crsp.msf as b
            where a.permno=b.permno
            and b.date >= a.start and b.date<= a.ending
            and b.date>='01/01/2000'
            order by date;
            ")
sp500 = dbFetch(query)
sp500


```

#### Trace

```{r}
# connect to trace data
trace = tbl(con, 'trace')
# lazy query
query = trace %>%
  filter(year(trd_exctn_dt)<=2005) %>%
  group_by(trd_exctn_dt) %>% 
  filter(row_number() ==n()) # last observations for each group
# show example
query
# show query
query %>% show_query
# submit and retrieve
data = query %>% collect
```

#### COMPUSTAT

Daily short interest file

```{r}
comp = tbl(con, 'sec_shortint')
query = comp %>% head()
query
```


Fundamentals Annual (funda)

```{r}
funda = tbl(con, 'funda')
temp = funda %>% 
  # select(gvkey, tic, cusip, fyr, dvt) %>%
  head(10000) %>% collect
temp
```





#### TAQ

General information for TAQ on WRDS
https://wrds-www.wharton.upenn.edu/pages/support/manuals-and-overviews/taq/general/wrds-overview-taq/

Due to the very large size of the data, each year is located in a different physical directory: 
/wrds/taq.YYYY for Monthly and 
/wrds/nyse/taq_msecYYYY for Daily





##### NBBOM
Millisecond national best bid and offer (NBBO) file

```{r}
library(tictoc)
```

```{r}
nbbom = tbl(con, 'nbbom_20200102')
query = nbbom %>% 
  select(date, sym_root, sym_suffix, time_m, best_bid,best_bidsiz, best_ask, best_asksiz ) %>%
  filter(is.na(sym_suffix), hour(time_m) >= 9, hour(time_m)<=15) %>% 
  mutate(temp= floor(minute(time_m)/10)) %>%
  group_by(sym_root, hour(time_m), temp) %>%
  filter(row_number()==n())
query %>% head()
toc()
# 451.146 sec elapsed # including download & memory loading
# 371.793 sec elapsed
```


##### WCT

WRDS Consolidated Trades (WCT)

The NBBO's Bid and Ask midpoints are matched to each trade at seconds 0, -1, -2 and -5 relative to their trade time, and stored along with trades in the same datasets. 

These WRDS-generated trades files are labeled as `wct_YYYYMMDD`. This approach provides users with all necessary components to infer the trade directions, regardless of what specifications and assumptions are employed on the trade-quote lag or on trade filters. Please see the Lee and Ready research application for more information. Additionally the following article, "Matching TAQ Trades and Quotes in the Presence of Multiple Quotes", may be useful for understanding more about WCT files.



Millisecond stock trading volume and buy/sell indicator using Lee Ready algorithm


SAS code is here:
https://wrds-www.wharton.upenn.edu/pages/support/applications/microstructure-research/lee-and-ready-1991-algorithm/


```{r}
wct = tbl(con, 'wct_20200102')
query = nbbom %>% 
  select(date, sym_root, sym_suffix, time_m, best_bid,best_bidsiz, best_ask, best_asksiz ) %>%
  filter(is.na(sym_suffix), hour(time_m) >= 9, hour(time_m)<=15) %>% 
  mutate(temp= floor(minute(time_m)/10)) %>%
  group_by(sym_root, hour(time_m), temp) %>%
  filter(row_number()==n())
query %>% head()
toc()
```






#### OptionMetrics


`optionmnames` :

Includes 
  1) secid, symbol, optionid, root, suffix, effect_date, cusip, ticker, class, issuer, issue
  
```{r}
opnames = tbl(con,in_schema('optionm_all', 'optionmnames'))
sample2 = opnames %>% head(100) %>% collect
sample2
```


```{r}
opnames %>% filter(optionid == 130940651) %>% collect
print(opnames)
```

To match, should use *'secid' and 'effect_date'*




`opprcd` : Option Prices data

includes 
  1) option price
  2) delta gamma open_interest implied_vol vega theta
  

```{r}
opprcd = tbl(con, in_schema('optionm_all','opprcd2020')) # It includes year
sample = opprcd %>% head(100) %>% collect
sample

```


`secprd2020` : Underlying Security price data by year

includes
  secid symbol optionid root  suffix effect_date cusip ticker class issuer issue 

```{r}
secprd2020 = tbl(con, in_schema('optionm_all','secprd2020'))
print(secprd2020)
```









### RPostgres

#### Connection

```{r}
library(RPostgres)
wrds <- dbConnect(Postgres(),
                  host='wrds-pgdata.wharton.upenn.edu',
                  port=9737,
                  dbname='wrds',
                  sslmode='require',
                  user='mgson633')

```

#### Query

Query is done through combinations of query + fetch + close

1.  `dbSendQuery()` : Send query to existing connection, `wrds`
2.  `dbFetch()` : Fetch data from resulting response `n` : number of rows fetching
3.  `dbClearResult(res)` : closes connection (response)


#### All libraries

All data libraries available at WRDS


```{r}
res = dbSendQuery(wrds, "select distinct table_schema
                   from information_schema.tables
                   
                   order by table_schema")

temp = dbFetch(res)
dbClearResult(res)
temp
```



#### All datasets within library

```{r}
res = dbSendQuery(wrds, 
                   "
                   select distinct table_name
                   from information_schema.columns
                   where table_schema='taqm_2021'
                   order by table_name
                   "
                   )
temp = dbFetch(res, n=-1)
dbClearResult(res)
temp %>% View
```

#### All columns within dataset

```{r}
res = dbSendQuery(wrds, 
                   "
                   select column_name
                   from information_schema.columns
                   where table_schema='optionm'
                   and table_name='opvold'
                   order by column_name
                   ")
temp = dbFetch(res, n=-1)
temp
dbClearResult(res)
```

Check the table

```{r}
res = dbSendQuery(wrds, 
                   "
                   select *
                   from taqm_2011.ctm_20110401
                   limit 1000
                   ")
temp = dbFetch(res, n=-1)
dbClearResult(res)
temp %>% View


res = dbSendQuery(wrds, 
                   "
                   select *
                   from taqmsec.ctm_20201201
                   limit 1000
                   ")
temp = dbFetch(res, n=-1)
dbClearResult(res)
temp %>% View
```

#### PostgreSQL
##### Wildcards

`%` matchs any numbers of characters
`_` matchs only ne character

##### IN, BETWEEN, LIKE

```{r}
  SELECT * 
  FROM baby_names
  WHERE 
  state IN('CA', 'NY', 'TX') and
  state NOT IN('FL', 'GA') and
  year BETWEEN 2010 and 2014 and
  name LIKE 'J%n'
  
```

##### Programmatic Query

```{r}
mytext = 'taqmsec.cqm_20201201'
symbols = c('AAPL','AA')
symbols1 = shQuote(symbols) %>% str_c(collapse = ',')
res = dbSendQuery(wrds, 
                  str_glue(
                  "
                  SELECT sym_root as symbol, date, time_m, bid, bidsiz, ask, asksiz
                  FROM {mytext}
                  WHERE 
                  bid !=0 and 
                  ask !=0 and 
                  sym_root IN ({symbols1})
                  LIMIT 1000
                  "
                  ))
```



##### Dates and Times

Use `extract` function to extract year/month/hour etc.
```{r}
res = dbSendQuery(wrds, 
                   "
                   SELECT cusip_id, bond_sym_id, trd_exctn_dt
                   FROM trace.trace_btds144a
                   WHERE extract(year from trd_exctn_dt) <= 2005
                   LIMIT 100
                   "
                   )
```




##### TRACE examples

TRACE btds query

```{r}
res = dbSendQuery(wrds, 
                  "
                  SELECT *
                  FROM trace.trace
                  WHERE extract(year from trd_exctn_dt) <= 2005
                  LIMIT 100
                  "
)

trace2002.2005 = dbFetch(res)
dbClearResult(res)

```

TRACE btds 144a query

```{r}
res = dbSendQuery(wrds, 
                   "
                   SELECT cusip_id, bond_sym_id, trd_exctn_dt
                   FROM trace.trace_btds144a
                   WHERE extract(year from trd_exctn_dt) <= 2005
                   LIMIT 100
                   "
                   )

trace_btds2002.2005 = dbFetch(res)
```

##### Compustat examples

Compustat fundamental annual

```{r}
res = dbSendQuery(wrds,
                  "
                  select gvkey, datadate, fyear,cusip, at,ch,mkvalt,revt,xint,oibdp,ppegt
                  from compa.funda
                  where
                  consol ='C' and
                  indfmt = 'INDL' and
                  datafmt = 'STD' and
                  popsrc = 'D' and
                  curcd = 'USD'
                  "
                  )
```

Compustat fundamental quarterly

```{r}
res = dbSendQuery(wrds,
                  "
                  select *
                  from comp.fundq
                  where
                  consol ='C' and
                  indfmt = 'INDL' and
                  popsrc = 'D' and
                  datafmt = 'STD' and
                  curcdq = 'USD'
                  "
)
```

Joining two compustat

```{r}

res <- dbSendQuery(wrds, 
                   "select a.gvkey, a.datadate, a.tic,
                   a.conm, a.at, a.lt, b.prccm, b.cshoq
                   from comp.funda a join comp.secm b
                   on a.gvkey = b.gvkey
                   and a.datadate = b.datadate
                   where a.tic = 'IBM'
                   and a.datafmt = 'STD'
                   and a.consol = 'C'
                   and a.indfmt = 'INDL'")
data = dbFetch(res, n = -1)
dbClearResult(res)
data
```

##### CRSP examples

CCM link table

```{r}
res = dbSendQuery(wrds,"
                  select * 
                  from crsp.ccmxpf_lnkhist
                  ")
crspa.link = dbFetch(res)
crspa.link
```

CRSP msenames

```{r}
res = dbSendQuery(wrds,"
                  select * 
                  from crsp.msenames
                  ")
temp = dbFetch(res, n=1000)
temp
```

CRSP daily stock file

```{r}
res <- dbSendQuery(wrds,
                  "select *
                  from crsp.dsf
                  ")

```

CCM Linktable SAS Query

```{r}
# Code from Phil

# 1. Merge permno on Compustat using linktable

# proc sql;
#   create table comp2 as
#   select a.*, b.lpermno as permno
#   from key_control as a left join crsp.ccmxpf_lnkhist as b
#   on a.gvkey_string=b.gvkey and b.linkprim in ('P', 'C') and
#   b.LINKTYPE in ('LU', 'LC') and
#   a.datadate >= b.LINKDT and (a.datadate <= b.LINKENDDT or missing(b.LINKENDDT))
#   order by gvkey, datadate;
# quit;
# 
 
# 2. Merge ncusip and others on Compustat using msenames

# proc sql;
#       create table comp3
#       as select a.*, b.ncusip, b.COMNAM,b.siccd, b.ticker as ticker_crsp
#       from comp2 as a left join (select distinct ncusip, COMNAM, permno, namedt, nameendt, ticker, siccd from crsp.msenames
#     where not missing(ncusip)) as b
#       on a.permno = b.permno and b.namedt<=a.datadate<=b.nameendt;
# quit;


# rsubmit;
# Data funda;
# Set comp.funda;
# Where at~=. Or at>=0;
# where sale~=. or sale>=0;
# where ebitda~=.;
# Keep gvkey datadate;
# Run;
# 
# proc sql; 
#   create table getf_3 as 
#   select a.*, b.lpermno as permno_all
#   from funda a left join crsp.ccmxpf_linktable b 
#     on a.gvkey eq b.gvkey 
#     and b.lpermno ne . 
#     and b.linktype in ("LC" "LN" "LU" "LX" "LD" "LS") 
#     and b.linkprim IN ("C", "P")  
#     and ((a.datadate >= b.LINKDT) or b.LINKDT eq .B) and  
#        ((a.datadate <= b.LINKENDDT) or b.LINKENDDT eq .E)   ; 
# quit;
# 
# proc sql;
#       create table getf_4
#       as select a.*, ncusip as ncusip_all 
#       from getf_3 as a left join (select distinct ncusip, COMNAM, permno, namedt, nameendt, ticker  from crsp.msenames
#     where not missing(ncusip)) as b
#       on a.permno_all = b.permno and b.namedt<=a.datadate<=b.nameendt;
# quit;
# 
# proc download data=getf_4 out=getf_4;run;
# endrsubmit;
#         
# proc sql;
#   create table comp2 as
#   select a.*, b.lpermno as permno
#   from key_control as a left join crsp.ccmxpf_lnkhist as b
#   on a.gvkey_string=b.gvkey and b.linkprim in ('P', 'C') and
#   b.LINKTYPE in ('LU', 'LC') and
#   a.datadate >= b.LINKDT and (a.datadate <= b.LINKENDDT or missing(b.LINKENDDT))
#   order by gvkey, datadate;
# quit;
# 
# 
# 
# proc sql;
#       create table comp3
#       as select a.*, b.ncusip, b.COMNAM,b.siccd, b.ticker as ticker_crsp
#       from comp2 as a left join (select distinct ncusip, COMNAM, permno, namedt, nameendt, ticker siccd from crsp.msenames
#     where not missing(ncusip)) as b
#       on a.permno = b.permno and b.namedt<=a.datadate<=b.nameendt;
# quit;
# 
# rsubmit;
# Data funda;
# Set comp.funda;
# Where at~=. Or at>=0;
# where sale~=. or sale>=0;
# where ebitda~=.;
# Keep gvkey datadate;
# Run;
# 
# proc sql; 
#   create table getf_3 as 
#   select a.*, b.lpermno as permno_all
#   from funda a left join crsp.ccmxpf_linktable b 
#     on a.gvkey eq b.gvkey 
#     and b.lpermno ne . 
#     and b.linktype in ("LC" "LN" "LU" "LX" "LD" "LS") 
#     and b.linkprim IN ("C", "P")  
#     and ((a.datadate >= b.LINKDT) or b.LINKDT eq .B) and  
#        ((a.datadate <= b.LINKENDDT) or b.LINKENDDT eq .E)   ; 
# quit;
# 
# proc sql;
#       create table getf_4
#       as select a.*, ncusip as ncusip_all 
#       from getf_3 as a left join (select distinct ncusip, COMNAM, permno, namedt, nameendt, ticker  from crsp.msenames
#     where not missing(ncusip)) as b
#       on a.permno_all = b.permno and b.namedt<=a.datadate<=b.nameendt;
# quit;
# 
# proc download data=getf_4 out=getf_4;run;
# endrsubmit;

```


##### TAQ examples

```
libname scratch "/scratch/ufl/mgson633/";

%macro loop;

%let start_date=20200101;
%let end_date=20200331;
data scratch.quotes_&end_date; stop; run;

%do %while (&end_date >= &start_date);
	%let date_id = &start_date;
	
	%if %sysfunc(exist(taqmsec.cqm_&date_id)) %then %do;
	
		data quotes / view=quotes;
		set taqmsec.cqm_&date_id (  /* where=(sym_root in("AAPL", "MCD", "AACQ", "AAIC")) */  keep=sym_root sym_suffix date time_m bid bidsiz ask asksiz);
		if not missing(sym_suffix) then delete;
		time = floor(time_m/600);
		if (54 <= time <= 90) and bid NE 0 and ask NE 0 and bidsiz NE 0 and asksiz NE 0 then output;
		run;
	
		data quotes1 (drop=sym_suffix);
		set quotes;
		by sym_root time;
		where not missing(sym_root);
		if last.time then output; 
		run;
		
		data scratch.quotes_&end_date; set scratch.quotes_&end_date quotes1; run;
		
	%end;
	
%let start_date = %eval(&start_date+1);
%end;

%mend loop;

%loop;
```




National best bid and offer

```
data quotes;
	set taqmsec.nbbom_20200102 (  where=(sym_root in("AAPL", "MCD", "AACQ", "AAIC")) 
	keep=sym_root sym_suffix date time_m best_bid best_bidsiz best_ask best_asksiz);
	if not missing(sym_suffix) then delete;
	time = floor(time_m/600);
	if (54 <= time <= 90) then output;
run;
	
data quotes1 (drop=sym_suffix);
set quotes;
by sym_root time;
where not missing(sym_root);
if last.time then output; 
run;
```




## Batch Job on WRDS

Info

Job Limits
Both interactive and batch jobs have the following limits:

You can use up to 5 concurrent jobs (10 Core Maximum), and may queue any additional jobs. Additional jobs will wait in the queue until active jobs have finished.
You can use up to 150GB memory, and may queue any additional jobs. Additional jobs will wait in the queue until your current total memory allocation drops below the limit.
Your subscribing institution can run up to 10 concurrent jobs across all users who are members of that institution, and may queue any additional jobs. Additional jobs will wait in the queue until active jobs have finished.
Each active batch job may run for one week (168 hours). Each active interactive job may run for 3 days (72 hours). Jobs running after the limit has elapsed will be automatically terminated.
Batch jobs additionally have the following CPU and RAM usage restrictions:

By default, a WRDS Cloud job is assigned to 2 cores (for a total of 4 threads) per job.
Users may request up to 8 cores (for a total of 16 threads) per job.
By default, a WRDS Cloud job is assigned 8GB per core (for a total of 16GB if left at the default core count).
Users may request up to 48GB RAM total per job (calculated as RAM * cores requested).



Connect to wrds cloud system

    ssh mgson633@wrds-cloud.wharton.upenn.edu

For interactive R jobs

    qrsh \# connect to the WRDS interactive node R --no-save --no-restore

### Batch job SAS example

When sourcing sas file directly

```
qsas my_sample.sas
```


When sourcing shell script

```
qsub script.sh
```



    #!/bin/bash
    #$ -cwd
    #$ -m abe
    #$ -M gunsu.son@ufl.edu
    #$ -pe onenode 8
    #$ -l m_mem_free=6G
    
    sas my_program.sas


### Batch job R example

Maximum example 

    #!/bin/bash
    #$ -cwd
    #$ -m abe
    #$ -M gunsu.son@ufl.edu
    #$ -pe onenode 8
    #$ -l m_mem_free=6G

    cd /home/ufl/mgson633/Projects/Fund_centrality/codes
    
    R CMD BATCH run_batch2_17-21.R 17-21.out




















Above MAXIMUM for 1 job: 8 core, 48 GB ram

  return log back to cwd, give email notification,
  8 core, 6g mem per core.


Default batch jobs:
  CPU Cores: 2 per job
  RAM Usage: 16GB per job
  
  
HARD LIMIT :
  5 jobs with 150G Memory / 10 cores maximum


### Submit batch job

    qsub my_program.sh
    qsub -q all.q@wrds-sas24-h my_program.sh 

### Managing jobs

Show all currently-running and queued jobs submitted by all users

    qstat -u \* 

Show all compute nodes, including number of processors and amount of RAM per node

    qhost
    qhost -j 

Delete your job #1234567

    qdel 1234567 





### request more computing power

<https://wrds-www.wharton.upenn.edu/pages/support/the-wrds-cloud/running-jobs/batch-jobs-wrds-cloud/>

Requesting Additional CPU / RAM By default, the WRDS Cloud imposes the following limitations on resource consumption (CPU and RAM) to maintain a fair use environment for all WRDS users:

batch jobs: CPU Cores: 2 per job RAM Usage: 16GB per job interactive jobs: CPU Cores: 1 per job RAM Usage: 8GB per job

If you need more than this for your job, you may request more up to the following hard limits:

CPU Cores: 8 per job RAM Usage: 48GB Total per Job

    \#!/bin/bash 
    \#\$ -cwd 
    \#\$ -pe onenode 8 
    \#\$ -l m_mem_free=6G






## HiPerGator


Log-in nodes : limit 16 cores, 64GB ram, 10 mins
  
  Do not run programs on login nodes

HiperGator 1 : 4 sockets, 16cores each - 64 cores on node , 250GB ram (retired)
HiperGator 2 : 2 sockets, 16cores each - 32 cores on node , 120GB ram
HiperGator 3 : 8 sockets, 16cores each - 128 cores on node, 1024GB ram


More info : `nodeInfo`


-   Home Area: /home/<user>

    -   40GB limit
    -   scripts, code, small data
    -   Do NOT use for job input/output

-   Blue storage : /blue/<group>/<user> (hypergator 3.0) : /ufrc/<group>/<user> is 2.0 ver

    -   ALL input/output from jobs should go here
    -   /blue/nimalendran/matthewson

### Base Info

To see running jobs

    sacct

slurm info

    slurmInfo
    
To browse all hypergator nodes specifiction : NodeInfo

    nodeInfo

show QoS info

    showQos nimalendran
    showQos nimalendran-b

To check the disk quota for blue
Currently we have 4TB quota for blue storage

    blue_quota



### Connection & ID

Connecting from Mac (Connect to Log in nodes )

    ssh <username>@hpg.rc.ufl.edu
    
Check group membership

    id
    
    uid=3670(gunsu.son) gid=3446(nimalendran) groups=3446(nimalendran)


Check Association of user

    showAssoc gunsu.son
    

### Connect 

#### connect to development server

From log in node to development, 60 mins

    srundev -t 60

now node is from login to development

Instead, interactive on SLURM (connect to compute server) is another option, see below SLURM INTERACTIVE `srun`

#### connect to to compute server

    srun --partition=hpg-default --ntasks=1 --cpus-per-task=128 --qos=nimalendran-b --nodes=1 --mem=1024gb --time=96:00:00 --mail-type=END,FAIL --mail-user=gunsu.son@ufl.edu --pty bash -i



### RStudio Server

#### Setup

1. Installation


Take a look at the server info:
```
lsb_release -a
```

HPG is RedHatEnterpriseServer
Release is 7.7

RedHat/CentOS 7 is the right one for HPG.

```
wget https://download2.rstudio.org/server/centos7/x86_64/rstudio-server-rhel-1.4.1717-x86_64.rpm
sudo yum install rstudio-server-rhel-1.4.1717-x86_64.rpm
```


2. User Setup







### Run R 

    module load R
    R




Check computing allocation of group

    slurmInfo nimalendran
    sacct
    
    showQos nimalendran
    
    

SFTP Setup

  - port 22
  - hpg.rc.ufl.edu



### SLURM


#### SLURM INTERACTIVE

You can request resources for an interactive session (i.e. job) and start a command shell (bash, for example). Within that command shell you will have access to the resources you requested and can run whatever commands and processes you wish. Consider the example below. It will give you 4 GB of memory for 8 hours on a real compute host and present you, once the job starts, with an interactive command shell (bash). From that shell you can run commands and launch processes just as you would from any other host (login or otherwise).


QOS

    
    srun --partition=hpg2-compute --ntasks=1 --cpus-per-task=24 --qos=nimalendran --nodes=1  --mem-per-cpu=3gb --time=08:00:00 --pty bash -i

QOS-b

Hypergator 2.0 : 1 node max = 32 cpu (2 socket 16 cores each) 125GB ram

    srun --partition=hpg2-compute --ntasks=1 --cpus-per-task=32 --qos=nimalendran-b --nodes=1  --mem-per-cpu=3gb --time=08:00:00 --pty bash -i



Hypergator 3.0 : 1 node max = 128 cpu (8 socket 16 cores each) 1TB ram 
Check with `nodeInfo`

    srun --partition=hpg-default --ntasks=1 --cpus-per-task=128 --qos=nimalendran-b --nodes=1 --mem-per-cpu=7gb --time=72:00:00 --pty bash -i

Hypergator 3.0 : 1 node 100 cpu 600 GB ram

    srun --partition=hpg-default --ntasks=1 --cpus-per-task=100 --qos=nimalendran-b --nodes=1 --mem-per-cpu=6gb --time=08:00:00 --pty bash -i
    




    srun --partition=hpg-default --nodes=10 --ntasks-per-node=1 --cpus-per-task=12 --qos=nimalendran-b --mem-per-cpu=6gb --time=08:00:00 --pty bash -i

    
Suppose you need 16 cores. Here are some use cases:

Processes : think of a new terminal, or R instances

you use mpi and do not care about where those cores are distributed: --ntasks=16
you want to launch 16 independent processes (no communication): --ntasks=16
you want those cores to spread across distinct nodes: --ntasks=16 and --ntasks-per-node=1 or --ntasks=16 and --nodes=16
you want those cores to spread across distinct nodes and no interference from other jobs: --ntasks=16 --nodes=16 --exclusive
you want 16 processes to spread across 8 nodes to have two processes per node: --ntasks=16 --ntasks-per-node=2

you want 16 processes to stay on the same node: --ntasks=16 --ntasks-per-node=16
you want one process that can use 16 cores for multithreading: --ntasks=1 --cpus-per-task=16
you want 4 processes that can use 4 cores each for multithreading: --ntasks=4 --cpus-per-task=4


Slurm complicates this, however, by using the terms `core` and `cpu` interchangeably depending on the context and Slurm command. 
--cpus-per-tasks= for example is actually specifying the `number of cores` per task.



#### Submit batch jobs

Save .sh script and run the script file with `sbatch job_file.sh`

    #!/bin/bash
    #SBATCH --job-name=slurm_r_similarity      # Job name
    #SBATCH --mail-type=END,FAIL         # Mail events (NONE, BEGIN, END, FAIL, ALL)
    #SBATCH --mail-user=gunsu.son@ufl.edu    # Where to send mail.  Set this to your email address
    #SBATCH --ntasks=1                  # Number of MPI tasks (i.e. processes)
    #SBATCH --cpus-per-task=64            # Number of cores per MPI task (multithreading)
    #SBATCH --qos=nimalendran-b         # QOS
    #SBATCH --nodes=1                    # Maximum number of nodes to be allocated
    #SBATCH --ntasks-per-node=1         # Maximum number of tasks on each node
    #SBATCH --ntasks-per-socket=1        # Maximum number of tasks on each socket
    #SBATCH --distribution=cyclic:cyclic # Distribute tasks cyclically first among nodes and then among sockets within a node
    #SBATCH --mem-per-cpu=3Gb          # Memory (i.e. RAM) per processor
    #SBATCH --time=24:00:00              # Wall time limit (days-hrs:min:sec)
    #SBATCH --output=R_run_log.log     # Path to the standard output and error files relative to the working directory
    #SBATCH --partition=hpg2-compute   # To which partition
    
    
    cd /blue/nimalendran/gunsu.son/projects/code
    
    
    module load R
    
    Rscript --vanilla slurm_work.R
  


    
to check the queue : my job running on the server

    squeue -u username

to cancel job

    scancel job_id_number

historical info about jobs

    sacct


##### Multiple batch jobs in parallel


You could ask for 29 tasks, 1 cpu per task (you will get from 29 cpus on a node to 1 cpu in 29 different nodes), and in the slurm script you should start your calculus with srun, telling srun to allocate one task/cpu per chunk.

```{sh eval=F}
#SBATCH --ntasks=29
#SBATCH --cpus-per-task=1

for n in {1..29}
do
    srun -n 1 <your_script> $n &
done
wait
```



#### MPI jobs

To run MPI script, must use `srun`

    srun --mpi=pmix_v2 myApp


You must set --ntasks=1, and then set --cpus-per-task to the number of OpenMP threads you wish to use.

1 node and multiple cores (say 64 cores with group-b QOS)


Maximum 32 workers and workers has 1 core, 2 nodes (server computers), each nodes can have 16 max workers. 
Since server computers have two sockets and each socket has 16 cores, setting ntasks per socket to be half of that for fun, 8

    #SBATCH --ntasks =32
    #SBATCH --cpus-per-task=1
    #SBATCH --nodes =2
    #SBATCH --ntasks-per-node=16
    #SBATCH --ntasks-per-socket=8
    #SBATCH --constraint=infiniband



Other example

Using 4 server computer nodes. Each node can do 4 tasks in parallel (ntasks per node). Each task can have 4 workers (cores). 

    #SBATCH --ntasks =16
    #SBATCH --cpus-per-task=4
    #SBATCH --nodes =4
    #SBATCH --ntasks-per-node=4
    #SBATCH --ntasks-per-socket=2
    #SBATCH --constraint=infiniband


--ntasks=# : Number of "tasks" (use with distributed parallelism).

--ntasks-per-node=# : Number of "tasks" per node (use with distributed parallelism).

--cpus-per-task=# : Number of CPUs allocated to each task (use with shared memory parallelism).


Q: if every node has 24 cores, is there any difference between these commands?

sbatch --ntasks 24 [...]
sbatch --ntasks 1 --cpus-per-task 24 [...]


Answer:

Yes there is a difference between those two submissions. 
You are correct that usually ntasks is for mpi and cpus-per-task is for multithreading, but let’s look at your commands:



For your first example, the sbatch --ntasks 24 […] will allocate a job with 24 tasks. These tasks in this case are only 1 CPUs, but may be split across multiple nodes. So you get a total of 24 CPUs across multiple nodes.

For your second example, the sbatch --ntasks 1 --cpus-per-task 24 [...] will allocate a job with 1 task and 24 CPUs for that task. Thus you will get a total of 24 CPUs on a single node.

In other words, **a task cannot be split across multiple nodes**. 
Therefore, using --cpus-per-task will ensure it gets allocated to the same node, while using --ntasks can and may allocate it to multiple nodes.

Suppose you need 16 cores. Here are some use cases:

you use mpi and do not care about where those cores are distributed: --ntasks=16
you want to launch 16 independent processes (no communication): --ntasks=16
you want those cores to spread across distinct nodes: --ntasks=16 and --ntasks-per-node=1 or --ntasks=16 and --nodes=16
you want those cores to spread across distinct nodes and no interference from other jobs: --ntasks=16 --nodes=16 --exclusive

--exclusive     ensures srun uses distinct CPUs for each job step

you want 16 processes to spread across 8 nodes to have two processes per node: --ntasks=16 --ntasks-per-node=2
you want 16 processes to stay on the same node: --ntasks=16 --ntasks-per-node=16
you want one process that can use 16 cores for multithreading: --ntasks=1 --cpus-per-task=16
you want 4 processes that can use 4 cores each for multithreading: --ntasks=4 --cpus-per-task=4


https://login.scg.stanford.edu/faqs/cores/







#### H2O on SLURM

    module load R
    R
    
https://docs.h2o.ai/h2o/latest-stable/h2o-docs/faq/clusters.html#how-can-i-create-a-multi-node-h2o-cluster-on-a-slurm-system


```{r}
library(stringr)
# Check on the nodes we have access to.
p
cat("SLURM nodes:", node_list, "\n")

# Loop up IPs of the allocated nodes.
if (node_list != "") {
  id = str_match(node_list, '(.*)\\[(.*)\\]')[1,2]
  numbers = strsplit(str_match(node_list, '(.*)\\[(.*)\\]')[1,3], ',')
  nodes = strsplit(node_list, ",")[[1]]
  ips = rep(NA, length(nodes))
  for (i in 1:length(nodes)) {
    args = c("hosts", nodes[i])
    result = system2("getent", args = args, stdout = T)
    # Extract the IP from the result output.
    ips[i] = sub("^([^ ]+) +.*$", "\\1", result, perl = T)
  }
  cat("SLURM IPs:", paste(ips, collapse=", "), "\n")
  # Combine into a network string for h2o.
  network = paste0(paste0(ips, "/32"), collapse=",")
  cat("Network:", network, "\n")
}

# Specify how many nodes we want h2o to use.
h2o_num_nodes = length(ips)

# Options to pass to java call:
args = c(
  # -Xmx30g allocate 30GB of RAM per node. Needs to come before "-jar"
  "-Xmx30g",
  # Specify path to downloaded h2o jar.
  "-jar ~/software/h2o-latest/h2o.jar",
  # Specify a cloud name for the cluster.
  "-name h2o_r",
  # Specify IPs of other nodes.
  paste("-network", network)
)
cat(paste0("Args:\n", paste(args, collapse="\n"), "\n"))

# Run once for each node we want to start.
for (node_i in 1:h2o_num_nodes) {
  cat("\nLaunching h2o worker on", ips[node_i], "\n")
  new_args = c(ips[node_i], "java", args)
  # Ssh into the target IP and launch an h2o worker with its own
  # output and error files. These could go in a subdirectory.
  cmd_result = system2("ssh", args = new_args,
                       stdout = paste0("h2o_out_", node_i, ".txt"),
                       stderr = paste0("h2o_err_", node_i, ".txt"),
                       # Need to specify wait=F so that it runs in the background.
                       wait = F)
  # This should be 0.
  cat("Cmd result:", cmd_result, "\n")
  # Wait one second between inits.
  Sys.sleep(1L)
}


# Wait 3 more seconds to find all the nodes, otherwise we may only
# find the node on localhost.
Sys.sleep(3L)

# Check if h2o is running. We will see ssh processes and one java process.
system2("ps", c("-ef", "| grep h2o.jar"), stdout = T)

suppressMessages(library(h2oEnsemble))

# Connect to our existing h2o cluster.
# Do not try to start a new server from R.
h2o.init(startH2O = F)

#################################

# Run H2O commands here.

#################################
h2o.shutdown(prompt = F)
```

# UNIX SHELL

## Head

```{r}
terminalExecute('head -n 5 data.csv')
```

## Word count

`wc`


## List files 

```{r}
ls folder
```

list files with pattern matching - using * (any)

```{r}
ls folder/regex
```



## Move files

```{r}
terminalExecute()
```


```{r}
mv fromPath toPath
```

with pattern : use regex


```{r}
mv crust.etcMC* /home/out
```



# LaTeX

## TinyTex

<https://yihui.org/tinytex/>

Install TinyTex

```{r}
install.packages('tinytex')
tinytex::install_tinytex()
```

In terminal, make the folder writable

    sudo chown -R `whoami`:admin /usr/local/bin



## BibTex

Put this codebox on top of the code to identify the workding folder



```{r}
#Set Environment Variables
TEXINPUTS="~/Dropbox (UFL)/My Projects/Fund_centrality/code/latex" #Path to tex file in Windows

Sys.setenv(TEXINPUTS="~/Dropbox (UFL)/My Projects/Fund_centrality/code/latex", BIBINPUTS=TEXINPUTS,BSTINPUTS=TEXINPUTS) 

#Path to texfiles in Windows, set BIB files and BST files the same

#Run before clicking "Compile PDF"
```


### package install

Install missing packages from log file :

```{r}
# put log file location
tinytex::parse_install('latex/beamer/uf_theme.log')
```


Search package name by filename

```{zsh}
tlmgr search --global --file "/authblk.sty"
```


Then the package name one level upper folder : `preprint`

```{zsh}
tlmgr install preprint
```


Usually the filename identical to the package name. 

```{r}
tinytex::tlmgr('install ae grfext')
```


Or within the terminal

    tlmgr install ae grfext





## Sweave

Put this

    \\SweaveOpts{concordance=TRUE}





## Knitr

Use package PatchSynctex for concordance, and setting working directory for .bib recognition.


    \documentclass[11pt]{article}
    
    <<Setup, eval=T, echo=FALSE, include = FALSE, results='hide'>>=
    opts_knit$set(concordance=TRUE, self.contained=TRUE)##$
    require(patchSynctex)
    setwd('~/Dropbox (UFL)/My Projects/Fund_centrality/code/latex')
    Sys.setenv(TEXINPUTS=getwd(),
    BIBINPUTS=getwd(),
    BSTINPUTS=getwd())
    @
    
    \usepackage[margin=1in]{geometry} % for margin set
    \usepackage{setspace} % for double spacing
    \usepackage{hyperref} % hyperlinks




## Stargazer

Descriptive statistics table

```{r}
stargazer::stargazer(iris, 
                     type='text', 
                     title ='Descriptive stat, iris',
                     notes = 'This is note of the table.')
```



T statistics instead of standard error


```{r}
trace(stargazer:::.stargazer.wrap, edit = T)
# .format.t.stats.left <- "t = " and .format.t.stats.right <- "" modify this, line at (7050~7060)
```


My example of regression table

```{r}
stargazer(list(spec1,spec2,spec3,spec4,spec5,spec6), 
          # dep.var.caption = 'Dependant Variables',
          # initial.zero = F
          # type='text',
          
          report='vc*t',
          label = 'tab:table3',
          digits=3,
          title = "Holding Distinctiveness and Alpha",
          dep.var.labels = c('$Alpha_{t}$','$Alpha_{t+1}$'),
          covariate.labels = c('Centraltity','Concentration','ln(Size)','Age','Expense Ratio','Turnover Ratio','Flow','Centrality * Concentration'),
          add.lines = list(c("Fund Fixed Effect", "Yes","Yes","Yes","Yes","Yes","Yes"),
                           c("Month Fixed Effect", "Yes","Yes","Yes","Yes","Yes","Yes"),
                           c("Fund SE Cluster", "Yes","Yes","Yes","Yes","Yes","Yes")),
          omit.stat = c('rsq','ser'), # omit rsquared, standard error residual
          omit.table.layout = "n" # omit notes below 
          ) 
```





# Git (Github)

Default git installation (MacOS)

```
/usr/bin/git
```

Git : version control of folder and all files

Staging area : waiting stage that will be committed finally in later

Git has a staging area in which it stores files with changes you want to save that haven't been saved yet.


## Basic Commands

### status
Status of git (which files are modified but not not staged for commitment?, or which files are staged to be committed?)

```{zsh}
git status
```

### diff
Show the modified part of the files 

```{zsh}
# See what changes have made
git diff # show all changes 
git diff file1 # comapre previous filname1 and revised filename1 

git diff -r HEAD # -r : compare to particular revision, HEAD : the most recent commit
git diff -r HEAD~1 # HEAD~1 refers to a commit before most recent commit

# compared to the most recent commitment, show all changes 
git diff -r HEAD file1 # compare file1 with most recent commitment

```

Show the difference between two commits

```{zsh}
git diff ID1..ID2 # two hash IDs that are connected through dotdot
git diff abc123..def456
git diff HEAD..HEAD~2
```


### add

Add files to be staged (modifications to be done)

```{zsh}
# Add files to the staging area so that it may be committed later
git add file1
```

When another change in file1 was done, then it has to be staged again to be committed

### reset

When unstaging should be done, use `git reset`.

```{zsh}
git reset # unstage all
git reset HEAD # same

git reset file1 # undo staging file1 
git reset HEAD file1 # same
git reset HEAD -- file1 # same
```

### checkout

Checkout is like loading the (specific) saved version, discarding new changes made, while committing is saving the current version.
Undo the changes that you made since the last commit.
When modified file(s) should not be staged, use `git checkout`. 

When modify a file but would not want to stage the modifications made, use git checkout
```{zsh}
git checkout file1 # remove unstaged modification and load the last committed version
git checkout -- file1 # be careful that -- should be spaced between 
```


It can be used to go back further into the file's history. 
```{zsh}
git checkout hash file1 # load the specific history version of the file, removing changes that were made after this.

```


### commit
Making commitment to the github

```{zsh}

git commit # commit it
git commit -m "Commitment Message" # add commitment message to it
git commit --amend -m "Amend the message I just committed" # to amend the commit just made
```


### log

Browsing the log of git

```{zsh}
git log # show the most recent git committments with commitment messages. The most recent one is on the top
git log file1 # show only git commits related to the file1
git log -2 file1 #show most recent 2 commits of file1
```



### show
Browse specific commit by its hash

```{zsh}
git show someHash #View specific commit by its hash, with some initial characters of hash
```

See the change of the file in detail: who changed when, what in a file
```{zsh}
git annotate file1
```


### remove

Clean (remove) files that are not staged
```{zsh}
git clean -n # list of files that are in the repository, but untracked - currently it does not have any history.
git clean -f # delete those untracked file in the repository
```


## Ignore files

generate .gitignore file to make exceptions


To create list of files that git should ignore : generate .gitignore file such as

    foldername
    *.mpl


## Config

Show config setting 

```{zsh}
git config --list # lists all the custom configs
git config --list --local # for the specific project (repo)
git config --list --global # for every projects
git config --list --system # for the system, every user of this computer
```

Set config

```{zsh}
# set name and email address globally
git config --global user.name John Doe # set user name as John Doe
git config --global user.email address@gmail.com # set user email address

```






# GmailR

http://console.developers.google.com/

and setup Gmail API & Credentials (OAuth -> get client ID and pwd in json file)

It is TESTing mode when created -> Change it into production mode.


## Install

```{r}
install.packages('gmailr')
```

## Usage

client_secret.json : downloaded from the google cloud platform console.

```{r}
library(gmailr)
gm_auth_configure(path = '~/Dropbox (UFL)/Data Workshop/R/client_secret.json')
?gm_auth_configure
```


Send email

```{r}
email_message = 
  gm_mime() %>% 
  gm_to('gsist156@gmail.com') %>% 
  # gm_from('mgson633@gmail.com') %>%  #doesn't work actually.
  gm_subject('Test message') %>% 
  gm_text_body('This is the body') %>% 
  gm_attach_file('~/Dropbox (UFL)/Data Workshop/R/R_CookBook.nb.html')

gm_send_message(email_message)
```

# WEB DATA

## Read CSV from web

```{r}
csv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv"
csv_data = read.csv(csv_url)
csv_data %>% head
```

## Download files

download.file() does the trick.

```{r}
csv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv"
download.file(url = csv_url, destfile = "feed_data.csv")

```

## Using API

The first thing to check : whether there's R package available for the website.

-   Google search CRAN [website address]

If no API wrapper found in CRAN then use below methods.

Package `httr` can be alternative of python's `BeautifulSoup`

### GET, POST

```{r}
library('httr')
# Get the url, save response to resp
url <- "http://www.example.com/"
resp <- GET(url)
# Print resp
resp

# Get the raw content of resp: raw_content
raw_content <- content(resp, as = "text") # Default => automatic JSON parsing -> list if JSON

# Print the head of raw_content
head(raw_content)
```

```{r}
# Make a POST request to http://httpbin.org/post with the body "this is a test"
post_result <- POST(url = 'http://httpbin.org/post',body = 'this is a test')
print(post_result)
```

### Page error handling

`http_error` function returns `TRUE` if the web page is broken.

```{r}
fake_url <- "http://google.com/fakepagethatdoesnotexist"

# Make the GET request
request_result <- GET(fake_url)

# Check request_result
if(http_error(request_result)){
	warning('The request failed')
} else {
	content(request_result)
}
```

### API

There are two types of URLs:

1.  Directory based URLs\
    This is very intuitive case, one can simply work on the string manipulation to work on this stuff.
2.  Parameter based URLs\
    One has to send query on the homepage, and `httr`'s `GET` function nicely does the job.

#### Directory based URL API

```{r}
# Construct a directory-based API URL to `http://swapi.co/api`,
# looking for person `1` in `people`
directory_url <- paste("http://swapi.co/api", "people", "1", sep = '/')

# Make a GET call with it
print(directory_url)
result <- GET(directory_url)
print(result)
```

#### Query based URL API

```{r}
# Create list with nationality and country elements
query_params <- list(nationality = 'americans', 
    country = 'antigua')
    
# Make parameter-based call to httpbin, with query_params
parameter_response <- GET('https://httpbin.org/get', query = query_params)

# Print parameter_response
print(parameter_response)
```

#### User agents

```{r}
url <- "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Aaron_Halfaker/daily/2015100100/2015103100"

# Add the email address and the test sentence inside user_agent()
server_response <- GET(url, user_agent("my@email.address this is a test"))
server_response
Sys.sleep(1) # sleep function
```

#### JSON to dataframe

`jsonlite` package, and `httr` packages `httr`'s `content` function uses `jsonlite` at the backend.

```{r}
# jsonlite use case
library(jsonlite)
# Definition of quandl_url
quandl_url <- "https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?auth_token=i83asDsiWUUyfoypkgMz"
# Import Quandl data: quandl_data
quandl_data <- fromJSON(quandl_url)
quandl_data # JSON is parsed into list object


# httr use case
library(httr)
response = GET(quandl_url)
content(response) # Also parsed into list object
```

```{r}
# use dplyr package
# Notice : content(resp_json) is list class
http_type(resp_json) # check if the response is json type
revs %>%
  bind_rows() # bind_rows function makes named list into dataframe

?http_type
```

#### XML to dataframe

```{r}
library('xml2')
rev_text = content(resp_xml, as = 'text') # as ='parsed' does the xml parsing but it is being explicit here
rev_xml = read_xml(rev_text)
# Examine the structure of rev_xml
xml_structure(rev_xml)

```
3

Now using XPath to find elements

```{r}
# Find all nodes using XPATH "/api/query/pages/page/revisions/rev"
xml_find_all(rev_xml, "/api/query/pages/page/revisions/rev")

# Find all rev nodes anywhere in document
rev_nodes <- xml_find_all(rev_xml, '//rev')

# Use xml_text() to get text from rev_nodes
xml_text(rev_nodes)
# All rev nodes
rev_nodes <- xml_find_all(rev_xml, "//rev")

# The first rev node
first_rev_node <- xml_find_first(rev_xml, "//rev")

# Find all attributes with xml_attrs()
xml_attrs(first_rev_node)

# Find user attribute with xml_attr()
xml_attr(first_rev_node, 'user')

# Find user attribute for all rev nodes
xml_attr(rev_nodes, 'user')

# Find anon attribute for all rev nodes
xml_attr(rev_nodes, 'anon')
```

## Web scraping

### RSelenium





#### Installation

##### Java Installation

Prerequisite: Java has to be installed, so that hitting `java` on terminal should work.
Installing openjdk from brew does not set the link well, so go to http://www.java.com and download (jre)
Or below code downloads the file & opens it

```{r}
download.file('https://javadl.oracle.com/webapps/download/AutoDL?BundleId=244576_d7fc238d0cbf4b0dac67be84580cfb4b', '~/Downloads/jre8.dmg')
system('~/Downloads/jre8.dmg')
```


##### Rselenium

First install RSelenium package
```{r}
install.packages('RSelenium')
```

Then download Selenium Server binary, which can be found here http://selenium-release.storage.googleapis.com/index.html
Look for selenium-server-standalone-x.xx.x.jar.

This command would do the trick for now.

```{r}
download.file('http://selenium-release.storage.googleapis.com/3.9/selenium-server-standalone-3.9.1.jar', '~/.R/selenium-standalone-3.9.1.jar')
```

For standalone server operation. I put that in `~/.R/` folder.

Run .jar file from console on mac using 

    java -jar ~/.R/selenium-standalone-3.9.1.jar

For Safari, enable developer menu > allow automation.
For Chrome, download proper version of chrome driver and locate it in `/usr/local/bin` which is already in the path(then it is done)


```{r}
driver_file_zipped = '~/Downloads/chromedriver_mac64.zip'
download.file('https://chromedriver.storage.googleapis.com/90.0.4430.24/chromedriver_mac64.zip', driver_file_zipped) 
# then move the file to the /usr/local/bin
```


##### Authrization problem

IF Chromedriver is not authorized app by Apple, so needs to be un-quarantined.
Navigate to chromedriver path (/usr/local/bin/) and 

    sudo xattr -d com.apple.quarantine /usr/local/bin/chromedriver 

(this worked for me)

If Apple cannot identify the developer still,

    spctl --add --label 'Approved' <name-of-executable>



#### Basic Commands

Start selenium server

On terminal,

```{zsh}
java -jar ~/.R/selenium-standalone-3.9.1.jar
```
Or by using rstudioapi package

```{r}
rstudioapi::terminalExecute('java -jar ~/.R/selenium-standalone-3.9.1.jar')
```




Run driver
```{r}
library(RSelenium)
remDr = remoteDriver(
  remoteServerAddr = "localhost",
  port = 4444L,
  browserName = "chrome"
)
```


Basic examples

```{r}
remDr$open() # fire up the browser

```






### rvest
with `rvest` package (similar to BeautifulSoup4 in python, for simple web scraping only)

```{r}
# Load rvest
library('rvest')

# Hadley Wickham's Wikipedia page
test_url <- "https://en.wikipedia.org/wiki/Hadley_Wickham"

# Read the URL stored as "test_url" with read_html()
test_xml <- read_html(test_url)

test_xml
```

```{r}
test_node_xpath = "//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"vcard\", \" \" ))]"
# Use html_node() to grab the node with the XPATH stored as `test_node_xpath`
node <- html_node(x = test_xml, xpath = test_node_xpath)

# Print the first element of the result
node[1]
```

Extracting with `rvest`

-   `html_text` get text contents
-   `html_attr` get specific attribute
-   `html_name` get tag name
-   `html_table` HTML table structures can converted to `data.frame`

```{r}
html_text(node)
html_name(node)
html_attr(node, 'class')
html_table(node)
```

### CSS selector

The major difference between XPath and CSS is that CSS finds all of the meeting condition.

CSS is great for finding objects with `id` or `class`.

To select a certain `class`, add `.`\
To select a certain `id`, add `#`

```{r}

# Select the table elements
html_nodes(test_xml, css = 'table')

# Select elements with class = "infobox"
html_nodes(test_xml, css = '.infobox')

# Select elements with id = "firstHeading"
html_nodes(test_xml, css = '#firstHeading')
```


# CUSTOM FUNCTIONS

## Descriptive Statistics

```{r}

colms = c(
  'Cust_vol','Proc_vol','MM_vol',
  'MID_QUOTE','BA_SPREAD','PCT_SPREAD','EFF_SPREAD','PEF_SPREAD',
  'IMPL_VOL','DELTA','GAMMA','THETA_E','VEGA_E'
  )

summary_stat = qsu(CBOE, cols =  colms)
summary_stat = summary_stat %>% as.data.table(keep.rownames = T) %>% setnames(., old='rn', new='Variable')


probs = CBOE[,lapply(.SD, quantile, na.rm =T, probs = c(0.01,0.25,0.5,0.75,0.99)), .SDcols = colms ]
probs = transpose(probs) %>% as.data.table

summary_stat = cbind(summary_stat, probs)
summary_stat = summary_stat[,.(Variable, N,Mean, SD, P1 = V1,P25=V2,P50=V3,P75=V4,P99=V5)]

```


advanced : groupby summary

```{r}

#### Summary (10 min) of Spread by moneyness  ----

# Summary of Spread and greeks by MONEYNESS
names(CBOE)
vol_cols = c('PCT_SPREAD','PEF_SPREAD', 'ABS_DELTA','GAMMA','THETA_E','VEGA_E', 'MONEYNESS2')


temp = qsu(CBOE[,.SD,.SDcols = vol_cols], by = ~MONEYNESS2) %>% as.data.table

probs = CBOE[,lapply(.SD, quantile, na.rm =T, probs = c(0.01,0.25,0.5,0.75,0.99)), .SDcols = vol_cols[vol_cols !='MONEYNESS2'] , keyby=MONEYNESS2 ]
probs[, stat := rep(c('P1','P25','P50','P75','P99'),3)]
probs = melt(probs, id.vars = c('MONEYNESS2','stat'))
setnames(probs, new = c('V1','V2','V3','value'))
temp = rbind(temp,probs)

temp[, V3 := factor(V3, levels = vol_cols)]
temp = dcast(temp, V1 + V3 ~ V2 , value.var = 'value')

temp = na.omit(temp)
setcolorder(temp, c('V1','V3','N','Mean','SD','Min','Max'))
setnames(temp, old = c('V1','V3'), new = c('Moneyness','Variable'))

temp %>% select(-Min,-Max)
```




## Summary Statistics

Package `collapse` provides very handy and fast summary stat tool.


```{r}

su = function(DT, quant = FALSE){
  require(data.table)
  require(tdigest)
  setDT(DT)
  
  types = lapply(DT, class) %>% unlist
  colnames = names(types)
  
  # include only numeric columns
  
  numeric_cols = colnames[sapply(types, function(x) x == 'numeric')]
  
  # N, mean, St.dev, Min, Median, Max
  
  N = DT[, lapply(.SD, function(x) sum(!is.na(x))), .SDcols =numeric_cols]
  Mean = DT[, lapply(.SD, mean, na.rm = T), .SDcols =numeric_cols]
  SD = DT[, lapply(.SD, sd, na.rm = T), .SDcols =numeric_cols]
  Min = DT[, lapply(.SD, min, na.rm = T), .SDcols =numeric_cols]
  Med = DT[, lapply(.SD, median, na.rm = T), .SDcols =numeric_cols]
  Max = DT[, lapply(.SD, max, na.rm = T), .SDcols =numeric_cols]
  
  if (quant != TRUE){
    result = as.data.table(t(rbindlist( list(N,Mean,SD,Min,Med,Max))), keep.rownames = T)
    setnames(result, c('Variable', 'N','Mean','St.Dev','Min','Median','Max'))
    return(result)
  } else {
    message('Working on Quantiles..')
    for (i in seq_along(numeric_cols)){
      td = tdigest(DT[, get(numeric_cols[i])])
      quan = tquantile(td, c(0.1,0.25,0.75,0.9))
      names(quan) = c('P10','P25','P75','P90')
      
    }
  }
  
}


```


## Winsorize

From package `DescTools`

```{r}
library(DescTools)

# my winsor function
ms_winsor = function(x, min=0.01, max = 0.99){
  winsored_vec = Winsorize(x, minval = quantile(x, min, na.rm=T), maxval = quantile(x, max, na.rm=T), probs = c(min,max), na.rm = T)
  return(winsored_vec)
}

```


## Standardize

Min-max normalization : scale into [0,1]

```{r}
standardize <- function(x){(x-min(x))/(max(x)-min(x))}
```


## Linear interpolation

Usage example : `approxfun`

```{r}
monthly[,.(caldt, yearq, yearm, crsp_portno, avg_sim, test = approxfun(1:.N, avg_sim)(1:.N)), by=crsp_portno]
```


## Fast Weighted Mean

By far, for weighted average `collapse` package gives the best result.
It gives fast groupby and fast collapsing(aggregating) with `fmean`

```{r}
mtcars %>%  fgroup_by(cyl,vs,am) %>%                 # Equivalent and faster !
  fselect(mpg,hp) %>%  fmean(hp) # weight by hp

```


My example

```{r}
# weighted average of all variables fselected : weight by trade_size
agg_sample = cboe_price %>% 
  fgroup_by(underlying_symbol,option_type, expiration, days_to_expire, strike, by10) %>% 
  fselect(trade_size, BA_SPREAD, MID_QUOTE, PCT_SPREAD, EFF_SPREAD, PEF_SPREAD) %>% 
  fmean(trade_size) %>% 
  fselect(-sum.trade_size)
```



```{r}
# Aggregation 2 : Selecting variables by regex expression

agg_sample2 = vola_data %>% 
  fgroup_by(SYM_ROOT, option_type, DateTime) %>% 
  get_vars("^D_|GT", regex=T) %>% 
  fsum() %>% 
  setDT


```





## Rowwise Median

Package `matrixStats` is faster than `robustbase`.

```{r}
library(matrixStats)
data[, EndPrice := rowMedians(cbind(BID, ASK))]
```

## rowSums

```{r}

cboe_volume[1:100, rowSums(cbind(OPEN_INTEREST_MM, OPEN_INTEREST_MMBD))]

cboe_volume[1:100, rowSums(cbind(.SD)), .SDcols = .SDcols = c('OPEN_INTEREST_MM', 'OPEN_INTEREST_MMBD')]

cboe_volume[1:100, rowSums(as.matrix(.SD)), .SDcols = .SDcols = c('OPEN_INTEREST_MM', 'OPEN_INTEREST_MMBD')] # as.matrix is slightly faster

cboe_volume[1:100, rowSums2(as.matrix(.SD)), .SDcols = .SDcols = c('OPEN_INTEREST_MM', 'OPEN_INTEREST_MMBD')] # rowSums2 is faster

cboe_volume[1:100, do.call(rowSums, list( cbind(OPEN_INTEREST_MM, OPEN_INTEREST_MMBD) ))]
cboe_volume[1:100, do.call(rowSums, list( cbind(.SD) )), 
            .SDcols = c('OPEN_INTEREST_MM', 'OPEN_INTEREST_MMBD') ]

```


## Standard Error of mean

```{r}
se_mean = function(x) {
  sqrt( var(x, na.rm=T) / sum(!is.na(x)) )
}
```

## Cumsum ignoring NA

Skip NA and leave NA as NA

```{r}
skipna_cumsum = function(x){
  y = x
  y[!is.na(x)] <- cumsum(y[!is.na(x)])
  return(y)
}
```




Making NA as 0
```{r}
# define cumsum that ignores NA
na_cumsum = function(x){ # x is a vector
  
  miss <- is.na(x) # TRUE/FALSE placeholder
  x[miss] <- 0 # convert NAs to 0
  
  cs <- cumsum(x)
  cs[miss] <- NA # convert back to NA 
  return(cs)
}
```



## Edgelist generator

```{r}
gen_edge = function(unique_vector){
  d = data.table(V1 = unique_vector)
  d[, `:=`(id1 = 1L, id2 = .I)]  ## add interval columns for overlaps
  setkey(d, id1, id2)
  olaps = foverlaps(d, d, type='within', which=T)[xid != yid]
  ans = as.data.table(list(d$V1[olaps$xid], d$V1[olaps$yid]))
  return(ans)
}
```

```{r}
prep = function(dt){
  setnames(dt, c('from','to'))
  setorderv(dt, c('from','to'))
  dt[,`:=`(unionN = integer(.N),
           intersectN = integer(.N))]
}
```

## Fund flow generator


Use this in the rolling regression setting with zoo::rollapply

```{r}
# generate monthly flow 
flow_generator_dt = function(DT){ 
  # Data table with two columns : first TNA, second RET
  # needs to be applied in the zoo::rollapply function. 
  flow = (DT[2,1] - DT[1,1] * (1+DT[2,2])) / (DT[1,1])
  return(flow)
}

```



## Linear interpolation

```{r}
# linear interpolate fund age
monthly[, fund_age_port := approxfun(1:.N, fund_age_port)(1:.N), by=crsp_portno]

```

## Decile factorization

Using `cut` and `quantile` functions

```{r}
cut(monthly$avg_sim_imp, 
    breaks = quantile(monthly$avg_sim_imp,probs=c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1), na.rm=T),
    labels = c('1st','2nd','3rd','4th','5th','6th','7th','8th','9th','10th')
    )
```





## Multiple .SDs


*Multiple `.SD`s

```{r}
d = as.data.table(iris)
d[, 
  c(lapply(SD1, sum), lapply(SD2, mean)), 
  env = list(SD1 = as.list(c('Sepal.Length','Sepal.Width')), 
             SD2 = as.list(c('Petal.Length','Petal.Width'))),
  by = Species]

as.list(names(d))
```

GForce Optimized with name set

```{r}
DT = as.data.table(iris)

cols1 = c('Sepal.Length','Sepal.Width')
cols2 = c('Petal.Length','Petal.Width')

DT[, 
   .j, 
   env = list(.j = c(
     lapply(lapply(setNames(nm=cols1), as.name), function(v) call("sum", v)),
     lapply(lapply(setNames(nm=cols2), as.name), function(v) call("mean", v))
   )),
   by=Species
] # GForce ON


```





```{r}
cols1 # vector
setNames(nm=cols1) # named vector that has name as its element
lapply(setNames(nm=cols1), as.name) # named list that has name as its elemeent

a = lapply(setNames(nm=cols1), as.name)

a = as.list(cols1) # each element as separate list
b = list(cols1) # one list with elements

?as.name
as.name(cols1)
lapply(setNames(nm=cols1), as.name)
setNames
?as.name
```



## Time Interval Grouping 

1. Using lubridate::round_date

Most clean for the purpose

```{r}
floor_date(time, unit = "5 min") # round_date, ceiling_date
```



2. Using cut

cut by breaks, but does not guarantee even (5 min cut could be 47, 52, 57, 02, 07, 12, etc..)

```{r}
cut(dat$time, breaks="15 min")
```


## Vector Splitting 

```{r}
spliter = function(x,n) split(x, cut(seq_along(x), n, labels = FALSE))
```



## Force Time zone

lubridate::force_tz is a bit slower (ten times)

```{r}
datetime=rep(as.POSIXct("2011-01-01 12:32:23.234",tz="GMT"),1e6)
f <- function(x,tz) return(as.POSIXct(as.numeric(x), origin=as.POSIXct("1970-01-01", tz=tz), tz=tz))

library(lubridate)
system.time(datetime2 <- f(datetime,"Europe/Paris"))
system.time(datetime3 <- force_tz(datetime,"Europe/Paris"))
identical(datetime2,datetime3)

```

## Fast date/time conversion

fasttime::fastPOSIXct with fixed = N (number of year digits)

```{r}
bzx[, trade_datetime := fastPOSIXct(trade_datetime, fixed=4, tz='UTC')]
bzx[, trade_date := as.Date(trade_datetime)]
bzx[, expiration_date := fastPOSIXct(expiration_date, fixed=4, tz='UTC') %>% as.Date]
```


# R Compile & Installation

## Renviron Setup

R Environmental variables are set every time R session is launched.



Rstudio Error 
Error: vector memory exhausted (limit reached?)

    echo "R_MAX_VSIZE=1000Gb" >> ~/.Renviron
    echo "R_DEFAULT_INTERNET_TIMEOUT=500" >> ~/.Renviron

Windows Powershell

    Add-Content c:\Users\$env:USERNAME\Documents\.Renviron "R_MAX_VSIZE=100Gb"

Windows CMD

    echo R_MAX_VSIZE=500Gb >> %userprofile%\Documents\.Rinviron

    

## Rprofile Setup

Rprofiles are run when R session begins. Usually when one needs to set different default options. It should be located at default "HOME" variable path,
which can be browsed by `Sys.getenv("HOME")`

Mac/Linux

    open ~/.Rprofile

Windows

    C:/Users/gunsu.son/Documents



```{r}
options(reticulate.repl.quiet = TRUE) # disable reticulate message
utils::memory.limit(1e9) # for windows machine
```



## Rstudio Setup

### RStudio IDE options

Like keybindings, dictionaries and other setups, it is saved in 

On Mac

    ~/.config/rstudio

or

On Windows

    AppData/Roaming/RStudio

    
    
### View column limit

Increase column limit

```{r}
rstudioapi::writeRStudioPreference("data_viewer_max_columns", 100L)
```



## R/RStudio removal on MacOS

R, RStudio and miniconda removal

On Terminal,

    sudo rm -rf /Applications/R.app
    sudo rm -rf /Applications/RStudio.app
    sudo rm -rf /Library/Frameworks/R.framework
    sudo rm -rf /Library/Saved Application state/org.rstudio*
    sudo rm -rf ~/.config/rstudio
    sudo rm /usr/local/bin/{R,Rscript}
    sudo rm /private/var/db/receipts/org.R-project*
    sudo rm /private/var/db/receipts/org.r-project*
    sudo rm /private/var/db/receipts/org.rstudio*
    rm -rf ~/Library/Application Support/R
    rm -rf ~/.Renviron

    conda deactivate
    sudo rm -rf ~/Library/r-miniconda/
    rm -rf ~/.zshrc
    rm -rf ~/.bashrc

Remove Rstudio config

    sudo rm -rf ~/.config/rstudio
    sudo rm -rf ~/.local/share/rstudio




## OpenMP(data.table, fst) for MacOS

[Wiki](https://github.com/Rdatatable/data.table/wiki/Installation#openmp-enabled-compiler-for-mac) 

R for MacOS doesn't come with OpenMP support after R 4.0

1.  Install Apple Xcode command line tools

    xcode-select --install

2. Install homebrew

    /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

3. Install gcc, pkg-config

    brew install gcc pkg-config


4. Check GCC version 

    brew info gcc
    
    
5.  Generate Makevars
    
    mkdir ~/.R
    nano ~/.R/Makevars



Copy & Paste

```
LOC = /usr/local
#/opt/homebrew for ARM Macs (M1 and its successors)
#/usr/local for Intel Macs

GCC_VER=11
CC=$(LOC)/bin/gcc-$(GCC_VER) -fopenmp
CXX=$(LOC)/bin/g++-$(GCC_VER) -fopenmp
CXX11=$(LOC)/bin/g++-$(GCC_VER) -fopenmp # for fst package

CFLAGS=-g -O3 -Wall -pedantic -std=gnu99 -mtune=native -pipe
CXXFLAGS=-g -O3 -Wall -pedantic -std=c++11 -mtune=native -pipe
LDFLAGS=-L$(LOC)/lib -Wl,-rpath, -I$(LOC)/lib
CPPFLAGS=-I$(LOC)/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include
```


And then install it from the type='source' to install

```{r}
install.packages("data.table", type = "source",
    repos = "https://Rdatatable.gitlab.io/data.table")
install.packages('fst', type = 'source')
```


## RQuantlib

Either installing binary (intraday disabled) or install with brew

    brew install boost
    brew install quantlib
    
Disable ~/.R/Makevars (especially with above gcc conflicts : complie with mac default clang)

Then install from source with configurations

```{r}
install.packages("RQuantLib", configure.args = "--with-boost-include=/usr/local/include/ --with-boost-lib=/usr/local/lib/", type = "source")
```

It takes a while to compile them.

Then ressurect Makevars file.




## RPostgres

Temporary solution for error (now fixed)
```{r}
install.packages("RPostgres", repos = "https://r-dbi.r-universe.dev")
```




## Install Archived packages

Should be compiled in the computer using archived package directory.

```{r}
install.packages("https://cran.r-project.org/src/contrib/Archive/lfe/lfe_2.8-5.1.tar.gz", repos=NULL, type='source') 
```

For 'lfe', gfortran was needed for compliling on Mac, which could be installed from R homepage `tools` section.






## Rtools40  (Windows)

Starting with R 4.0.0 (released April 2020), R for Windows uses a toolchain bundle called rtools40.
R 4.0 requires to install Rtools to install from source (C, fortran) for windows machines
https://cran.rstudio.com/bin/windows/Rtools/

In .Renviron file located in the documents folder, paste


    PATH="${RTOOLS40_HOME}\usr\bin;${PATH}"


To test if the installation & path is set, `make` should work.

    Sys.which("make")
    ## "C:\\rtools40\\usr\\bin\\make.exe"



## Delete all User installed packages

```{r}
# # create a list of all installed packages
# ip <- as.data.frame(installed.packages())
# head(ip)
# # if you use MRO, make sure that no packages in this library will be removed
# ip <- subset(ip, !grepl("MRO", ip$LibPath))
# # we don't want to remove base or recommended packages either\
# ip <- ip[!(ip[,"Priority"] %in% c("base", "recommended")),]
# # determine the library where the packages are installed
# path.lib <- unique(ip$LibPath)
# # create a vector with all the names of the packages you want to remove
# pkgs.to.remove <- ip[,1]
# head(pkgs.to.remove)
# # remove the packages
# sapply(pkgs.to.remove, remove.packages, lib = path.lib)
```

## Remove Miniconda completely


Not r-reticulate miniconda

```
rm -rf opt/miniconda3
rm -rf ~/.condarc ~/.conda ~/.continuum
```


## Arrow with snappy compression

Before installing arrow run
```{r}
Sys.setenv(ARROW_WITH_SNAPPY = "ON")
```

Then

```{r}
install.packages('arrow')
```


## RETICULATE Python

### Installation

Check default installation path :

```{r}
reticulate::miniconda_path()

# Mac default installation path:
# /Users/matthewson/Library/r-miniconda/

# Windows default installation path :
# "C:/Users/gunsu.son/AppData/Local/r-miniconda"

```

### Mac

1. setup conda :

     source /Users/matthewson/Library/r-miniconda/bin/activate
     
2. Initialize conda - base will be activated all the time

Mac - zsh:

    run conda init zsh
    
linux - bash:
 
    run conda init bash


3. .Renviron Setup

    echo 'RETICULATE_PYTHON="~/Library/r-miniconda/envs/r-reticulate/bin/python"' >> ~/.Renviron


4. Browse conda environments


    conda env list


5. Use reticulate conda

    conda activate r-reticulate


To activate conda

    source /Users/matthewson/Library/r-miniconda/bin/activate
    

#### Apple Sillicon

Download and install Miniforge3 for Apple silicon, default location `~/miniforge3`

Then create a new environment, called `r-reticulate` with python installed in it.

    conda create -n r-reticulate python
    

Then set environment variable for Rstudio:

    echo 'RETICULATE_PYTHON="~/miniforge3/envs/r-reticulate/bin/python"' >> ~/.Renviron



### Windows

0. Use Anaconda Prompt, or



Make Anaconda prompt from CMD

1. Create symlink (shortcut) file for cmd located at

    %windir%\System32\cmd.exe
    
and go properties and set target

    %windir%\System32\cmd.exe "/K" %userprofile%\AppData\Local\r-miniconda\Scripts\activate.bat

set Start in:

    %HOMEPATH%

In order it to be found in the start menu, paste it into

    %userprofile%\AppData\Roaming\Microsoft\Windows\Start_Menu\Programs\Miniconda3




### Conda setup

#### Environments

Browse list of environments

    conda env list
    
Remove environment

    conda env remove -n env_name

#### Channel

Change the priority channel

    conda config --prepend channels conda-forge

Get channel names

    conda config --get channels





# R MARKDOWN

## Beamer EXAMPLES
```
---
title: "FIN4453 : Financial Modeling"
subtitle: "Course Introduction"
author: "Matthew Son"
date: "Week 1"
output:
  beamer_presentation:
    includes:
      in_header:
        - ~/Dropbox (UFL)/Data Workshop/LaTeX/RMarkdown Templates/Mytheme.tex
---
```


### Hide Section slide

https://stackoverflow.com/questions/38180441/dropping-frames-of-sections-and-subsections-titles-for-knitr-beamer-slides

Add this in YAML header.

```
header-includes: 
- \AtBeginSubsection{}
- \AtBeginSection{}
```



### Use custom style 

Using `.sty` file as external template for latex (beamer in this example)

```
output: 
  beamer_presentation:
    includes:
      in_header:
        - Mytheme1.sty
```


## Article example

### Use Custeom Pandoc Template

Because of adjustbox issue, this fixes the issue.
```
---
title: "Testing"
output: 
  pdf_document:
    template: test.tex
date: '2022-02-16'
header-includes:
  - \usepackage{booktabs}
  - \usepackage{geometry}
  - \usepackage{tikz}
  - \usepackage{adjustbox}
---
```
This is the code for test.tex (Gin is making issue with adjustbox)

https://github.com/jgm/pandoc-templates/blob/master/default.latex

**Comment out Gin part**





### Import LaTeX packages

There are basically two ways to include external packages in YAML header:


  1. Use `extra_dependencies` on pdf_document:
  2. Use `header_includes` and \usepackage{}

```
---
title: "Analysis"
output: 
  pdf_document:
    extra_dependencies: ["float"]
header-includes:
  - \usepackage{booktabs}
  - \usepackage{geometry}
---
```

### Prevent floating

In order to prevent floating, package `float` must be imported

```
---
output: 
  pdf_document:
    extra_dependencies: ["float"]
---
```

Then make a default setup

```{r}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
```




### Subfigures

In header-includes: -usepackage{subfig} needs to be included

```
\begin{figure}[H]
\subfloat[Customer Volume\label{fig:volume_trend-1}]{\includegraphics[width=0.5\linewidth]{plots/trend_customer} }
\subfloat[Professional Volume\label{fig:volume_trend-2}]{\includegraphics[width=0.5\linewidth]{plots/trend_prof} }
\caption{Option Trade Volume Trend}\label{fig:volume_trend}
\textit{Note:} The sample consists of all CBOE options trades on S\&P500 stocks from October 2019 to March 2021. Figure on left display all customer option transaction volume by option types. Figure on the right is trend of professional customer volume by option types.
\end{figure}
```


## chunk options

-   echo=FALSE : no code chunk but output

```{r, echo=FALSE}
print('echo FALSE')
```

-   results='hide' : no text output results

```{r, results='hide'}
print('text')
```

-   include=FALSE : code and output not included, but executed

```{r, include=FALSE}
print('include = FALSE')
```

-   eval=FALSE : code chunk not executed but knitted

```{r eval=F}
print('this chunk is not evaluated. It is good use to only show the code')
```

-   collapse=TRUE : image and text output are merged into a single block.\
    Good when warning message(text) is printed when the chunk is executed. Usually it is the case when plotting graphs.

-   error=TRUE : when error has to be reported.

### global chunk options

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, 
                      fig.height=8, 
                      # fig.dim = c(12,8), # width and height
                      fig.cap = 'Here goes figure caption', 
                      fig.align='center',
                      echo=FALSE, warning=FALSE, message=FALSE)
```

## YAML Header

### YAML example

```{r}

title: "R CookBook"
author: "Matthew Son"
date: "Last Updated : `r Sys.Date()`"
output:
  html_notebook:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: yes

```

### parameter


### style

Style can be saved with .css file and be loaded at css

Detailed options for stylish presentation

-   body : for entire body of the html

-   pre : for code chunks style

-   \#TOC : for table of contents box

-   \#header : for header

    -   h1.title
    -   h4.author
    -   h4.date

-   color

-   background-color

-   font-family

-   font-size

-   opacity


## Markdown

### text

**bold** 

*italic* 

~~strikethrough~~

    Message in the Box : double tap


    - Double tap and hypen

### hyperlink

This is the [hyperlink text](http://google.com/)

### embed image

![This is a caption of the image put below](https://i.imgur.com/Wwx25wY.jpg)

### LaTeX

#### subscript / superscript

Limits

$$\sum\limits_1^N$$

#### Curly brackets up and down

Overbrace and Underbrace


$$\text{NPV} = \overbrace{\text{CF}_0}^{\substack{\text{Excel' NPV function} \\ \text{does not accont for this}}} + \underbrace{\sum_{t=1}^N\frac{\text{CF}_t}{(1+r)^t}}_{\substack{ \text{This is what Excel's} \\ \text{NPV function calculates}} }$$






## Kable (KableExtra)

### Table digits and formatting

Example

```{r}
summary %>% 
  kbl(digits = 2,
      format.args = list(big.mark = ",", 
                         scientific = FALSE)) %>% 
  kable_styling(latex_options = "HOLD_position")

```




```{r}
knitr::kable(iris %>% head, align = 'ccc', caption = 'This is caption')
```

Comparison : without kable

```{r}
print(iris %>% head)
```

#### kableExtra

Package that adds more features on kable.

```{r messae =F}
library(kableExtra)
```

```{r}
iris %>% 
  kbl() %>% 
  kable_classic_2() %>% 
  kable_paper() %>%
  scroll_box(width = "100%", height = "200px")
```

### Internal links

Pandoc supports explicit and implicit section references for headers; see the pandoc manual.

-   explicit: you give a custom name to a header \#\# Test {\#test} and later refer to it with a link syntax: see [the relevant section](#test).

-   implicit: headers where you don't set a custom name, like \#\# Test, can still be refered to: See the section called [Test].













# BASE R / RStudio

## R Function Tricks

### Expressions to string

```{r}
deparse(substitute(iris))
deparse(quote(iris)) # similar but takes the function's argument instead
```


### assign to string object

#### assign / .GlobalEnv

```{r}
x = 3
assign('x', 3)
.GlobalEnv$'x' = 3

# .GlobalEnv gives accessibility 

.GlobalEnv$'x'[[2]] = 3 # automatically generate as list
.GlobalEnv[['x']][[2]] = 3 # This one accepts functions

var = 'x'
.GlobalEnv[[glue::glue('{var}')]][[2]] = 3
```




### string evaluation

Two versions : `evalparse` and `get`

#### eval parse

```{r}
eval(parse(text='iris'))
```

#### get / mget

##### get

A string to point object in environment

```{r}
#example
a = 'Hello, world!'
print('a') # this is a string
print(get('a')) # get('a') == a
print(a)
```

##### mget

Convert a vector of string object names into list of objects

```{r}
# example
a = c(1,2,3)
b = c(3,4,5)
obj_names = c('a','b')
mget(obj_names)
```


## Options

### .Rprofile

`.Rprofle` needs to be setup.

On Mac

    echo "options(datatable.prine.class =T)" >> ~/.Rprofile

On Windows

    echo options(reticulate.repl.quiet = T) > %userprofile%\Documents\.Rprofile


### data.table options

Print options with `data.table`

```{r}
# change default options
options(datatable.print.nrows = 50)
options(datatable.print.topn = 10)
options(datatable.print.class= T)

# change locally
print(DT, topn = 20)
```

### str options
Print options with `str` function - list.len

```{r}
options(str = strOptions(list.len = 1e5))
```

### Reticulate options

```{r}
options(reticulate.repl.quiet = T)
```


## data.frame

### transpose data.frame

When data.frame has row names and column names
```{r}
iris_transpose <- as.data.frame(t(as.matrix(iris_df)))
```


## shQuote

Adding quotes to each characters

```{r}
shQuote(c('AAPL','AA'))
# [1] "'AAPL'" "'AA'"
```


## outer

Outer product of two arrays

```{r}
months= str_pad(9:12, 2, pad ='0')
days = str_pad(1:31,2,pad='0')
md = c(outer(months, days, FUN = str_c))
```


## list.files

Making the list of file names

```{r}
library(readxl)
file.list <- list.files(pattern='*.xlsx')
df.list <- lapply(file.list, read_excel)

```



## str

Print all of the data.frame (data.table)

```{r}
# print more
str(df, list.len=ncol(df))
```


## cut

Cut and make groups. Useful for time cutting.

My usage example
```{r}
test2[,by10 := fastPOSIXct(cut(quote_datetime, breaks='10 min'), tz='UTC') + 600]
```



## factor

### Unordered

labeling makes the column as if character vector

```{r}
mtcars$fam <- factor(mtcars$am, labels = c(`0` = 'automatic',
                                           `1` = 'manual'))

```

### Ordered 

: use `level` and `label` both

```{r}
mtcars$fam <- factor(mtcars$am,
                     levels = c(1, 0),
                     labels = c("manual", "automatic"))

```





## Error Handling

### exists()
Stop and error when condition is met.
Object name is given in character.
```{r}
simplefunc = function(x){
  exists(x)
  print('Success')
}
simplefunc('iris')
```


### stop()


### stopifnot()


### try()

When error has to be skipped in the loop, use `try`

```{r}
# lapplying log function, error on the last
a = list(1,2,3,'oh',4)
# lapply doesn't report the interim output because of error
lapply(a, log)
```

```{r}
k = lapply(a, function(x) try(log(x))) # it does work, and shows error
class(k[[4]]) # try-error
```

When ran in data.table, errors are recorded with NA

```{r}
library(data.table)
irist = as.data.table(iris)
irist[3, a := 5]
irist
for (i in 2:nrow(irist)){
  irist[i, a := try(sqrt(Species), silent = TRUE)] # data.table coerces to NA for numeric column
}


```





### tryCatch()

More on error handling : what should do when it generates error?

It should be given in functional form. Give anonymous function for simple operations.

```{r}
tryCatch(sqrt('a'),
         error = function (x) print('You silly, put number instead')
         )
?tryCatch
```




## RStudioapi

### run code on terminal
Run code on the terminal from R script

```{r}
rstudioapi::terminalExecute('ls')
```

### increase View column limit

```{r}
rstudioapi::writeRStudioPreference("data_viewer_max_columns", 1000L)
```





# BENCHMARKING

## system configurations

For bug reporting, etc.

```{r}
sessionInfo()
```




`benchmarkme` package : checking out the specification of the local.

```{r}
# Load the package
library(benchmarkme)
# Assign the variable ram to the amount of RAM on this machine
get_ram()
# Assign the variable cpu to the cpu specs
get_cpu()
# Load the parallel package
library(parallel)
# Store the number of cores in the object no_of_cores
detectCores() # number of cores
?detectCores
```

`microbenchmark` package : for efficiency and speed comparision

```{r}
library(microbenchmark)
microbenchmark(
  print('ak'),
  print('abcdefg'),
  times=100L
)
```

## benchmarking time

Using `Sys.time()`

```{r}
tic = Sys.time()
print('haha')
toc = Sys.time()
toc - tic
```

Using `system.time()`

```{r}
system.time(
  print('hahaha')
)
```






# PARALLEL PROCESSING

Basic introduction :

`parallel`, `foreach`, `future.apply` packages uses master / worker.

They are good for single machine and medium size data.

`sparklyr` utilizes Spark, which uses Map-Reduce.


## DISK.FRAME

disk.frame is an future object that connects to a folder that contains multiple .fst files.
Those multiple fst files are chunks from the original file.

```{r}
iris.df = as.disk.frame(iris) # without path specification, it saves to temp foler
iris.df # nchunks 8
```


### Setup

```{r}
setup_disk.frame(workers = 8)
options(future.globals.maxSize = Inf,  # Unlimited data communication
        future.rng.onMisuse = 'ignore') # Ignore random seeds
```

### Basic operations

#### Read disk.frame

##### disk.frame()

Linking the file folder path as disk.frame. It expects .fst files within the folder.
Folder itself is the `disk.frame`.


```{r}
disk.frame(path = 'folderpath')
```


##### csv_to_disk_frame()

When csv is to large to read, read in chunks

```{r}
flights.df <- csv_to_disk.frame(
  csv_path, 
  outdir = df_path, 
  in_chunk_size = 1e7, # chunk size by number of rows
  nchunks = 8, # number of chunks
  .progress = TRUE,
  select = c('colnames') # passed onto fread
  )

```

###### JIT transformation

This approach is faster than reading csv and transform and save.

```{r}
df = csv_to_disk.frame('filepath', 
                       inmapfn = function(chunk) {
                         # convert to date_str to date format and store as "date"
                         chunk[, date := as.Date(date_str, "%Y-%m-%d")]
                         chunk[, date_str:=NULL]
                       })

```

##### zip_to_disk.frame()

zip file to directly read to disk.frame. Arguments are same as csv_to_disk.frame

```{r}
zip_to_disk.frame()
```

#### Write disk.frame

##### write_disk.frame()

```{r}
write_disk.frame() # useful for rechunking as well!
```



#### Prompt evaluation

Currenlty, data.table opertaions are evaluated promptly, for each chunk of `.fst` files.

```{r}
iris.df[] # collect all, warning!!
iris.df[1] # 1st row of all chunks
iris.df[1:5] # first 5 rows of all chunks
```



#### Lazy evaluation

Manipulating disk.frame with lazy evaluation

Use evaluation with `cmap`
Warning : since it is not in memory, can't be overwritten on the same disk.frame!
When `cmap` is called without object name definition, it is evaluated promptly.

```{r}
# prompt function performance & write disk.frame
cmap(iris.df, function(chunk){chunk[,newval2:= 1]},
     # keep = c('col1','col2'),
     outdir = 'df_name',
     lazy=FALSE, # prompt writing
     overwrite = T,
     compress = 100)
```

To do lazily, save into an object and use with `write_disk.frame`.

Warning : since it is not in memory, can't be overwritten on the same disk.frame!
```{r}
k = cmap(iris.df, function(chunk){chunk[,newval2 := 2]})
write_disk.frame(k,
                 outdir = 'df_name2', # shoulf be different folder than original disk frame
                 compress=100,
                 overwrite=TRUE
                 )
```





#### Access chunks

##### get_chunk()

Get chunk with integer id number. Does not guarantee the integer is in sequence when hased (shardby) !!

```{r}
get_chunk(iris.df, 1) # needs 
```

##### get_chunk_ids()

```{r}
get_chunk_ids(iris.df)
```





#### Rechunk

Two approaches : using `write_disk.frame` and provide explicit nchunks / use `rechunk` function

`write_disk.frame













### Examples

#### shardkey




## FUTURE


```{r}
library(future)
plan(multisession) 
plan(multicore) # forking, no RStudio, no windows
```

Options

```{r}
options(future.gc=TRUE) # garbage collection
options(future.globals.maxSize = Inf)
```


### Basics of Future


Basics : when future is called, the process is done behind the scene - opens up a new session and do the job then returns the value to the original session.

```{r}
# a slow process that takes 20 secs
slow_sum = function(x){
  Sys.sleep(length(x))
  sum(x)
}
slow_sum(1:20) # has to wait 20 secs to get the answer!
```

Now use future to run it behind the scene. 

```{r}
fa = future(slow_sum(1:10)) # does job behind the scene, 10sec
fb = future(slow_sum(11:20)) # does job behind the scene, 10 sec
```

To take out the calculated value, call `value()`. If it is called too early, then it waits until the future is complete!

```{r}
a = value(fa) # when called within 10 sec, it waits!
b = value(fb)
a + b
```

In order to do it in one step,
```{r}
a %<-% {slow_sum(1:10)} # future and store value when resolved
a # waits if it is not resolved

```


To know if the future was resolved, 
```{r}
resolved(fa)
```

#### Garbage collection

Can be set in the beginning
```{r}
plan(cluster, workers = cl, gc = TRUE)
```


If garbage collection needed after value was retrieved;
```{r}
x <- future({ expr }, gc=TRUE)
```


### Future with data.table

```{r}
library(data.table)
dt_iris = as.data.table(iris)
dt_iris
f = future(dt_iris[, mean(Sepal.Length)], packages='data.table')
v = value(f)
```


### Export globals

From future documentation :

In most cases, such automatic collection of globals is sufficient and less tedious and error prone than if they are manually specified. 
However, for full control, it is also possible to explicitly specify exactly which the globals are by providing their names as a character vector. 
In the above example, we could use


```{r}
a <- 42
f <- future({ b <- 2; a * b }, globals = "a")
f <- future({ b <- 2; a * b }, globals = list(a = a)) # same
```
we can disable the automatic search for globals by using
```{r}
f <- future({ a <- 42; b <- 2; a * b }, globals = FALSE)
```

```{r}
a <- 42
b <- 2
f <- future({ a * b }, globals = FALSE)
f2 <- future({a*b}, globals = 'a') # only a
v = value(f) # varible a not found!
v = value(f2) # b not found
```


Example : dt_iris and "a" defined in the global environment, but I'm only exporting subset of globals
```{r}
dt_iris = as.data.table(iris)
a = 30
f = future({print(dt_iris); print(a); iris[Sepal.Length == 5.1]}, globals= list(iris = dt_iris[Species=='setosa']), package='data.table')
v = value(f)  # prints first error message, and does not do the job afterwards! 
v
```



### Errors in the future


Example : dt_iris and "a" defined in the global environment, but I'm only exporting subset of globals
```{r}
f = future({message('working');print(dt_iris); print(a); iris[Sepal.Length == 5.1]; message('work done here')}, globals= list(iris = dt_iris[Species=='setosa']), package='data.table')
v = value(f) # prints first error message, and does not do the job afterwards! 
v # no object assigned
```

I can igore the error and just proceed with tryCatch with empty function `error = function() {NULL}` 

```{r}
f = future(
  tryCatch(
    {message('working');print(dt_iris); print(a); dt_iris[Sepal.Length == 5.1]; message('work done here') },
    error = function() {NULL}
  ),
  globals= list(iris = dt_iris[Species=='setosa']), 
  package='data.table')
v=value(f)
v
```

```{r}
tryCatch(
    {message('working');print(dt_iris); print(a); dt_iris[Sepal.Length == 5.1]; message('work done here') },
    error = function() {NULL}
  )

```








### Set workers (cores) for each process

#### Multisession + Sequential



```{r}
availableCores()
availableWorkers()

plan(list(
  tweak(multisession, workers = 4), # 4 R sessions
  tweak(multisession, workers = 2)) # nested future use 2 separate sessions
)
plan(multisession)
```

H2O example for this

```{r}
library(h2o)
library(future)

plan(tweak(multisession, workers = 2)) # only two R processes

# initiate multiple h2o clusters and match them to separate R processes
jar_path = system.file('java', 'h2o.jar', package='h2o')

require(stringr)
system(str_glue('java -Xmx4g -jar {jar_path} -name n1 -nthreads 4  -port 54321'), intern = F, wait =F)
system(str_glue('java -Xmx4g -jar {jar_path} -name n2 -nthreads 4  -port 54322'), intern = F, wait =F)


f1 = future({
  # h2o.shutdown(prompt = F)
  h2o.init(startH2O = F, port = 54321)
  Sys.getpid()
}, lazy =T)
f2 = future({
  # h2o.shutdown(prompt = F)
  h2o.init(startH2O = F, port = 54322)
  Sys.getpid()
}, lazy =T)

```



#### Future + H2O

Split data by ticker, about equal size

```{r}
my_tickers = alldays[,unique(TICKER)][1:20] # first 20 tickers for example
num_tickers = seq_along(my_tickers)
each = 10
chunks = split(my_tickers, ceiling(num_tickers/each))
chunks %>% class #list
```



```{r}
plan(list(tweak(multisession, workers = 2), tweak(multisession, workers = 4)))
```

Simple example

```{r}
plan(list(
  tweak(multisession, workers = availableCores() %/% 4),
  tweak(multisession, workers = 4)
))
```





```{r}
v <- {
  cat("Hello world!\n")
  3.14
}
v
```


```{r}
library(future)
v %<-% {
  cat('hello\n')
  3.14
}
v # evaluates when it is called!

plan(multisession)
future::availableWorkers()
future::availableCores()

v %<-% {
  cat('hello\n')
  3.14
}
v # evaluates when it is called!

```



Explicit future

```{r}
f = future({
  cat('hello\n')
  3.14
})
v = value(f)

```





```{r}
plan(sequential)
availableWorkers()
a = 1
x %<-% {
  a = 2
  2 * a
}
x 
a
```


```{r}
plan(sequential)
pid = Sys.getpid()
pid #1] 24919

a %<-% {
  pid = Sys.getpid()
  cat('Future "a" ...\n')
  3.14
}
b %<-% {
  rm(pid)
  cat('Future "b" ...\n')
  pid
}
c %<-% {
  cat('Future "c" ...\n')
  2 * a
}

```


```{r}
plan(multisession)
pid = Sys.getpid()
pid #1] 24919
a %<-% {
  pid = Sys.getpid()
  cat('Future "a" ...\n')
  3.14
}
b %<-% {
  rm(pid)
  cat('Future "b" ...\n')
  Sys.getpid()
}
c %<-% {
  cat('Future "c" ...\n')
  2 * a
}
b
pid
c
```


Nested future

```{r}
plan(list(sequential,multisession))
pid = Sys.getpid()
a %<-% {
  cat('Future "a" ...\n')
  Sys.getpid()
}

b %<-% {
  cat('Future "b" ...\n')
  b1 %<-% {
    cat('Future "b1" ...\n')
    Sys.getpid()
  }
  b2 %<-% {
    cat('Future "b2" ...\n')
    Sys.getpid()
  }
  c(b.pid = Sys.getpid(), b1.pid =b1, b2.pid = b2)
}

pid

a # [1] 25887 
b # 25943  25943  25943 




```





```{r}
library(future)
library(listenv)
x = listenv()

for (i in 1:3){
  x[[i]] %<-% {
    y = listenv()
    for (j in 1:3){
      y[[j]] %<-% { i + j /10}
    }
    y
  }
}

unlist(x)

```

### Examples

#### Export small global object
```{r}
f1 = future(
  {
    message("Working on ", "firm_open_buy_qty");
    slim_cboe
  }, 
  globals = list(slim_cboe = cboe[,.SD, .SDcols = c(key_cols, "firm_open_buy_qty")]),
  packages = c('data.table')
)
value(f1) # OK
```

#### Future with Function wrap

```{r}

test_func = function(colname, whole_data){
  f = future(
    {
      tic();
      slim_cboe[, test := frollapply(get(colname), n=2, FUN = function(x) {x[2]-x[1]})]
      toc();
      slim_cboe[,test]
    }, 
    globals = list(slim_cboe = whole_data[,.SD, .SDcols = c(key_cols, colname)],
                   colname = colname), # colname needs to be exported again
    packages = c('data.table', 'tictoc')
  )
  return(f)
}

hmm = test_func('firm_open_buy_qty', cboe)
resolved(hmm) #OK!
```



### doFuture

```{r}


library(doFuture)
options(future.globals.maxSize = Inf)
options(future.rng.onMisuse = "ignore") # ignore RNG warning
registerDoFuture()
plan(multisession)

getDoParWorkers()

library(progressr)
handlers(global = TRUE)
handlers("progress")

```


### Basics 

Exporting objects

When it needs to be manual, it can be turned off with 
```{r}
options(doFuture.foreach.export = ".export")
```


Note that when using doFuture, arguments `.packages` and `.export` in foreach() are not necessary, as the package deals with the exports automatically.

```{r}
library(doFuture)
registerDoFuture()
plan(multisession, workers =4)
getDoParWorkers()

foreach(n = c(3,4,2,5,1,5,6,2,6)) %dopar% {
  print('Sleeping 1 sec...')
  Sys.sleep(1)
  rnorm(n)
}

```




doFuture.foreach.export:
Specifies to what extent the .export argument of foreach() should be respected or if globals should be automatically identified.

If ".export", then the globals specified by the .export argument will be used "as is".

If ".export-and-automatic", then globals specified by .export as well as those automatically identified are used.

The ".export-and-automatic-with-warning" is the same as ".export-and-automatic", but produces a warning if .export lacks some of the globals that the automatic identification locates

this is helpful feedback to developers using foreach().





### Parallel data.table

For data.table to be applied in parallel, all DTs should be splitted and saved into a list.
`split` function is a possible use, or using disk.frame to split and read.fst is a way.


```{r}
foreach(dt = splited_dt, .packages='data.table') %dopar% {
    dt[, str_replace(DATE, '(\\d{4})(\\d{2})(\\d{2})', '\\1-\\2-\\3')]
  }
```


##### fread in chunks

```{r}
data=NULL
chunk.size = 1e8
for (i in 0:20){
    data[[i+1]] = fread("my_data.csv", nrow = chunk.size, skip = chunk.size*i )
}
```




#### Global assignment

<https://github.com/HenrikBengtsson/doFuture/issues/29>

```{r}
registerDoFuture()
plan(multisession)
x <- data.frame(a=1:5,b=NA,c=NA)
foreach(i = 1:nrow(x)) %dopar% {
  x$b[i] <- runif(1)
  x$c[i] <- runif(1)
}
x # doesn't work!
```

The reason for this not working is that the parallel code (what's inside the foreach { ... } expression) is evaluated in a separate R process that the main R session from where it is called. It is not possible for other R processes on your machine to update the variables in your main R session. An analogue is when you run two separate R sessions manually and you do x \<- 42 in one of them - then you wouldn't expected x to be assigned in the other R session. That's how parallel processing in R works too. **Instead, all values must be "returned" at the end** of a parallel evaluation.

Instead, treat foreach() as you treat lapply(). If you do, then you're example is effectively equal to:

```{r}
lapply(1:nrow(x), FUN = function(i) {
  x$b[i] <- runif(1)
  x$c[i] <- runif(1)
}
)
```

Instead, you should always "return" values. That is, make sure all of your foreach() calls are assigned to a variable, cf. 
y <- lapply(...). So, in your case you can to do something like:

```{r}
x <- data.frame(a=1:5,b=NA,c=NA)
y <- foreach(i = 1:nrow(x), .combine = rbind) %dopar% {
  data.frame(b = runif(1), c = runif(1))
}
x$b <- y$b
x$c <- y$c
```

In the ideal case, all it takes to start using futures in R is to replace select standard assignments (<-) in your R code with future assignments (%\<-%) and make sure the right-hand side (RHS) expressions are within curly brackets ({ ... }). Also, if you assign these to lists (e.g. in a for loop), you need to use a list environment (listenv) instead of a plain list.






## future.apply package

```{r}
library(future)
library(future.apply)
```

### plan

How to plan future :

1.  sequential : sequentially, not parallel

2.  multisession : PSOCK (both MAC/LINUX + WINDOWS)

2-1. multicore : resolve futures asyncronously in separate FORKED R sessions. (UNSTABLE with Rstudio & MAC/LINUX only)


3.  cluster : resolve futures asyncronously in separate R sessions - one or more machines.

4.  remote : separate machine - in different network

```{r}
plan(multisession)
## Explicitly close multisession workers by switching plan
plan(sequential)
```

### availableCores

```{r}
availableCores()
```

### future_lapply

```{r}
plan(sequential)
future_lapply(1:10, rnorm)
```

### reproducibility

put `future.seed` argument for reproducibility.

```{r}
plan(sequential)
res1= future_lapply(1:5, rnorm, future.seed = 1234)
plan(multiprocess)
res2= future_lapply(1:5, rnorm, future.seed = 1234)
identical(res1,res2)
```





## parallel package

```{r}
library(parallel)
```

base R : `parallel` package.

With this package, users can use current R session as the master process, while each worker is a separate R process. 

To detect number of cores (hyperthreaded) and make a cluster of nodes :

```{r}
detectCores()
detectCores(logical=FALSE) # physical cores only, no hyperthreading
ncores = detectCores()
```

Make clusters & stop clusters

```{r}
# start a cluster
cl <- makeCluster(ncores) # 'PSOCK' is the default

# cl = makeForkCluster(ncores)
# cl = makePSOCKcluster(ncores)

# stop connection to cluster
stopCluster(cl)
```

Socket vs Fork

In a fork cluster, each worker is a copy of the master process, whereas socket workers start with an empty environment.

```{r}
# A global variable and is defined
a_global_var <- "before"

# Create a socket cluster with 2 nodes
cl_sock <- makePSOCKcluster(2) # empty environment

# Evaluate the print function on each node
clusterCall(cl_sock, print, a_global_var) # a_global_var is not defined
stopCluster(cl_sock)
stopCluster(cl)

``` 


What is the process id for each cluster?

```{r}
Sys.getpid() # gives current R process id

clusterCall(cl, Sys.getpid) # clusters has different, independent R process

```

### clusterApply()

```{r}
# single node processing : lapply
lapply(ncores:1, rnorm)

# parallel processing : clusterApply
clusterApply(cl, x=ncores:1 , fun = rnorm, sd=1:100) # x : how many times function is called

# x = c(8,7,6,5,4,3,2,1)
# rnorm(8) for first worker
# rnorm(7) for second worker
# ...
# rnorm(1) for eighth worker

```

Example : more tasks than worker

```{r}
mean_of_rnorm = function(n){
  random_numbers <- rnorm(n)
  mean(random_numbers)
}
n_replicates <- 50
n_numbers_per_replicate <- 10000
x = rep(n_numbers_per_replicate, n_replicates)
x # 10000 10000 ... 10000

# Parallel evaluation on n_numbers_per_replicate, n_replicates times
means <- clusterApply(cl, 
                      x = x, 
                      fun = mean_of_rnorm)
unlist(means)
```

#### load balancing

Instead of waiting for the results from nodes in specific order, it accepts results from the first worker that finished and immediately send back new task.

Improves speed if the task is imbalanced, but for small tasks overhead costs outweighs speed gains.

```{r}
set.seed(1)
tasktime = sqrt(abs(rnorm(16)))
cl = makeCluster(8)
library(future.apply)
plan(multisession)

microbenchmark::microbenchmark(
  clusterApply(cl, tasktime, Sys.sleep),
  clusterApplyLB(cl, tasktime, Sys.sleep),
  future_lapply(tasktime, Sys.sleep), # does automatic load balancing
  times=3
)

```

Plotting the job

```{r}
plot_cl_apply = function(cl, x, fun) 
    plot(snow::snow.time(snow::clusterApply(cl, x, fun)),
            title = "Cluster usage of clusterApply")
plot_cl_applyLB = function(cl, x, fun) 
    plot(snow::snow.time(snow::clusterApplyLB(cl, x, fun)),
            title = "Cluster usage of clusterApplyLB")

plot_cl_apply(cl, tasktime, Sys.sleep)
plot_cl_applyLB(cl, tasktime, Sys.sleep)
```

### clusterEvalQ()

Evaluate the literal expressions on all workers (nodes).

```{r}
cl = makeCluster(2)
clusterEvalQ(cl, {
  library(tidyverse)
})
clusterCall(cl, function() iris %>% head) # pipe operator works, because tidyverse is imported
```

### clusterExport()

Exports objects from master to all workers (nodes).

```{r}
this <- 'hello'
clusterEvalQ(cl,{
  print(this)
}) # not found
```

```{r}
clusterExport(cl, 'this')
clusterEvalQ(cl,{
  print(this)
}) 
```

### parApply()

Wrapper for `clusterApply()`.

`parLapply()`, `parSapply()` and\
`parLapplyLB()`, `parSapplyLB()` for load balancing,

Mac/Linux has shared memory system, while windows does not.

For both Mac / Linux and Windows.

```{r}
parLapply(cl, c(1,1), rnorm) 
clusterApply(cl, c(1,1), rnorm) # same, output is list

parSapply(cl, c(1,1), rnorm)
```



Functions : cluster's do not know custom functions without explicit export.

Use `clusterExport()`

```{r}
# If function is defined, then it should be exported to cluster to be used.
play <- function() {
    total <- no_of_rolls <- 0
    while(total < 10) {
      total <- total + sample(1:6, 1)
  
      # If even. Reset to 0
      if(total %% 2 == 0) total <- 0 
      no_of_rolls <- no_of_rolls + 1
    }
    no_of_rolls
  }

# Export the play() function to the cluster
clusterExport(cl, "play")

# Re-write sapply as parSapply
res <- parSapply(cl, 1:100, function(i) play())

```

### progress bar

```{r}
# parallel appraoch
library(parallel)
library(pbapply)
my_cl = makeCluster(8)

split_max = function(col){
  split = strsplit(col,'|',fixed=T)
  int_split = lapply(split, as.integer)
  maxed_col = suppressWarnings(sapply(int_split, max, na.rm=T))
  maxed_col[is.infinite(maxed_col)] = -1
  return(maxed_col)
}


paste0(names(sdc)[256:ncol(sdc)],'_MAX')


lapply(strsplit(a$q, '|', fixed=T),as.integer) %>% sapply(max, na.rm=T)
sdc[,S_P_I_LT] %>% head(50) %>% strsplit('|',fixed=T) %>% lapply(max)
```








# Progress bar

## DoFuture Foreach progressbar

```{r}
library(doFuture)
registerDoFuture()
plan(multisession)

library(progressr)
handlers(global = TRUE)
handlers("progress")

# Example
my_fcn <- function(xs) {
  p <- progressor(along = xs)
  y <- foreach(x = xs) %dopar% {
    Sys.sleep(6.0-x)
    p(sprintf("x=%g", x))
    sqrt(x)
  }
}

my_fcn(1:5)


```


My usage

```{r}
my_fcn = function(split_dt){
  p = progressor(along = split_dt)
  result_dtlist = foreach(x = split_dt, j = seq_along(split_dt)) %dopar% {
    p(sprintf("x=%g", j))
    x[, mflow_fundno := c(NA,rollapply(x[,.(mtna,mret)],2, flow_generator_dt, by.column=F))]
  }
  return(rbindlist(result_dtlist))
}

mon_ret_flow = my_fcn(seq_along(split_ret),split_ret)
```


Usage 2

```{r}
my_fcn <- function(filelist) {
  p <- progressor(along = filelist)
  list_df <- foreach(x = seq_along(filelist)) %dopar% {
    p(sprintf("x=%g", x))
    fread(cmd = filelist[[x]])
  }
  return(list_df)
}

a = my_fcn(bzx_files[1:20])
```


## for loop progress bar

`progress` package

Use for `for` loop or `foreach` loop for parallel

```{r}
library(progress)

# basic

pb <- progress_bar$new(total = 100) 
for (i in 1:100) {
pb$tick()
Sys.sleep(1 / 100) }
```

```{r}
# percent and eta
pb <- progress_bar$new(
      format = "  downloading [:bar] :percent eta: :eta",
      total = 100, clear = FALSE, width= 60)
for (i in 1:100) {
pb$tick()
Sys.sleep(1 / 100) }
```

```{r}
# Elapsed time
pb <- progress_bar$new(
  format = "  downloading [:bar] :percent in :elapsed",
  total = 100, clear = FALSE, width= 60)
for (i in 1:100) {
pb$tick()
Sys.sleep(1 / 100) }

```

## apply progressbar

`pbapply` package

`pbLapply`, `pbSapply` are often used

```{r}
library(pbapply)

```




# Data I/O

`getwd()` to get the current working directory\
`file.path('~','data')` to make a path in programmatic form (os independent)\



## List of files in directory

`list.files()`

```{r}
list.files()
```

## Read flat files

1.  baseR `utils` package : `read.csv`, `read.delim`, `read.table`(ultimate)\
2.  Tidyverse `readr` package : `read_csv`, `read_tsv`, `read_delim`(ultimate)\
3.  `data.table` package : `fread` 


```{r}
# read some cols only
# Column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")

potatoes <- read_tsv("potatoes.txt", col_names = properties)

# Import all data, but force all columns to be character: potatoes_char
potatoes_char <- read_tsv("potatoes.txt", col_types = "cccccccc", col_names = properties)
```

## Read excel files

Tidyverse `readxl` package

`excel_sheets` : names of sheets in the file\
`read_excel` : read the sheets\

```{r}
# To read all excel sheets with in a file
pop_list <- lapply(excel_sheets("urbanpop.xlsx"), read_excel, path = "urbanpop.xlsx") # a list of dfs
```

## Read Stata files

Tidverse `haven` package

`read_stata` or `read_dta`

R also reads labeled columns well.

```{r}
library(haven)
sugar <- setDT(read_dta("http://assets.datacamp.com/production/course_1478/datasets/trade.dta"))
glimpse(sugar)

sugar$Import # label and format are also shown
```

## Database backend

```{r}
# Load the DBI package
library(DBI)

# Edit dbConnect() call
con <- dbConnect(RMySQL::MySQL(), 
                 dbname = "tweater", 
                 host = "courses.csrrinzqubik.us-east-1.rds.amazonaws.com", 
                 port = 3306,
                 user = "student",
                 password = "something")

# Build a vector of table names: tables
tables <- dbListTables(con) #"comments" "tweats"   "users"

# Display structure of tables
str(tables)

# Import the users table from tweater: users
users <- dbReadTable(con, "users")

# Get table names
table_names <- dbListTables(con)

# Import all tables
tables <- lapply(table_names, dbReadTable, conn = con) # list of dfs

```

Use SQL Query to process on the server and retrieve the data

```{r}
# Import tweat_id column of comments where user_id is 1: elisabeth
elisabeth = dbGetQuery(con, 
                       "SELECT tweat_id FROM comments WHERE user_id =1"
                       )

```

Send-Fetch-Clear combo : what `dbGetQuery` automatically does at once

```{r}
# Send query to the database
res <- dbSendQuery(con, "SELECT * FROM comments WHERE user_id > 4")

# Use dbFetch() twice
dbFetch(res, n = 2)
dbFetch(res)

# Clear res
dbClearResult(res)
```

## Read parquet 

```{r}
# parquet file : fast to read, but makes it small (1.3GB csv -> 78 MB)
library(arrow)
pqdata = read_parquet('~/Dropbox (UFL)/Gunsu_Phil/data/SDC munibond data/sdc_toxic.parquet')
pqdata = pqdata[1:50,1:10] # first 50 rows 
head(pqdata)
```



# DATA.TABLE

DT[i,j,by] i : for `filter` and `order` rows\
j : for `select/drop`, `add` and `summerize` columns\
by : for grouping operations\

## `i` operations

`IMPORTANT`: i operations are executed the first thing before j=, by= arguments. If this should be performed later, than should use [] chain.

When ordering : use DT[order(col1),] for ordering and `setorder` for in-place operation.



### REMOVE ROWS BY REFERENCE

```{r}
delete <- function(DT, del.idxs) {           # pls note 'del.idxs' vs. 'keep.idxs'
  keep.idxs <- setdiff(DT[, .I], del.idxs);  # select row indexes to keep
  cols = names(DT);
  DT.subset <- data.table(DT[[1]][keep.idxs]); # this is the subsetted table
  setnames(DT.subset, cols[1]);
  for (col in cols[2:length(cols)]) {
    DT.subset[, (col) := DT[[col]][keep.idxs]];
    DT[, (col) := NULL];  # delete
  }
  return(DT.subset);
}
```

Example

```{r}
iris_dt = as.data.table(iris)
delete(iris_dt, iris_dt[,which(Species=='setosa')])
```

```{r}
cboe = delete(cboe, cboe[,which(security_type==1)])
```




### order rows

setorder() function

`setorder()` and `setorderv()` : fast inplace ordering function

`setorder()` : supports unquoted varaible names

```{r}
gapminder %>% head
setorder(gapminder, country, continent, -year) # - for descending order
gapminder %>% head
setorderv(gapminder, c('country', 'continent','year'), order=c(1,1,-1)) # ascending, descending order for character vector
```

Bracket syntax - not by reference

```{r}
gapminder
gapminder[order(country,continent,year)] # not inplace, new 
```

#### setkey

`setkey()` and `setkeyv()`

When the data.table's key is set, then the original data.table is sorted by the key variable(s).

setting key is useful for :\
1. efficient filtering\
2. efficient joining\

This performance is achieved binary search algorithm, which is only available when it is sorted.

To undo setting key, `setkey(data, NULL)`

When `keyby=` was used, then the data.table gets those vairables as its key automatically (therefore the return are also ordered by `keyby=` variables)

```{r}
setkey(gapminder,NULL) # remove key
haskey(gapminder) # FALSE
gapminder[,.SD,keyby =continent] # this is a new object
gapminder[,.SD,keyby =continent] %>% haskey
```

Check if data.table has key

```{r}
haskey(x) # logical output
x_key = key(x) # assign the key to object

setkeyv(y, x_key) # set key programmatically to y
```

### filtering rows

#### Helper functions

`%in%` : if element is equal to one of several values\
`%chin%` : fast character checker instead of `%in%`\
`%between%` : numeric check if between\
`%like%` : string like somthing. accepts regex\

```{r}
# 1. %in%
gapminder[year %in% c(1952,1957)]
# 2. %between%
gapminder[lifeExp %between% c(28,40)]
# 3. %chin%
gapminder[,continent := as.character(continent)] # %chin% only works with character vector, and continent was a factor variable
gapminder[continent %chin% c('Asia', 'Europe')]
# 4. %like%
gapminder[continent %like% '^[AE]'] # Asia, Africa, America, Europe


```


#### advanced filtering

CJ and SJ are convenience functions to create a data.table to be used in i when performing a data.table 'query' on x.





### count unique

`uniqueN()` function for counting unique specified variables.

Similarly `DT[, .N, by = var]` does similar thing (value counts).

```{r}
uniqueN(mtcars, 'cyl')
```

### show duplicates

Two approaches

1.  Generating duplicated column

```{r}
state_county[, duplicated := .N >1 , by=c('State','County')]
state_county[duplicated==T]
```

2.  use `duplicated(x)` + `duplicated(x, fromLast= T)`

<!-- -->

1)  using base::duplicated

Use the column name (one and only one column) for duplicate check

```{r}
# single column duplicates : input single column
addon[duplicated(fyear) | duplicated(fyear, fromLast=T)]
```

2)  using data.table S3 method Mutiple columns duplicated rows

```{r}
# multiple column duplicates : input the data.table
addon[duplicated(addon[,.(fyear, gvkey)]) | duplicated(addon[,.(fyear, gvkey)], fromLast = T)]

# or use plyr syntax 
addon %>% .[duplicated(.,by=c('fyear','gvkey'))|duplicated(.,by=c('fyear','gvkey'), fromLast=T)]
```





### drop duplicates

`unique()` function to drop duplicates.

By default, the first observation survives and others are dropped. Refer to `duplicated()` function.

unique(x, incomparables=FALSE, fromLast=FALSE, by=seq_along(x), ...)

For faster way, one can first set keys with `setkey()` and then call `unique()` to drop duplicates by those keys.

```{r}
dim(mtcars)
unique(mtcars) # drop by all variables duplicate
unique(mtcars, by=c('cyl','gear')) # drop by duplicates with cyl and gear
```

As shown below, the first occurrence is marked FALSE and others are TRUE. Those are dropped when `unique()` function was called.

```{r}
duplicated(mtcars, by='cyl') 
```

### na.omit

`na.omit()` function for fast dropping rows that contain NAs.\
Drops any NAs with columns specified.

```{r}
a <- data.table(V1 = c(NA, 1, 2),
                V2 = c(1, NA, 2))
na.omit(a,c('V1','V2'))
```

### Join

Right / Left (by reference) Inner joins are possible.

To use data.table syntax : use `on` in bracket to connect two data.tables\

TIP : remember to use `on` for two data.tables. `by` is already being used for `group by` operations!

Addtiional arguments:

1.  nomatch = 0

    -   it drops non_matching, resulting inner join

2.  roll = {TRUE, FALSE, Inf, -Inf, 'nearest' } - rolling join

3. mult = {'last'} - keep only the last row of join result 


#### Right join

```{r}
# on data.table x, work on rows that matches with data.table y
# on keys column 
x[y, on = .(keys), nomatch= 0] # basically RIGHT JOIN x to y. to make inner join put nomatch = 0
x[y, on = .(xcol = ycol, xcol2 = ycol2)] # when colnames are different 
```


#### Anti join

```{r}
x[!y, on = .(keys)]
```



#### Left join

`i.` prefix to refer to columns that are located in the right data.table (i part of the data.table)

```{r}
# left join
ip[link_t, c('new_added_state','new_added_county') := .(i.statecode, i.countycode), on=.(GeoFIPS = FIPS)] # i.statecode and i.countycode is from link_t data
```

Set keys to join : makes joins a lot more efficient and faster

```{r}
setkey(x, colname)
setkey(y, colname)
x[y] # automatically use on = colname
```

###### Subset left join

In cases it may be required to perform left join on subset of left data.table.

Using `i.` with `fifelse` does the trick.

Example use I had before

```{r}
temp[cpon[,.(Issue_id,Coupon)], 'Coupon' := fifelse(Coupon_type=='F', i.Coupon, NA_real_), on = .(Issue_id)] # only joined when Coupon type is 'F'
```




#### Rolling join




Rolling join is only supported for `=` matches. It is not equal to non-equi join that only closest matching will survive. 
Default rolling join is right join with forward rolling. 
The time on the right table will be fixed, and the closest time on left table will be matched.

```{r}
DT <- data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)], 
                 B = c(5, 4, 1, 9, 8, 8, 6), 
                 C = 6:12, 
                 key = "A,B")
DT
#    A B  C
# 1: a 4  7
# 2: a 8 10
# 3: b 1  8
# 4: b 5  6
# 5: b 8 11
# 6: c 6 12
# 7: c 9  9
```

When `roll=TRUE`, then it tries to match value less than input. Same as `roll=+Inf`.
In above example, since A matches, but B does not match 4, so it matches with B = 1 instead.\
When `roll='nearest'` then it matches nearest one, which is B = 5 in this case.\

`roll = +Inf` : last observation carried forwards (locf)\
`roll = -Inf` : next observation carried backwards (nocb)\

```{r}
DT[.('b',4)]
DT[.('b',4), roll=TRUE] # matches .('b', 1) -> C value 4 is chosen to be match
DT[.('b',4), roll= +Inf] # same

DT[.('b',4), roll= -Inf] # matches .('b', 5)
DT[.('b',4), roll='nearest'] # matches .('b', 5) in this case
```

When mathcing limits are needed : `roll = 2`

```{r}
DT[.('b',4), roll=2] # search .('b', 2:4) - NA if no matches within
DT[.('b',4), roll=-2] # search .('b', 4:6) - matches ('b', 5)
```

Filling in ends : `rollends =` argument\
When ends of data.table are NAs, it can filled in by using `rollends`\
c(TRUE, TRUE) : fill above and below

```{r}
DT[.('b',-2:10)] # non matched are NAs
#     A  B  C
#  1: b -2 NA
#  2: b -1 NA
#  3: b  0 NA
#  4: b  1  8
#  5: b  2 NA
#  6: b  3 NA
#  7: b  4 NA
#  8: b  5  6
#  9: b  6 NA
# 10: b  7 NA
# 11: b  8 11
# 12: b  9 NA
# 13: b 10 NA

DT[.('b',-2:10), roll=T] # last observation carried forward (locf), and top is NA
#     A  B  C
#  1: b -2 NA
#  2: b -1 NA
#  3: b  0 NA
#  4: b  1  8
#  5: b  2  8
#  6: b  3  8
#  7: b  4  8
#  8: b  5  6
#  9: b  6  6
# 10: b  7  6
# 11: b  8 11
# 12: b  9 11
# 13: b 10 11

DT[.('b',-2:10), roll=T, rollends = T]# fill the top with rollends = T
#    A  B  C
#  1: b -2  8
#  2: b -1  8
#  3: b  0  8
#  4: b  1  8
#  5: b  2  8
#  6: b  3  8
#  7: b  4  8
#  8: b  5  6
#  9: b  6  6
# 10: b  7  6
# 11: b  8 11
# 12: b  9 11
# 13: b 10 11
```



##### Example







### Merge

Using `merge()` function for left, right, inner(default), full join

`merge (x, y, by = c(''))` : inner join

`merge (x, y, x.by = , y.by = , all = T )` : all =T means full, F means inner\

`merge (x, y, x.by = , y.by = , all.x = T )` : all.x = T means Left join

```{r}
# Example code
merge (x, y, x.by = , y.by = , all = TRUE ) # all = TRUE means full, FALSE means inner by default
```

#### merge indicator

```{r}
crsp_mstr = merge(test1[,l:=.I],test2[,r:=.I], by.x= 'ncusip', by.y = 'CUSIP', all=T)
crsp_mstr[, merge_ :=fcase(is.na(l), 'right_only',
                          is.na(r), 'left_only',
                          default='both')]
crsp_mstr[,.N,merge_]
```

#### multiple merge 

with Reduce()

`Reduce()` function reduces argument by putting outcome as input until it is exhausted.

```{r}
Reduce(
  f = function(x,y){paste0(x,y,"|")}, # f takes two arguments
  x = c('a','b','c') # x has three 
)
# evaluate with a, b first and then evaluate with outcome of f(a,b) with c

```

When merging multiple dataset repeatedly with same column, it can be useful.

```{r}
Reduce(
  f = function(x,y){merge(x,y,by='obstime')}
  x = list(DT1,DT2,DT3)
)
# merges DT1 and DT2 and then merge DT3
```




### choose random rows

```{r}
DT[sample(.N, 3)]
```






## `j` operations

DT[i,j,by] 

i : for `filter` and `order` rows\
j : for `select`,`assign/drop`, and `summerize` columns\
by : for grouping operations\

Multiple Assignments :

Use `:=`(col_name = max(col1), col_name2 = min(col2)) form instead, or,\
use `c('col_name','col_name2') := .(max(col1), min(col2))`




### order columns

`setcolorder()` function

If not all variables are explicitly mentioned, then the columns mentioned come front.

```{r}
setcolorder(gapminder, c('year')) # bring year to first
```

### selecting columns

#### basics

Multiple ways to selecting columns

1.  Use list of unquoted variable names `list()` or `.()`\
2.  Use character vector (supported in later version of data.table)\
    When using name object with character vector : use `..`prefix or `with=F`\
    When `:=` is used within j, then `()` for name object\

When selecting EXCEPT columns, use `-` or `!` in front of character vector

```{r}
# When only one colname -> returns vector
iris[1:6,Species] # returns vector, but seldomly used...
# using get() function : get evaluates string as variable
iris[1:6, get('Species')] # equivalent, vector returned

# Notice that selecting with character vector returns data.table 
iris[1:6, .(Species)] # returns dt
iris[1:6, 'Species']   # same
```

```{r}
# unquoted names or character vector
iris[,.(`Species`,Sepal.Length)]  # USE backtick ` ` to include column names with special characters or whitespace
iris[,c('Species','Sepal.Length')] # also returns data table 

# When DEselecting columns, must use character vector
iris[, !c('Species','Sepal.Length')] # deselecting Species and Sepal.Length 
iris[, -c('Species','Sepal.Length')] # deselecting Species and Sepal.Length
```

Selecting columns with outside named objects

```{r}
# using prefix or with=F
my_cols = c('Species','Sepal.Length')
iris[, my_cols, with=FALSE] # with argument to recognize character vector OR

# or use dotdot (but not much generalizable)
iris[, ..my_cols] # check 'upper' environment, not in gapminder DT env
eval(parse(text = 'Species'))
```

```{r}
# DO NOT put list of characters
gapminder[,.('country','Sepal.Length')] # generates new data table and assigns those values 

# in order to evaluate string as column reference : use get() function
gapminder[,.(get('country'))]
gapminder[,.(country)] # equivalent 

# 2nd method : using .SDcols argument to input character vector (or incides)
my_cols = c('country','year')
gapminder[, .SD, .SDcols = c('country','year')] # .SDcols expects character vectors 


```

#### select columns in sequence

Select columns in sequence by its name : use `year:pop` colon notation.

```{r}
names(gapminder) # [1] "country"   "continent" "year"      "lifeExp"   "pop"       "gdpPercap"
gapminder[, .SD, .SDcol=year:pop]
```

#### select columns with name patterns

`grep()` is the fastest. <https://stackoverflow.com/questions/30189979/select-columns-of-data-table-based-on-regex>

When using name patterns to select columns,

1.  Use `grep()` function :: <fastest> ,
2.  Use `%like%` and `with=F` argument
3.  Use `patterns()` function inside `.SDcols` argument
4.  Use `%like%` in `.SDcols` argument

```{r}
# 1. using %like% with = F 
gapminder[, names(gapminder) %like% '^co', with=F]
gapminder[,!names(gapminder) %like% '^co', with=F] # negation

# 2. using grep
gapminder[, grep('^co', names(gapminder)), with=F]
grep('^co', names(gapminder)) # gets indices

# 3. using patterns()
mydt[, .SD, .SDcols = patterns("<regex pattern>")]

# 4. using %like% on .SD (slow)
gapminder[, .SD, .SDcols= names(gapminder) %like% '^co']
gapminder[, .SD, .SDcols= !names(gapminder) %like% '^co'] # negation

```

Benchmark result is on below

```{r}
# Benchmarking
n <- 100000 d
foo_cols <- paste0("foo", 1:30)
big_dt <- data.table(bar = rnorm(n), baz = rnorm(n))
big_dt[, (foo_cols) := rnorm(n)]
head(big_dt)

# Methods
subsetting <- function(dt) {
    subset(dt, select = grep("bar|baz", names(dt)))
}

usingSD <- function(dt) {
    dt[, .SD, .SDcols = names(dt) %like% "bar|baz"]
}

usingWith <- function(dt) {
    cols <- grep("bar|baz", names(dt), value = TRUE)
    dt[, cols, with = FALSE]
}

usingDotDot <- function(dt) {
    cols <- grep("bar|baz", names(dt), value = TRUE)
    dt[, ..cols]
}

usingPatterns <- function(dt) {
  dt[, .SD, .SDcols = patterns("bar|baz")]
}

usingLike <- function(dt) {
  dt[, names(dt) %like% "bar|baz", with=FALSE]
}
# Benchmark
microbenchmark(
    subsetting(big_dt), usingSD(big_dt), usingWith(big_dt), usingDotDot(big_dt), usingPatterns(big_dt), usingLike(big_dt),
    times = 5000
)

# Unit: microseconds
#                   expr     min        lq     mean    median        uq       max neval
#     subsetting(big_dt) 370.582  977.2760 1194.875 1016.4340 1096.9285  25750.94  5000
#        usingSD(big_dt) 554.330 1084.8530 1352.039 1133.4575 1226.9060 189905.39  5000
#      usingWith(big_dt) 238.481  832.7505 1017.051  866.6515  927.8460  22717.83  5000
#    usingDotDot(big_dt) 256.005  844.8770 1101.543  878.9935  936.6040 181855.43  5000
#  usingPatterns(big_dt) 569.787 1128.0970 1411.510 1178.2895 1282.2265 177415.23  5000
#      usingLike(big_dt) 262.868  852.5805 1059.466  887.3455  948.6665  23971.70  5000

```

### dropping columns

Drop columns from a data.table : two methods

1.  Drop explicitly using `:= NULL`\

2.  Anti-Select

3.  Drop explicitly

When dropping variables programatically by calling an object name, it should be `()` wrapped.

```{r}
drop_cols = c('continent','year')
gapminder[, (drop_cols) := NULL ] # should be wrapped when := operation is used with
gapminder[, drop_cols := NULL, with=F] # this is deprecated

```

2.  Anti-Select

```{r}
gapminder[,!c('pop','year')]

drop_cols = c('pop','year')
gapminder[,!drop_cols, with=F] 
gapminder[,!..drop_cols] # same


# appendix
gapminder[,(drop_cols)] # paranthesis does not behave as expected


```

### replace values

Below replaces all values inplace, which is memory efficient and fast.

Empty strings to NA, all DT

```{r}
for (colnum in seq_along(sdc)){
  set(sdc, i=which(sdc[[colnum]]==''), j=colnum, value=NA)
}
```

NA to -1 conversion, chosen columns

```{r}
for (colnum in seq_along(sdc[,S_P_I_LT_MAX:Moody_s_U_LT_MAX])){
  set(sdc, i=which(is.na(sdc[[colnum]])), j=colnum, value=-1)
}
```

NA to 0 conversion, all DT

```{r}
f_dowle2 = function(DT) {
  for (i in names(DT))
    DT[is.na(get(i)), (i):=0]
}
```

#### nafill imputation

Currently only numeric vectors work well

```{r}
colnames = c('col1','col2')
DT[, (colnames) := lapply(.SD, function(x) nafill(x, fill = mean(x, na.rm=T))),
   .SDcols = colnames ,
   by = .(TICKER, Date)] # filling NAs with groupby means
```



Use for only simple cases

```{r}

# last observation carried forward (locf)

# next obseravation carried backward (nocb)

?nafill
x = 1:10
x[c(1:2, 5:6, 9:10)] = NA
# [1] NA NA  3  4 NA NA  7  8 NA NA
nafill(x, "locf")
# [1] NA NA  3  4  4  4  7  8  8  8

dt = data.table(v1=x, v2=shift(x)/2, v3=shift(x, -1L)/2)
#     v1  v2  v3
#  1: NA  NA  NA
#  2: NA  NA 1.5
#  3:  3  NA 2.0
#  4:  4 1.5  NA
#  5: NA 2.0  NA
#  6: NA  NA 3.5
#  7:  7  NA 4.0
#  8:  8 3.5  NA
#  9: NA 4.0  NA
# 10: NA  NA  NA

nafill(dt, "nocb") %>% as.data.table
#     V1  V2  V3
#  1:  3 1.5 1.5
#  2:  3 1.5 1.5
#  3:  3 1.5 2.0
#  4:  4 1.5 3.5
#  5:  7 2.0 3.5
#  6:  7 3.5 3.5
#  7:  7 3.5 4.0
#  8:  8 3.5  NA
#  9: NA 4.0  NA
# 10: NA  NA  NA

setnafill(dt, "locf", cols=c("v2","v3"))
```

#### zoo::na.locf, na.nocb






### assigning columns

Assigning one column : `col_name := max(col1)` can be used.\

Assigning multiple columns : `:=` operator cannot be inside of list.

1.  Use `:=`(col_name = max(col1), col_name2 = min(col2)) or,

2.  use `c('col_name','col_name2') := .(max(col1), min(col2))`



#### lapply .SD assign

To programmatically write multiple functions with `lapply`

```{r}
colnames = c('name1','name2')
# !doesn't work
DT[, colnames := lapply(.SD, mean), .SDcols = colnames] # does not work : LHS should be a character vector

# Correct
DT[, (colnames) := lapply(.SD, mean) .SDcols = colnames] # works

```



### rename columns

`setnames()` is data.table function that works inplace.

```{r}
# Ex 1
setnames(gapminder, old=c('year','pop'),new=c('YEAR','POP'), skip_absent=T)
```

Add suffixes by reference

```{r}
setnames(gdi_us_w, names(gdi_us_w)[3:13], paste0(names(gdi_us_w)[3:13],'_us'))
```

Example : tagging function

```{r}
tag_names <- function(DT,cols){
  setnames(DT, old = cols, new= paste0(cols, '_mytag'), skip_absent = T)
}
tag_names(gapminder, c('country','invalid')) 
names(gapminder)
```

### columns class converting

When looping over, pass if it generates error

```{r}
data[,grep('Date', names(data), value=T) := 
       lapply(.SD, 
              function(x) tryCatch(
                as.Date(x, tryFormats = c('%Y-%m-%d','%Y-%m-%d %H:%M:%S')), error= function(e) NULL)
              ) ,
     .SDcols = grep('Date', names(data), value=T)]

```

### grep function

`grep(pattern='^engine', x = names(gapminder))` returns indecies that are matching, and argument `value=T` returns values instead.

```{r}
col_pattern = grep('^co', names(gapminder), value=T)
print(col_pattern)
gapminder[,.SD,.SDcol = col_pattern]
```

However, more intuitive and beautifuly way:

```{r}
gapminder[, names(gapminder) %like% '^co']
```


### lapplying function calls

`lapply` and `sapply` works slightly differently : `lapply` returns DT, `sapply` returns a vector.

```{r}
gapminder[,lapply(.SD,function(x){mean(is.na(x))})] # returns DT
gapminder[,sapply(.SD,function(x){mean(is.na(x))})] # returns vector
```

### lead & lag columns

`shift(x, n=1L, fill=NA, type=c("lag", "lead", "shift"), give.names=FALSE)`\
To generate a column with lead or lag.\

```{r}
gapminder[,.(
  lag1_year = shift(year, n=1, type='lag'),
  lifeExp,
  pop), by=country]
```


## `by` operations


## `env` operations

## `mult` opertations

The default is `mult = all`, which means all mathcing results would be retrieved.
Other optionss are `first` or `last`.

## Special symbols

### `.N`

`.N` refers to *the number of rows* for each group (if by argument was there)

Use cases :

1)  Value counts : NAs would also be counted.

```{r}
mtcars[,.N, by=cyl] # calculate nrow (.N) for each unique value of cyl
```

2)  Last row number :

```{r}
# last row
gapminder[,.SD[.N], by=continent]
# first row
gapminder[,.SD[1], by=continent]

```

```{r}
gapminder[,.SD[which.min(lifeExp)], by=continent]
```

3)  Generating sequence numbers to the members of each group

with `seq_len()` function

```{r}
gapminder[, seq_len(.N), by=.(continent, country )]
```



### `.SD`

`.SD` is useful especially when\

1)  lapplying same function over all or specific columns(`.SDcols`) or\

2)  subgroup operations\

3)  mentioning data.table itself within the bracket\

```{r}
# Example for 1) 
gapminder[, lapply(.SD, sum), .SDcols= c('lifeExp', 'pop'), by= continent][,setnames(.SD,'lifeExp','LIFEEXP')]

gapminder[,.SD, .SDcols = 'year']
```



#### *Multiple `.SD`s

```{r}
d = as.data.table(iris)
d[, 
  c(lapply(SD1, sum), lapply(SD2, mean)), 
  env = list(SD1 = as.list(c('Sepal.Length','Sepal.Width')), 
             SD2 = as.list(c('Petal.Length','Petal.Width'))),
  by = Species]

as.list(names(d))
```



### `.I`

.I is useful to get the row number.

### `.EACHI`

Used on by part of operations.

### `.GRP`

Assigns integer numbers to each groups


#### Assign Chunk group

```{r}
n = 8 # number of chunks
DT[, rep(1:n, each = round(.N/8), length.out = .N)]
```



### `.GRPN`

Assigns the number of observations to each groups



## Function with data.table

### get(), ()

`get()` and `()` are very useful notations when defining a function with `data.table`\

`get()` function to evaluate string to refer column names.\

`()` to assign a column with pre-defined name object

When assigning new column name, `get(colname)` doesn't work properly. Use `(colname)` instead.

```{r}
# add 10 and assign new column with suffix _plus10

# ERROR 
add10 <- function(DT, cols){ # DT is data.table, cols is character vector 
    
  for (col in cols){
    new_name <- paste0(col, "_plus10")
    
    
    # Here gets Error : data.table assigns colname as new_name literally, not as intended as .._plus10 
    DT[, new_name := get(col) + 10] # assigns column name literally as new_name (!) which is not intended behavior

        # ANSWER: use () for generating new column names
    DT[, (new_name) := get(col) + 10] # works correctly
  }
}
add10(gapminder,"year") # no error returned
print(gapminder)
```



## Read multiple files as data.tables

Use `rbindlist()` function together with `list.files()`\
`rbindlist` is similar to `rbind`, yet more efficient and faster.\

`rbindlist` takes a list of data.tables and rbind it.

```{r}
# Example workflow of reading multiple csv files and concatenating them
table_files = c('sales2000.csv','sales2001.csv')
list_tables = lapply(table_files, fread) # list of data.tables
rbindlist(list_tables)
```

```{r}
# fill T fills NA if columns does not match each other, idcol = specifies which file it
rbind (x, y , fill=T, idcol='id_name' ,use.names = TRUE)
rbindlist(some_list, use.names="check", fill=FALSE, idcol=NULL) # use.names='check' => by default it warns if position does not match names
```

## Combinations

`foverlaps()` is fast way to implement. My use case was to generate an edgelist for network analysis.

```{r}
as.data.table(t(combn(d$id, 2))) # slow
```

```{r}
d <- data.table(id=as.character(paste0("A", 10001:15000))) 
d[, `:=`(id1 = 1L, id2 = .I)] ## add interval columns for overlaps
setkey(d, id1, id2)
```

```{r}
foverlaps(d, d, type='within', which=T)[xid != yid]
olaps = foverlaps(d, d, type='within', which=T)[xid != yid]
ans = setDT(list(d$id[olaps$xid], d$id[olaps$yid]))

```

## Set operations

When working with multiple and duplicated data.tables

`fintersect()` : choose UNIQUE set of rows that are common on two data tables. If `all=TRUE`, then numbers of rows duplicated that intersect will be also returned.\
`funion()` : choose UNIQUE set of UNION rows\
`fsetdiff()` : choose UNIQUE set of rows that are EXCLUSIVELY existing on the left\



## MELT

data.table melting : m to 2 columns

`melt()` function

Multiple columns (measure.vars) -\> two columns (variable, value)

-   one with name of column(variable.name),\
-   and one with values(value.name)

id.vars = columns that are kept from melting

```{r}
melt(ebola_wide, id.vars="Location", measure.vars = c("Week_50", "Week_51"), 
     variable.name = "period", value.name = "cases"
     ) 

# except for Location, 
# Week_50 - 51 columns will be stacked and other columns will be dropped, 
# generating key (variable) and value pair
```

```{r}
?melt
```

## DCAST

data.frame casting.

`dcast` function : multiple casting is available.

Formula expression

-   LHS = Identifing variables that should stay the same

-   RHS = The separator variable

-   value.var = values that are filling in the new columns

-   if multiple columns :

    Special variables : `.` and `...`\
    `.` : no variable\
    `...` : all variables not mentioned\

### Example

```{r}
# The name of new columns : year (2000, 2001) are there
# (one column) => many columns

# the value that will be filling those new columns : value.var
dcast(gdp_oceania, formula = country + continent ~ year, value.var = c("gdp", "population"))
# year as separator variable : 2000, 2001, 2002 ,... will be spreaded to the right hand side
# Keep country and continent as identifier variables, cast year variable and values are gdp (generate 2000_gdp, 2001_gdp etc) and population (2000_population, 2001_population etc.)

# when identifiers are NOT UNIQUE, then it tries to aggregate with some functions. 
# When identity needs to be filled, make sure the identifiers uniquely match each observations.


```


## CJ

Cross-Join function

```{r}
a <- c("ABC", "ABC", "DEF", "GHI")
b <- c("2012-05-01", "2012-05-02", "2012-05-03", "2012-05-04", "2012-05-05")
CJ(a,b) # All are treated as unique
CJ(a,b,unique=T) # only Unique ids are used
```

### example

```{r}
times = seq(ISOdate(2011,1,1,9,00, tz='UTC'), ISOdate(2011,12,31,16,00, tz='UTC'), by = '30 min')
times = times[9 <= hour(times) & hour(times) <= 16 ]
```








# STRING MANIPULATION

## regular expressions

------------------------------------------------------------------------

Characters 
`.` : `.` means any character\
`\\.` : escape, literaly dot\
`\\s` : one spacebar\
`[0-9]` : one numeric\
`\\d` : single digit\
`[a-zA-z]` : one alphabet chracter\
`\\w` : word character including digit (alphanumeric)\
`[:punct:]` : punctuation `.`, `!`, `,`\

Multipliers

`*` : 0, 1, ore more numbers of prefix\
`+` : 1 or more numbers of prefix\
`?` : 0 or 1 (maybe there or not)\
`*?` : makes multiplier lazy instead of greedy\
`^` : starts with\
`$` : ends with\

Custom pattern `[]` : custom pattern\
`[^a-z]` : inverse of custom pattern\

------------------------------------------------------------------------

Repetitions

`\\w{2}` : exactly two characters\
`\\w{2,3}` : two to three characters\
`\\w{2,}` : minimum 2 to unlimited max\
`\\w+` : 1 or more\
`\\w*` : 0, 1, or more\

Groups

`()` : grouping for replacement and extraction etc.\
`(?:)` : non-capturing groups\
`\\1`,`\\2`, : backreference = referring the first, second, ... capture group\

Negations 

`\\D` : all but digits\
`\\W` : all but alphanumerics\
`[^a-zA-Z]` : all but alphabet\

Greedy vs Lazy

```{r}

str_match("Toy story 3 in digital 3d is really good", '.*3')  # Greedy : "Toy story 3 in digital 3"
str_match("Toy story 3 in digital 3d is really good", '.*?3') # Lazy : ""Toy story 3"
```


Regular expressions with base r package:

`grep` : Give indices that mathes pattern\
`grepl` : Give TRUE / FALSE result list that matches pattern\
`sub` : Substitute first matching result\
`gsub` : Substitute ALL\

```{r}
awards <- c("Won 1 Oscar.",
  "Won 1 Oscar. Another 9 wins & 24 nominations.",
  "1 win and 2 nominations.",
  "2 wins & 3 nominations.",
  "Nominated for 2 Golden Globes. 1 more win & 2 nominations.",
  "4 wins & 1 nomination.")
# use sub : substitute first occurence
sub(".*\\s([0-9]+)\\snomination.*$", "\\1", awards)
```



```{r}
test = c('koala','panda','monkey')
grep('.a$', test) # gives indecies that are TRUE
grepl('.a$',test) # gives logical back (TRUE/FALSE)
sub('a','A',test) # substitutes first matches
gsub('a','A',test) # substitue ALL matches

```

## base R

### paste

```{r}
# use of paste() function to attach string
pqdata = pqdata %>%
  mutate(new_state = paste('new_', State, sep='')) # mutate
pqdata %>% select(State, new_state)

```

### substr

extract substring of the string

```{r}
substr('This is the example text',1,6) 
# "This i"
```

### strsplit

splits each character elements and returns list

```{r}
strsplit(c('I_My_Me_Mine','You_Your_You_Yours'), '_')
# [[1]]
# [1] "I"    "My"   "Me"   "Mine"
# 
# [[2]]
# [1] "You"   "Your"  "You"   "Yours"
```

#### tstrsplit

From `data.table` package, transpose and strsplit

split a column into several, and generate columns

```{r}
dt[, c("PX", "PY") := tstrsplit(PREFIX, "_", fixed=TRUE)]
#    PREFIX VALUE PX PY
# 1:    A_B     1  A  B
# 2:    A_C     2  A  C
# 3:    A_D     3  A  D
# 4:    B_A     4  B  A
# 5:    B_C     5  B  C
# 6:    B_D     6  B  D
```

### grep, grepl

grep : globally search for regular expression and print

```{r}
str_match('Payload : "Adam, 5, 3", headers: h', 
          pattern = '([A-Za-z]+),\\s(\\d),\\s(\\d)') # full matched and then group
grep('([A-Za-z]+),\\s(\\d),\\s(\\d)',
     'Payload : "Adam, 5, 3", headers: h', value=T)
```

grepl : print logical of grep

### sub

substitute

### gsub

global substitute

#### Group Pattern extraction

with gsub

```{r}
gsub("\\((.*?) :: (0\\.[0-9]+)\\)",
     "\\1 \\2",
     "(sometext :: 0.1231313213)")
```

### format strings

```{r}
?format
# Define the names vector
income <- c(72.19,1030.18,10291.93,1189192.18)
income_names <- c("Year 0", "Year 1", "Year 2", "Project Lifetime")

# Create pretty_income
pretty_income <- format(income, digits=2, big.mark=',')
print(pretty_income) # character vector now

```

```{r}
# Create dollar_income
dollar_income <- paste('$', pretty_income,sep='')
print(dollar_income)
class(dollar_income)
dollar_income
# Create formatted_names
formatted_names <- format(income_names, justify = 'right')
formatted_names
# Create rows
rows = paste(formatted_names,dollar_income, sep='   ')
print(rows)
# Write rows
writeLines(rows)
```

## stringr

`str_c` : concatenate, similar to `paste` function\

`str_count` : number of times pattern occured on each character elements\
`str_remove` : remove a pattern\

`str_trim` : stripping whitespaces beginning and the end\
`str_to_upper`\
`str_to_lower`\

`str_length` : calculate length of character for all elements, works on factors as well\
`str_sub` : substring access to each character elements\
`str_pad` : pad numbers to match the width like leading zeros `000470`

`str_view` : shows how (regex) patterns match to the string vector

```{r}
pqdata %>% 
  mutate(test1 = str_detect(County, 'as')) %>%  # detect if word 'as' is found within column
  mutate(test2 = str_replace(County, 'as','AS')) %>% 
  mutate(test3 = str_remove(County,'as')) %>% 
  mutate(test4 = str_to_upper(County)) %>% 
  select(County, test1:test4)
```

```{r}
# str_split : returns list, for the number of split outcome is not determined with character vector input
str_split('Tom & Jerry & Albin' , pattern = ' & ')
str_split('Tom & Jerry & Albin' , pattern = ' & ', n=2) 

str_split(c('Tom & Jerry', 'Tom & Jerry & Albin'), pattern = fixed(' & '))  # fixed() means do not take this as regex pattern 
str_split(c('Tom & Jerry', 'Tom & Jerry & Albin'), pattern = ' & ', simplify = T) # returns matrix to match 
```

```{r}
# str_view example
x <- c("cat", "coat", "scotland", "tic toc")
str_view(x, pattern = 'ca')

```

### str_view()

Shows which patterns match

```{r}
str_view(c('ab', 'aab', 'b'), '(a)?b')
```

### str_match()

: extract matched groups from a string\

```{r}
str_match(c('The tiger','The dragon'), 'tiger')
#     [,1]   
# [1,] "tiger"
# [2,] NA     

str_match('Payload : "Adam, 5, 3", headers: h', 
          pattern = '([A-Za-z]+),\\s(\\d),\\s(\\d)') 
#      [,1]         [,2]   [,3] [,4]
# [1,] "Adam, 5, 3" "Adam" "5"  "3" 
```

### str_detect()

: Detect string and yield logical vector

equivalent to `grepl`

```{r}
str_detect(c('The tiger','The dragon','cat'), '^The')
# [1]  TRUE  TRUE FALSE
grepl('^The', c('The tiger','The dragon','cat'))

microbenchmark::microbenchmark(
  str_detect(c('The tiger','The dragon','cat'), '^The'),
  grepl('^The', c('The tiger','The dragon','cat')),
  times= 10000L
  )
```

### str_which()

detect pattern and yield the value that maching

### str_subset()

`str_subset` : detect and filter only those matches the pattern   Equivalent to `grep(pattern, x, value =T)`

### str_replace()

: replace `the first occurrence` that matches a pattern to specified string

similar to `sub`

`str_replace_all` : replace all if pattern occurs multiple times

similar to `gsub`

```{r}
# simple replacement

# group match and replacement
str_replace(
  "Payload : 'Adam, 5, 3', headers: h", 
  pattern = '([A-Za-z]+),\\s(\\d),\\s(\\d)',# matches (Adam),(5),(3)
  replacement = '\\1 tried to log in \\2 times'
  )

sub(
  pattern = '([A-Za-z]+),\\s(\\d),\\s(\\d)',
  replacement = '\\1 tried to log in \\2 times',
  x = "Payload : 'Adam, 5, 3', headers: h"
)
microbenchmark(
  str_replace(
  "Payload : 'Adam, 5, 3', headers: h", 
  pattern = '([A-Za-z]+),\\s(\\d),\\s(\\d)',# matches (Adam),(5),(3)
  replacement = '\\1 tried to log in \\2 times'
  ),
  sub(
  pattern = '([A-Za-z]+),\\s(\\d),\\s(\\d)',
  replacement = '\\1 tried to log in \\2 times',
  x = "Payload : 'Adam, 5, 3', headers: h"
  )
)
```

### str_split()

: split strings apart by separator specified\

similar to base `strsplit`

```{r}
strsplit(c('I_My_Me_Mine','You_Your_You_Yours'), '_')
str_split(c('I_My_Me_Mine','You_Your_You_Yours'), '_')
```

## glue package

When r code string needs to be evaluated within the string, glue package can be useful.

```{r}
library('glue')
animal <- "shark"
verb <- "ate"
noun <- "fish"
string="Sammy the {animal} {verb} a {noun}." # when called by glue, animal, verb, noun are executed in r
glue(string)
```

Can be used to glue (paste or attach) two or more strings as well.

```{r}
glue(animal,verb,noun, .sep = ' ')
```

Temporary variable

```{r}
glue(
  'Hi {username}!',
  username = 'Matthew'
)
```

## stringdist package

String distance

```{r}
library(stringdist)
stringdist("saturday", "sunday", method='lv')
```

Finding a match

```{r}
amatch(
  x = 'sonday',
  table = c('friday','sunday','saturday'),
  maxDist =1,
  method='lv'
)
```

### methods

-lv : Levenshtein distance -dl : Damerau-Levenshtein

-qgram : qgrams distance (n-grams) -jaccard : sum of not shared divided by shared -cosine : cosine difference between two vectors

```{r}
qgrams('Honolulu','Hanolulu', q=2) # the more shared, the more close
```

```{r}
stringdist('Honolulu','Hanolulu', method='qgram', q=2) # sum of not shared qgrams
stringdist('Honolulu','Hanolulu', method='jaccard') # more similar, close to zero
stringdist('Honolulu','Hanolulu', method='cosine') # more similar, less theta ( close to zero)
```

## fuzzyjoin package

```{r}
library(fuzzyjoin)

```

stringdist_join()

```{r}
stringdist_join(
  frame_a,
  frame_b,
  by = c("frame_a_col" = "frame_b_col"),
  max_dist = 3,
  distance_col = "distance",
  ignore_case = TRUE
)
```

Custom fuzzy matching

```{r}
func_1 = function(left,right){
  stringdist(left,right) <= 5 # string distance less than 5
}
func_2 = function(left,right){
  abs(left-right) <=3  # number distance less than 3
}

fuzzy_left_join(
  a,
  b,
  by = c('a_col1' = 'b_col1',
         'a_col2' = 'b_col2'),
  match_fun = c( 'a_col1' = func_1, 
                 'a_col2' = func_2 )
)
```

# DATE and TIME

## POSIX

ISO Time format :

    '2021-06-02 12:00:00'
    '%Y-%m-%d %H:%M:%S'


+:--------------------------------------------------------------------------------------------------------------------------------------------------+
| Date                                                                                                                                              |
+---------------------------------------------------------------------------------------------------------------------------------------------------+
| `%Y`: 4-digit year (1982)\                                                                                                                        |
| `%y`: 2-digit year (82)\                                                                                                                          |
| `%m`: 2-digit month (01)\                                                                                                                         |
| `%d`: 2-digit day of the month (13)\                                                                                                              |
+---------------------------------------------------------------------------------------------------------------------------------------------------+
| `%A`: weekday (Wednesday)\                                                                                                                        |
| `%a`: abbreviated weekday (Wed)\                                                                                                                  |
| `%B`: month (January)\                                                                                                                            |
| `%b`: abbreviated month (Jan)\                                                                                                                    |
+---------------------------------------------------------------------------------------------------------------------------------------------------+
| <https://stackoverflow.com/questions/45549449/transform-year-week-to-date-object/45587644#45587644>                                               |
+---------------------------------------------------------------------------------------------------------------------------------------------------+
| `%U`: Week of year as a decimal number (00-53), first Sunday of the year as day 1 of week 1 `%u`: Weekday as a decimal number (1--7, Monday is 1) |
+---------------------------------------------------------------------------------------------------------------------------------------------------+
| Time ?week `%H`: hours as a decimal number (00-23)\                                                                                               |
| `%I`: hours as a decimal number (01-12)\                                                                                                          |
| `%M`: minutes as a decimal number\                                                                                                                |
| `%S`: seconds as a decimal number\                                                                                                                |
| `%T`: shorthand notation for the typical format %H:%M:%S\                                                                                         |
| `%p`: AM/PM indicator\                                                                                                                            |
+---------------------------------------------------------------------------------------------------------------------------------------------------+


### Datetime Conversions

POSIXct is basically an integer number, second after 1970-01-01.

TAKEAWAY : USE `as.POSIXct`!

`Date` objects for date, and `POSIXct` objects for datetime.
`POSIXct` is most preferred and universal - `data.table` forces to use as.POSIXct instead of strptime.



```{r}
as.POSIXct('2017-12-25 12:34:02') # automatic identification of ISO format
strptime('2017-12-25 12:34:02', format = '%Y-%m-%d %T') %>% class # POSIXlt

strftime('2017-12-25 12:34:02') # basic format is recognized
strftime('2020-05-18', format = '%Y-%m-%d') 
```


Benchmark : Integer to Date


```{r}
# integer format to date
microbenchmark::microbenchmark(
  as.Date(as.character(880904), format = '%y%m%d'),
  strptime(as.character(880904), format = '%y%m%d'),
  strptime(880904, format = '%y%m%d'),
  as.POSIXct(as.character(880904), format = '%y%m%d'),
  as.POSIXct(strptime(880904, format = '%y%m%d'))
)

# Unit: microseconds
#                                                 expr    min      lq     mean  median      uq    max neval
#     as.Date(as.character(880904), format = "%y%m%d") 11.630 12.5580 13.25445 12.9200 13.3545 24.865   100
#    strptime(as.character(880904), format = "%y%m%d")  9.883 10.4540 11.16121 10.9035 11.1555 21.288   100
#                  strptime(880904, format = "%y%m%d")  9.863 10.2970 11.17324 10.7460 11.0925 37.027   100
#  as.POSIXct(as.character(880904), format = "%y%m%d") 26.980 27.7465 29.88938 28.4275 29.2475 80.642   100
#      as.POSIXct(strptime(880904, format = "%y%m%d")) 20.241 20.7970 23.07031 21.2615 21.8460 99.860   100
     
# string format to date
microbenchmark::microbenchmark(
  as.Date('880904', format = '%y%m%d'),
  strptime('880904', format = '%y%m%d'),
  as.POSIXct('880904', format = '%y%m%d'),
  as.POSIXct(strptime('880904', format = '%y%m%d'))
)

# Unit: microseconds
#                                               expr    min      lq     mean  median      uq     max neval
#               as.Date("880904", format = "%y%m%d")  7.748  8.2855  9.84746  8.5780  8.9335  32.249   100
#              strptime("880904", format = "%y%m%d")  6.442  6.8290  7.56529  6.9935  7.2230  29.866   100
#            as.POSIXct("880904", format = "%y%m%d") 20.680 21.3785 24.82438 21.6185 22.0085  94.032   100
#  as.POSIXct(strptime("880904", format = "%y%m%d")) 14.944 15.5050 18.69356 15.7945 16.0575 117.387   100

```



### Generate date / time variable

`as.Date`, `as.POSIXct`, `ISOdate`

```{r}
microbenchmark::microbenchmark(
  as.Date('2011-01-01'),  # generates date
  as.POSIXct('2011-01-01'), # POSIXct
  ISOdate(11,1,1) # POSIXct
)

# Unit: microseconds
#                      expr     min       lq      mean   median       uq     max neval
#     as.Date("2011-01-01")  32.563  37.0975  41.44812  39.8715  43.1170 143.752   100
#  as.POSIXct("2011-01-01") 104.800 114.0320 121.04771 117.5525 123.0000 242.566   100
#         ISOdate(11, 1, 1)  61.747  67.3165  71.76522  70.1420  73.3855 129.363   100
```


### Generate Time Sequence


Generate simple time sequence
```{r}
seq(from = ISOdate(2011,1,1), to = ISOdate(2012,1,1), by='30 min')
```


Generate day sequence with time frame
```{r}
TIMESLOTS = seq(from = ISOdate(2011,1,1,9,30), to = ISOdate(2011,1,1,16), by='30 min')
temp = TIMESLOTS

for (i in 1:364){
  add1 = TIMESLOTS + as.difftime(i, units='days')
  temp = c(temp, add1)
}
```

Only business days (weekdays)

```{r}
days = seq(from = ISOdate(2011,1,1), to = ISOdate(2011,1,15), by = 'day')
days[!weekdays(days) %chin% c('Saturday','Sunday')]
```





## data.table functions

For extracting integer date parts : for FAST sorting , aggregating and joining purposes

`year(x)` : 4 digit year\
`quarter(x)`\
`month(x)`\
`isoweek(x)`\
`week(x)`\
`yday(x)` : day of the year (1-366)\
`wday(x)` : day of the week\
`mday(x)` : day of the month (1\~31)\
`hour(x)` : hour (1\~24)\
`minute(x)`\
`second(x)`

## base R

```{r}
# Get the current date: today
today = Sys.Date() # Date
# Get the current time: now
now = Sys.time() # POSIXct
# See what now looks like under the hood
today;now
unclass(today);unclass(now)
```

### numeric to date

It's easier to convert to character 

#### numeric to character


```{r}
microbenchmark::microbenchmark(
  formatC(880904),
  as.character(880904)
)
# Unit: nanoseconds
#                  expr   min      lq     mean median      uq   max neval
#       formatC(880904) 16553 22516.0 29228.28  24649 28396.0 95061   100
#  as.character(880904)   985  1066.5  1993.59   1671  1924.5 15206   100

?as.ITime


as.POSIXct(as.character(880904), format = "%y%m%d")
```






#### as.Date

`as.Date()`

```{r}
# Definition of character strings representing dates
str1 <- "May 23, '96"
str2 <- "2012-03-15"
str3 <- "30/January/2006"

# Convert the strings to dates: date1, date2, date3
date1 <- as.Date(str1, format = "%b %d, '%y")
date2 = as.Date(str2, format = "%Y-%m-%d")
date3 = as.Date(str3, format = '%d/%B/%Y')


# Convert dates to formatted strings
format(date1, "%A") # take out weekday (full)
format(date2, "%m") # take out month 2 digit
format(date3, "%b %Y") # take out abb. month 4 digit year
```

```{r}
# Definition of character strings representing times
str1 <- "May 23, '96 hours:23 minutes:01 seconds:45"
str2 <- "2012-3-12 14:23:08"

# Convert the strings to POSIXct objects: time1, time2
time1 <- as.POSIXct(str1, format = "%B %d, '%y hours:%H minutes:%M seconds:%S")
time2 = as.POSIXct(str2, format = '%Y-%m-%d %H:%M:%S')

# Convert times to formatted strings
format(time1, format= '%M')
format(time2,format='%I:%M %p')
```

tryFormats : converts to date with different noticeable formats

```{r}
data[,grep('Date', names(data), value=T) := lapply(.SD, function(x) tryCatch(as.Date(x, tryFormats = c('%Y-%m-%d','%Y-%m-%d %H:%M:%S')), error= function(e) NULL)) , .SDcols = grep('Date', names(data), value=T)]
```

### time difference

`time_length()` function

```{r}
# years to maturity
fisd[, YEAR_TO_MAT := time_length(MATURITY-DATED_DATE, unit = 'year')]
```

## anytime

Automatid date and time parser, fast!


## lubridate

Automatic date and time parser, easy to work with!

```{r}
library('lubridate')
x = '2020, Apr 12'
ymd(x)

# Parse as date and time (with no seconds!)
mdy_hm("July 15, 2012 12:56")
```

```{r}
# Add a line to create whole months on air variable
baker_time <- baker_time  %>% 
  mutate(time_on_air = interval(first_date_appeared_uk, last_date_appeared_uk), # interval object
         weeks_on_air = time_on_air / weeks(1), # in weeks
         months_on_air = time_on_air %/% months(1)) # in month (modulo - round down version)
```

## POSIXct : base package

-   `POSIXlt` : list format time
-   `POSIXct` : calander time - integer

### Converting integer posix to posixct

```{r}
# Create POSIXct dates from a hypothetical Excel dataset
#  excelDT: integer "days since January 1, 1900"

excelDT[, posix := as.POSIXct(as.Date(timecol, origin = "1900-01-01"), tz = "UTC")]

# Convert strings to POSIXct
# stringDT: string dates like "2017-01-01"

stringDT[, posix := as.POSIXct(timecol, tz = "UTC")]

# Convert epoch seconds to POSIXct
# epochSecondsDT: integer seconds since January 1, 1970 ("epoch seconds")

epochSecondsDT[, posix := as.POSIXct(timecol, tz = "UTC", origin = "1970-01-01")]

# Convert epoch milliseconds to POSIXct
# epochMillisDT: integer milliseconds since January 1, 1970 ("epoch millis")

epochMillisDT[, posix := as.POSIXct(timecol / 1000, tz = "UTC", origin = "1970-01-01")]

```

### Generating posix

```{r}
# Generate a series of dates
march_dates <- seq.POSIXt(from = as.POSIXct("2017-03-01"), to = as.POSIXct("2017-03-31"), length.out = 31)
# Generate hourly data
hourly_times <- seq.POSIXt(as.POSIXct("2017-05-01 00:00:00"), as.POSIXct('2017-05-02 00:00:00'), length.out = 1 + 24) # to make hourly observations : 1 day 24 hours : 25 observations
?as.POSIXct

# Generate sample IoT data
iotDT <- data.table(
    timestamp = seq.POSIXt(as.POSIXct("2016-04-19 00:00:00"), as.POSIXct("2016-04-20 00:00:00"), length.out = 25),
    engine_temp = rnorm(n = 25),
    ambient_temp = rnorm(n = 25)
)
head(iotDT)
```

## xts package

```{r}
library(xts)
```

```{r}
# Simulated data : 2017/6/15 from 00 to 01, 100 observations
some_data <- rnorm(100)
some_dates <- seq.POSIXt(
  from = as.POSIXct("2017-06-15 00:00:00Z", tz = "UTC"),
  to = as.POSIXct("2017-06-15 01:00:00Z", tz = "UTC"),
  length.out = 100
)

# Make your own 'xts' object
myXTS <- xts(x = some_data, order.by = some_dates)
head(myXTS)
# check time zone
tzone(myXTS)
```

```{r}
# All observations after 2017-06-15 00:45:00
fifteenXTS <- myXTS['2017-06-15 00:45:00/']

# 10-minute aggregations
tenMinuteXTS <- to.minutes10(myXTS)
print(tenMinuteXTS) # automatically makes Open, High, Low, Close on 10 min intervals
# 1-minute aggregations
oneMinuteXTS <- to.minutes(myXTS)

``


# TIDYVERSE

## Package : dplyr

Package `dplyr` offers intuitive verbs for data wrangling:\
`filter`\
`mutate`\
`arrange`\
`select`\
`summarize`\

### Filter

`filter` to filter rows

# filter by subset
gapminder %>%
  filter(State=='CA' | Yield_Amount ==100) 
```

### Select

`select` to select cols, rename cols, and reorder cols.

There are helper funcitons that can be used inside `select`.\
`starts_with`, `everything` and `multiple renaming` are those examples.

```{r}
# filter by subset
gapminder %>%
  filter(State=='CA' | Yield_Amount ==100) %>% 
  select(State, starts_with('Y'), ends_with('ity'),ms, everything()) %>%  # starts_with , ends_with, everything
  select(-Yield_Amount) %>%  # drop a column
  select(Newname_ms = ms, everything()) # rename column ms to Newname_ms
```

```{r}
# Multiple columns renaming 
pqdata %>% 
  select(new_name_ = starts_with('Y'), everything()) # multiple renaming with prefix
```

### Mutate

#### Recode column

To Recode column

For basic recode, use `recode` , `recode_factor`\
For multiple recode, use `case_when`\

```{r}
pqdata %>% 
  mutate(State = recode(State, 'PA' = 'PAPAPA', .default = 'Not_my_interest')) %>% # single recode
  mutate(State2 = case_when( # multiple recoding
    State == 'Not_my_interest' ~ 'My_interest',
    State == 'PAPAPA' ~ 'HELLO',
    TRUE ~ NA_character_
    )) %>% 
  select(State,State2)
  
```

### Summarize : aggregation method

When it comes to summarize (or, aggregate) the data into one row, not necessarily boradcasting all the data, use `summerize` verb.

```{r}
# Calculate descriptive statistics
calcium %>%
  # Group by visit and group
  group_by(visit, group) %>%
  # Calculate summary stats of bmd
  summarize(mean_bmd = mean(bmd, na.rm = TRUE), # if na.rm not mentioned, then one NA will make it NA for all
           median_bmd = median(bmd, na.rm = TRUE),
           minimum_bmd = min(bmd, na.rm = TRUE),
           maximum_bmd = max(bmd, na.rm = TRUE),
           standev_bmd = sd(bmd, na.rm = TRUE),
           num_miss = sum(is.na(bmd)),
           n = n())
```

### Joining dataframes

`inner_join()` for Inner join\

```{r}
votes_joined <- votes_processed %>%
  inner_join(descriptions, by = c("rcid", "session"))


```

## Data concatenation (binding)

`rbindlist()` for data.table solution(fastest)\
`rbind()` for data.frames, baseR solution\
`bind_rows()` with tidyverse solution\

```{r}
rbind(2015 = sales_2015, 2016 = sales_2016, idcol = 'year') # now 2015, 2016 will be put into idcol variable 
```

## Cleaning column names

Using `janitor` package

```{r}
library(janitor)
pqdata %>% 
  clean_names(case = 'snake' ) %>% # All lowercase, no whitespace, snake (default)
  clean_names(case = 'upper_camel') # upper camel style
```

## Type (format) conversion

5 types of format in R : `integer`,`numeric`,`character`, `logical`,`factor`

```{r}
# To numeric 
pqdata$ms_yield = as.numeric(pqdata$ms_yield)
# To factor (category)
pqdata$d_call = as.numeric(pqdata$d_call)
pqdata$d_go = as.numeric(pqdata$d_go)
# to date
pqdata$Final_Maturity = as.Date(pqdata$Final_Maturity)
pqdata[,c("ms_yield","d_call","d_go","Final_Maturity")]
```

Instead, when only number parts are needed to be parsed, `readr::parse_number` function can be used within.

```{r}
ratings3 <- ratings2  %>% 
	# Unite and change the separator
	unite(viewers_7day, viewers_millions, viewers_decimal, sep = "") %>%
	# Adapt to cast viewers as a number
	mutate(viewers_7day = parse_number(viewers_7day))
```


# GGPLOT2


`ggplot2()`

```{r, include=F}
# packages
library(tidyverse)
library(gridExtra) # multiple plots 
library(data.table)

# data
data(diamonds)
```

Grammar of graphics (Leland Wilkinson, 1999)

Data : data being plotted 

    - 1. Aesthetics : the scales onto which we map our data (x, y, col, fill, size, alpha, linetype, labels, shape) 
    - 2. Geometrics : the visual elements used for our data (line, point, ...) 
    - 3. Facets : plotting small multiples 
    - 4. Statistics : representations of data to aid understanding (adding regression line to plot) 
    - 5. Coordinates : the space on which the data will be plotted (xlim, ylim, ...) 
    - 6. Themes : All non-data ink




## My samples

### Error bar

`geom_ribbon` : area errorbar with two standard deviation

```{r}
monthly[!is.na(peer_sim_rig_quintile),.(mean(ln_port_mtna,na.rm=T), se_mean(ln_port_mtna)), keyby = .(caldt, peer_sim_rig_quintile)] %>% 
  ggplot(aes(x=caldt, y=V1, color=peer_sim_rig_quintile)) + 
  geom_line() +
  geom_ribbon(aes(ymin = V1 - 2*V2, ymax = V1 + 2*V2, fill=peer_sim_rig_quintile), linetype=0,  alpha=0.2) +
  theme_bw()
```

`geom_errorbar` : dynamite like


```{r}
ggplot(acf_avg, aes(x = I, y = mean)) + 
  geom_col() + 
  geom_errorbar(aes(ymin = mean - 2*se, ymax = mean + 2*se), color = 'blue') +
  theme_bw()
```


### Legend


Legend position, title change

```{r}
ggplot(tot, aes( x= class_name, y=N, fill=ID)) +
  geom_bar(stat='identity', position='fill') +
  theme_bw() +
  ylab('Proportion') +
  xlab('Lipper-Class') +
  theme(legend.position = 'bottom'
        )+
  guides(fill=guide_legend(title="PeerStrength"))
```




### Scaling axis

#### Date ticks

Scale yearly ticks to monthly ticks
```{r}
ggplot(optvol[year(Year) >=2019], aes(x= Year, y= sum_volume,color = cp_flag)) + geom_line() + geom_point() +
  scale_x_date(breaks = '1 month', date_labels = '%b-%y')
```


#### Scientific notation

```{r}
intraday_sum[TIME != '09:30'] %>% 
  ggplot(aes(x=datetime, y=sum_vols_f)) + 
  geom_line() + 
  ggtitle("Intraday Sum Option Trading Volume, Firms")+
  scale_x_datetime(date_breaks = '30 min', date_labels = "%H:%M")+
  scale_y_continuous(labels = scales::scientific)
```


#### Million, K notation

```{r}
scale_y_continuous(labels = scales::label_number(suffix = " M", scale = 1e-6)) 
```





## 1 Aesthetics

### About aesthetics (aes)

Aesthetics and attributes are a bit confusing terms.

Aesthetics : calls variables, and are essential part\

Attributes : calls character vectors, and are non-essential.\
note :: attributes are always called in Geom layer.\

Aesthetics : elements(:= variables) that are being represented x y fill color size alpha : transparency level linetype labels : Text on the plot, or on the axis shape



### Adjusting Aesthetics

xlim() ylim() labs() scale\_\*\_\* position(within the geom function)

#### position

Adjusting overlapping position. How should `ggplot` handle those?

-   identity : just do overlap if plotting on the same coordination\
-   jitter : spread dots
-   dodge : plot bar next to each other
-   jitterdodge : spread dots and place next to each other
-   stack : plot bar on top of each
-   fill : normalize to 1 and fill color by proportion of each
-   nudge

For more detailed adjustments on jitter:

`position_jitter()` function

```{r}
ps = position_jitter(width=0.1)
ggplot(iris) +
  geom_point( aes(x=Sepal.Length, y=Sepal.Width, col=Species, position = ps)) # position ='identity' is the default
```

#### scale

Usually have `scale_*_*()` naming conventions.

scale_x\_ *scale_y\_* scale_color\_ *scale_fill\_* scale_shape\_ *scale_linetype\_* scale_size\_\*

Attributes: name : name of the scale - or, only for axis lable change, use `labs()` function. limits : breaks : expand : gap between the plot and axis labels : category name change

Scale axis and labels

`scale_x_continuous()`\
`scale_color_discrete()`\

```{r}
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species)) +
  geom_point()+
  scale_x_continuous('X label', limits = c(2,8), breaks =seq(2, 8, 3), expand = c(0,0)) + # scale x axis and modify x label
  scale_color_discrete('Label on Species', labels = c('SeToSa','VerSiColor','VirGiniCa')) # legend 
```

## 2 Geometrics

Attributes : change how it looks - ONLY CALLED IN GEOM LAYER!

color = 'red' size = 10 shape = 4 fill = 'black' storke = 2 alpha - 0.3

### Dot plots

#### geom_point()

`geom_point` and `geom_jitter` for describing relationship between two variablaes.

```{r}
# overplotting problem : => make it transparent, or use hollow dots instead, or use geom_jitter
ggplot(diamonds, aes(x = carat, y = price, color= clarity)) +
  geom_point(alpha=0.4)
```

`geom_jitter()` :: equivalent to `geom_point(position='jitter')` For overlapping dots, use `geom_jitter()` to see the density more clearly

```{r}
a1 = ggplot(iris, aes(x= Sepal.Length, y= Sepal.Width, col=Species)) +
  geom_point() + labs(title='geom_point plot')
a2 = ggplot(iris, aes(x= Sepal.Length, y= Sepal.Width, col=Species)) +
  geom_jitter() + ggtitle('geom_jitter plot')
a1;a2
```

one variable scatterplot

Dummy (meaningless) y axis for scatter plot set 0 in y

```{r}
ggplot(mtcars, aes(x = mpg, y = 0)) +
  geom_jitter() +
  ylim(c(-2,2))
```

#### geom_dotpoint()

`geom_dotpoint()` for clear points

```{r}
ggplot(mtcars, aes(as.factor(cyl), wt, fill = as.factor(am))) +
  geom_dotplot(stackdir = "center", binaxis = "y")
```

#### geom_violin()

: For describing distribution of several categories, with more graphical advantage

```{r}
ggplot(iris, aes(x = Species, y = Sepal.Length)) + 
  geom_violin() + # aes(fill = group) : fill color group by variable 'group'
  theme_bw(base_size = 16)
```

### Bar plots

#### geom_histogram()

It is closely related to stat function : `stat_bin()` `geom_histogram()` : For describinag distribution of one dimensional numeric CONTINUOUS variable

```{r}
ggplot(iris, aes(Sepal.Width)) +
  geom_histogram(binwidth = 0.1) #same as above for position = identity is the default
```

Density on y instead of count :

Use `aes(y=..density..)` inside `geom_histogram`

```{r}
ggplot(iris, aes(Sepal.Width, ..density..)) +
  geom_histogram(binwidth=0.1)
```

When `fill=` aesthetic was used, default position is 'stack'

```{r}
ggplot(iris, aes(Sepal.Width, fill=Species)) +
  geom_histogram(aes(y=..density..), binwidth=0.1, position='stack') +
  geom_histogram(aes(y=..density..), binwidth=0.1, position='dodge') +
  geom_histogram(aes(y=..density..), binwidth=0.1, position='fill') +
  geom_histogram(aes(y=..density..), binwidth=0.1, position='identity', alpha=0.7) # overlapped, it needs alpha to show overlapped regions
```

#### geom_bar(), geom_col()

geom_bar() : count of y (or density of y) in each factor x =\> stat_count() geom_col() : actual value of y in each factor x =\> stat_identity()

```{r}
# Change the position argument to stack
setDT(mtcars)
mtcars[,`:=`(am = as.factor(am), cyl=as.factor(cyl))]
mtcars
ggplot(mtcars, aes(x = cyl , fill=am)) +
  geom_bar() + # by default position ='stack'
  scale_fill_brewer(palette = "Set1") # color palette
```

```{r}
# Change the position argument to fill - proportion comparisions
ggplot(mtcars, aes(x = cyl, fill = am)) +
  geom_bar(position='fill') 
```

```{r}
ggplot(mtcars, aes(x = cyl, fill = am)) +
  geom_bar(position='dodge')
```

For more details on dodge position

```{r}
posn_d <- position_dodge(width = 0.2)
ggplot(mtcars, aes(x = cyl, fill = am)) +
  geom_bar(position = posn_d, alpha=0.6)
```

Custom range of colors

```{r}
mtcars[,gear:=as.factor(gear)]
ggplot(mtcars, aes(x = gear, fill = cyl)) +
  geom_bar(position = "fill") +
  scale_fill_brewer()
```

### Line Plots

Useful for time series(continuous or discrete) plots

#### geom_line()

`geom_line` : For comparing time trend of a (several) variables.

```{r}
ggplot(economics, aes(date,unemploy/pop)) +
  geom_line()
```

Lineplot with different groups

```{r}
# EX 1
ggplot(gdi[GeoFIPS=='00000'], aes(x= Year, y = value, color=tencode, group=tencode)) +
  geom_line()
```

Filling area

`geom_area()` :: same as geom_line(position='fill') (?)

```{r}
# use fill argument in aes
ggplot(by_year_continent, aes(x = year, y = medianGdpPercap, fill = continent)) +
  geom_area()
```

#### geom_rect()

Adding rectangular shades on the lineplot

`geom_rect` : rectangular shape

```{r}
# recession time data
recess = structure(list(begin = structure(c(-31, 1400, 3652, 4199, 7486, 
11382), class = "Date"), end = structure(c(304, 1885, 3834, 4687, 
7729, 11627), class = "Date")), .Names = c("begin", "end"), class = "data.frame", row.names = c(NA, 
-6L))

print(recess)

ggplot(economics, aes(x = date, y = unemploy/pop)) +
  geom_rect(data = recess,
         aes(ymax = Inf , xmin = begin, xmax = end, ymin = -Inf),
         inherit.aes = FALSE, fill = "red", alpha = 0.2) +
  geom_line()

```

Frequency polygraph : similar to histogram

```{r}
ggplot(mtcars, aes(mpg, color = cyl)) +
  geom_freqpoly(binwidth = 1)
```

#### geom_smooth()

`geom_smooth()` : with method 'lm', adds regression line

```{r}
mtcars$cyl = as.factor(mtcars$cyl) # factor variable to be colored
ggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +
  geom_point() + 
  geom_smooth(method='lm', se=F) +
  geom_smooth(aes(group=1), method='lm', se=F, linetype=2)
```

#### geom_density_ridges()

```{r}
library(ggridges)
ggplot(iris, aes(x = Sepal.Length, y = Species))+
  geom_density_ridges(bandwidth = 3.5, alpha=0.7)
```

### Box plots

`geom_col` : For describing distribution of several categories

```{r}
gapminder_1952 <- gapminder %>%
  filter(year == 1952)

# Add a title to this graph: "Comparing GDP per capita across continents"
ggplot(gapminder_1952, aes(x = continent, y = gdpPercap)) +
  geom_boxplot() +
  scale_y_log10() +
  ggtitle("Comparing GDP per capita across continents")
```

`fct_rev` : converts to factor vector and reverses the order.

```{r}
# Edit to reverse x-axis order

ggplot(bakers, aes(x = fct_rev(skill), fill = series_winner)) +
  geom_bar()
```

### Crosshairs

#### geom_vline()
`geom_vline()` and `geom_hline()`

Example : let's draw mean of values on the axis.

```{r}
iris = as.data.table(iris)
iris.summary = iris[, lapply(.SD, mean), by=Species]

ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species)) +
  geom_point()+
  geom_vline(data=iris.summary, aes(xintercept=Sepal.Length, col=Species), linetype=2)+
  geom_hline(data=iris.summary, aes(yintercept=Sepal.Width, col =Species), linetype=1)
```


#### geom_hline()



## 3 Facets

### facet_grid()

```{r}
ggplot(mtcars, aes(wt, mpg)) + 
  geom_point() +
  # Facet rows by am and columns by cyl
  facet_grid(rows = vars(am), cols = vars(cyl))
```

#### label facets

Labeling facets : labeller argument

The default value is

-   label_value: Default, displays only the value

Common alternatives are:

-   label_both: Displays both the value and the variable name
-   label_context: Displays only the values or both the values and variables depending on whether multiple factors are faceted

```{r}
ggplot(mtcars, aes(wt, mpg)) + 
  geom_point() +
  # Facet rows by am and columns by cyl
  facet_grid(cols = vars(vs, cyl) , labeller = 'label_context')
```

#### adjusting scale/space

`coored_fixed()` fixes the coordinates along the facets. This conflicts with `scales ='free_x'` and `'free_y'` or both, `'free'`.

```{r}
ggplot(msleep, aes(log(bodywt), log(brainwt))) +
  geom_point( alpha=0.6, shape = 16) +
  coord_fixed() + 
  facet_grid(rows = vars(vore),
             cols = vars(conservation)
             )
```

adjusting plot sizes

```{r collapse=T}
# without free space
ggplot(msleep, aes(log(bodywt), name)) +
  geom_point() +
  facet_grid(rows = vars(vore),
             scales = 'free_y',
  ) +
  ggtitle("without free space")
# with free space (y axis)
ggplot(msleep2, aes( bodywt_log, name)) +
  geom_point() +
  facet_grid(rows = vars(vore),
             scales = 'free_y',
             space = 'free_y') +
  ggtitle("with free space")
```

#### margin plot

```{r}
ggplot(msleep, aes(log(bodywt), log(brainwt))) +
  geom_point( alpha=0.6, shape = 16) +
  coord_fixed() + 
  facet_grid(rows = vars(vore),
             cols = vars(conservation),
             margin=TRUE, # or 'vore' / 'vonservation' for only one side of margin
             )
```

### facet_wrap()

When scale of `BOTH x and y axis` need to be different for EACH plots. All else are similar to `facet_grid()`

```{r fig.height=5, fig.width=10}
ggplot(msleep, aes(log(bodywt), log(brainwt))) +
  geom_point( alpha=0.6, shape = 16) +
  facet_wrap(facet = vars(vore), nrow =2, scales='free' # or 'fixed'
             )

```

### Multiple plots

`gridExtra` package

grid.arrange() function

```{r}
# Example
plot1 = qplot(mtcars$mpg)
plot2 = qplot(mtcars$disp)
grid.arrange(plot1, plot2, ncol=2)
```

## 4 Statistics

Two types of statistics :

1)  called within `geom` : `geom` are the wrapper function of `stat`.
2)  called independently

## 5 Coordinates

starts with `coord_`


### xlim, ylim

Unlike `coord_cartesian`, it drops observations (reduces data)





### coord_cartesian()

ZOOMing in (instead of cutting values)

If coord_cartesian were used

```{r}
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_jitter() +
  geom_smooth(method = "lm", se = FALSE) +
  coord_cartesian(xlim=c(5,6))
```

if corrd was not used : cut offs off the sample with scale

```{r}
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_jitter() +
  geom_smooth(method = "lm", se = FALSE) + # different fit, for it uses reduced data
  scale_x_continuous(limits = c(5, 6))
```

### coord_fixed()

Coordination ratio

```{r}
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_jitter() +
  geom_smooth(method = "lm", se = FALSE) +
  # Fix the coordinate ratio
  coord_fixed() # 1:1 ratio as default
```

```{r}
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_jitter() +
  geom_smooth(method = "lm", se = FALSE) +
  # Fix the coordinate ratio
  coord_fixed(ratio=0.1) # 10:1 ratio
```

### coord_expand()

expand sets a buffer margin around the plot, so data and axes don't overlap.

Setting expand to F draws the axes to the limits of the data.

```{r}
ggplot(mtcars, aes(wt, mpg)) +
  geom_point(size = 2) +
  theme_classic() +
  # Add Cartesian coordinates with zero expansion
  coord_cartesian(expand = F)
```

clip decides whether plot elements that would lie outside the plot panel are displayed or ignored ("clipped").

```{r}
ggplot(mtcars, aes(wt, mpg)) +
  geom_point(size = 2) +
  theme_classic() +
  # Add Cartesian coordinates with zero expansion
  coord_cartesian(expand = F,clip='off') # dots pop up over axis
```

### coordinates vs scales

coordinates does not transform the data, while scales transforms the data.

```{r}
msleep = as.data.table(msleep)
# Produce a scatter plot of brainwt vs. bodywt
ggplot(msleep, aes(bodywt, brainwt)) +
  geom_point() +
  ggtitle("Raw Values")
```

```{r}
# Plot with a scale_*_*() function:
ggplot(msleep, aes(bodywt, brainwt)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  # Add a log10 x scale
  scale_x_log10() +
  # Add a log10 y scale
  scale_y_log10() +
  ggtitle("Scale_ functions")
```

```{r}
# coords does not change the variable scale!
# Plot with transformed coordinates
ggplot(msleep, aes(bodywt, brainwt)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  # Add a log10 coordinate transformation for x and y axes
  coord_trans(x = "log10", y = "log10")
```

### coord_flip()

Flip x and y axis

```{r}
# Plot fcyl bars, filled by fam
ggplot(mtcars, aes(as.factor(cyl), fill = as.factor(am))) +
  # Place bars side by side
  geom_bar(position = "dodge") + 
  coord_flip()
```

### sec_axis()

```{r}
airquality = as.data.table(airquality)
airquality[,`:=`(Month = str_pad(Month,2,'left',0), Day = str_pad(Day,2,'left',0))]
airquality[,Date := as.Date(str_c(1973,Month, Day), format='%Y%m%d')]

# From previous step
y_breaks <- c(59, 68, 77, 86, 95, 104)
y_labels <- (y_breaks - 32) * 5 / 9
secondary_y_axis <- sec_axis(
  trans = identity,
  name = "Celsius",
  breaks = y_breaks,
  labels = y_labels
)

# Update the plot
ggplot(airquality, aes(x = Date, y = Temp)) +
  geom_line() +
  # Add the secondary y-axis 
  scale_y_continuous(sec.axis = secondary_y_axis) +
  labs(x = "Date (1973)", y = "Fahrenheit")
```

### polar coordinates

#### pie charts

```{r}
# Run the code, view the plot, then update it
ggplot(mtcars, aes(x = 1, fill = factor(cyl))) +
  geom_bar() +
  # Add a polar coordinate system
  coord_polar(theta = "y") +
  theme_void() 
```

```{r}
ggplot(mtcars, aes(x = 1, fill = as.factor(cyl))) +
  # Reduce the bar width to 0.1
  geom_bar(width = 0.1) +
  coord_polar(theta = "y") +
  # Add a continuous x scale from 0.5 to 1.5
  scale_x_continuous(limits = c(0.5, 1.5))
```

## 6 Themes

Base plot

```{r}
baseplot = ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) + 
  geom_violin() + # aes(fill = group) : fill color group by variable 'group'
  theme_bw(base_size = 16)
```

`theme()` function

axis panel legend plot

line : all lines (axis) rect : all rectangular shaped ones (legend, panel) text : all labels and texts (x,y axis labels)

element_blank() : remove the item

```{r}
ggplot(iris, aes(x= Sepal.Length, y= Sepal.Width, color= Species)) + 
  geom_jitter(alpha=0.6) +
  theme(line = element_blank(),
        text = element_blank(),
        rect = element_blank())
```

### axis

#### axis.text

#### axis.line

```{r}
baseplot + 
  theme(axis.line = element_line(color = "red", linetype = "dashed"))

```

#### axis.ticks

axis.ticks.length : length of ticks

```{r}
baseplot +
  theme(
    # Set the axis tick length to 2 lines
    axis.ticks.length = unit(2, "cm")
  )
```

### plot

#### plot.margin

```{r}
baseplot +
  theme(
    # Set the plot margin to (10, 30, 50, 70) millimeters
    plot.margin = margin(10, 30, 50, 70, "mm") 
  )

iris[,.N,Species][,.(N=sum(N)),Species]
```

#### plot.title

Title position

```{r}
baseplot + 
  ggtitle('This is the title')+
  theme(
  plot.title = element_text(hjust = 0.5),
  plot.subtitle = element_text(hjust = 0.5)
)
```

### panel

#### panel.grid

Removing veritcal grid

```{r}
baseplot +
  theme(
    panel.grid.major.x = element_blank()
  )
```

### line

### text

### rect

```{r}
ggplot(iris, aes(x = Sepal.Length, fill = Species)) + 
  geom_histogram() + 
  theme(rect = element_rect(fill = 'grey92'))
```

### legend

#### legend.position

Here, the new value can be

"top", "bottom", "left", or "right'": place it at that side of the plot. "none": don't draw it. c(x, y): c(0, 0) means the bottom-left and c(1, 1) means the top-right.

```{r}
ggplot(iris, aes(x = Sepal.Length, fill = Species)) + 
  geom_histogram() + 
  theme(legend.position = 'bottom')
```

#### legend.key

legend.key.size

```{r}
baseplot +
  theme(legend.key.size = unit(3,'cm'))
```

#### legend.margin

margins on legend

```{r}
baseplot + 
  theme(
  # Set the legend margin to (20, 30, 40, 50) points
  legend.margin = margin(20, 30, 40, 50, "pt")
)
```


Bar plot with two variables, x on the categorical, y on the numeric value

```{r}
iris = as.data.table(iris)
iris_summ = iris[, lapply(.SD, sum), by=Species]
iris_summ
```

To draw bar plot with above table : X on Species, y on Speal.Legnth

```{r}
ggplot(iris_summ, aes(x=Species, y=Sepal.Length)) +
  # geom_bar() throws an error for stat='count' is the default
  geom_bar(stat='identity') # throws an error
```

### Built-in themes

For more pre-built themes, `ggthemes` package.

```{r}
theme_classic()
theme_bw()
theme_dark()
```

### update theme

`theme_update()` function

```{r}
theme_recession <- theme(
  rect = element_rect(fill = "grey92"),
  legend.key = element_rect(color = NA),
  axis.ticks = element_blank(),
  panel.grid = element_blank(),
  panel.grid.major.y = element_line(color = "white", size = 0.5, linetype = "dotted"),
  axis.text = element_text(color = "grey25"),
  plot.title = element_text(face = "italic", size = 16),
  legend.position = c(0.6, 0.1)
)
baseplot + theme_recession
theme_recession = theme_update(axis.line = element_line(color='red'))
baseplot + theme_recession
```


### set default theme

`theme_set()` function.

```{r}
theme_set(theme_recession)
baseplot
```

### color arguments

<http://sape.inf.usi.ch/quick-reference/ggplot2/colour>

## Helper functions

### reorder

Plot with re-ordered variable

```{r}
# set y axis as country ordered with respect to logFoldChange
ggplot(aes(x = logFoldChange, y = reorder(country, logFoldChange))) +
  geom_point() +
  # add a visual anchor at x = 0
  geom_vline(xintercept = 0)
```

## EXAMPLES

```{r}
require(gapminder)
gapminder %>% setDT
gm2007 = gapminder[year ==2007, .(country, continent, lifeExp)][lifeExp <= sort(lifeExp)[10] | lifeExp >= sort(lifeExp, decreasing = T)[10]]
```

```{r}
plt_country_vs_lifeExp = ggplot(gm2007, aes(x = lifeExp, y = country, color = lifeExp)) +
  geom_point(size = 4) +
  geom_segment(aes(xend = 30, yend = country), size = 2) +
  geom_text(aes(label = round(lifeExp,1)), color = "white", size = 1.5) +
  scale_x_continuous("", expand = c(0,0), limits = c(30,90), position = "top") +
  labs(title="Highest and lowest life expectancies, 2007", caption="Source: gapminder")
  
plt_country_vs_lifeExp
```

```{r}
plot2 = 
  plt_country_vs_lifeExp +
  theme_classic() +
  theme(
    axis.line.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text = element_text(color = "black"),
    axis.title = element_blank(),
    legend.position = "none"
    )
plot2
```

```{r}
global_mean = 67.00742
x_start = global_mean +4
y_start = 5.5

plot3 = plot2 + geom_vline(xintercept = global_mean, color = "grey40", linetype = 3) +
  annotate(
    "text",
    x = x_start, y = y_start,
    label = "The\nglobal\naverage",
    vjust = 1, size = 3, color = "grey40"
  )
plot3
```

```{r}
x_end = global_mean
y_end = 7.5
# add curve
plot4 = plot3 +
  annotate(
    "curve",
    x = x_start, y = y_start,
    xend = x_end, yend = y_end,
    arrow = arrow(length = unit(0.2, "cm"), type = "closed"),
    color = "grey40"
  )
plot4
```

### dynamite plot

```{r}
mtcars[, fcyl := as.factor(cyl)]
# Plot wt vs. fcyl
ggplot(mtcars, aes(x = fcyl, y = wt)) +
  # Add a bar summary stat of means, colored skyblue
  stat_summary(fun = mean, geom = "bar", fill = "skyblue") +
  # Add an errorbar summary stat std deviation limits
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = "errorbar", width = 0.1)
  
```





# REGRESSION

Basic regression comparison between Stata & R\
In Stata, `i.var` indicates factor (categorical) and `c.var` indicates continuous

`#` : interaction only \<=\> `:`\
`##` : multiplication combination \<=\> `*`

|                        Stata                         |                             R                              |
|:----------------------------------------------------:|:----------------------------------------------------------:|
|                       y x1 x2                        |                        y \~ x1 + x2                        |
|                     y x1,nocons                      |                        y \~ 0 + x1                         |
| gen ylog = log(y) ; gen x2log = log(x2) ; ylog x2log |                     log(y) \~ log(x2)                      |
|               gen x3 = x1 + x2 ; y x3                |                      y \~ I(x1 + x2)                       |
|                        y i.x1                        |                     y \~ as.factor(x1)                     |
|                     y c.x1\#c.x2                     |                         y \~ x1:x2                         |
|                    y c.x1\#\#c.x2                    |                        y \~ x1\*x2                         |
|                    y c.x1\#\#i.x2                    |                   y \~ x1\*as.factor(x2)                   |
|           areg y x1 [w=x3], a(id1) cl(id1)           |  felm(y \~ x1 \| id1 \| 0 \| id1 , data=df, weight = x3 )  |
|      reghdfe y x3 (x2 = x1), a(id1) cl(id1 id2)      | felm(y \~ x3 \| id1 \| (x2 \~ x1) \| id1 + id2, data = df) |
|     reghdfe y x2, a(c.x3\#i.id1 id1) cl(id1 id2)     |              felm(y \~ x2 \| x3:id1 + id1, df              |

`Regression code example`

```{r}
lmOut = glm(count ~ time, data = dat, family = 'gaussian') # OLS regression
lr_1 = glm(goal ~ player, data = scores, family = 'poisson') # GLM (poisson) regression
lr_2 = glm( cbind(success,fail)~x, dataWide, family='binomial') # Logistic regression
busLogit <- glm(Bus ~ CommuteDays, data = bus, family = binomial(link = "logit")) # Logit regression
busProbit <- glm(Bus ~ CommuteDays, data = bus, family = binomial(link = "probit")) # Probit regression

```

## Linear regression

R's base function `lm` covers pretty much.

```{r}
pqdata$d_call = as.integer(pqdata$d_call) # somehow the type is not perfectly converted, so make it inteager again
pqdata$d_go = as.integer(pqdata$d_go)
reg1 = lm(ms_yield ~ d_go+d_call+d_go:d_call, data=pqdata)
reg2 = lm(ms_yield ~ d_go*d_call, pqdata)
reg3 = lm(ms_yield ~ as.factor(d_go)*as.factor(d_call), pqdata)
reg4 = lm(ms_yield ~ as.factor(d_go):as.factor(d_call), pqdata)

texreg::screenreg(list(reg1,reg2,reg3,reg4))
```

### Fixed Effects regression

Package `lfe` gives fast estimation results with high-dimiensional fixed effects.

General expression form :

$$
y  \sim  x1 + x2 \ \  | \ f1 + f2 \ | \ \ (Q \ | \ W  \sim  z1 + z2) \ | \ clu1 + clu2
$$

-   y : regressand\
-   x1, x2 : regressor\
-   f1, f2 : fixed effect variables\
-   Q,W : instrumented variables\
-   z1, z2 : instrument variables\
-   clu1, clu2 : clustered standard error\

Parts that are not used : put 0

example:

$$
y \sim x  \ | \ f1 \ | \ 0  \ | \ 0
$$

```{r}
model = felm(ms_yield ~ d_go + d_call | toxic_state_name + year | 0 | toxic_state_name + toxic_county_name, data = pqdata, cmethod="reghdfe")
summary(model)
```

## Logistic Regression

```{r}
glm(y~x, data, family='binomial') # family ='binomial' uses logit function as link function

```

## programatic formula

```{r}
# our modeling effort, 
# fully parameterized!
f <- as.formula(
  paste(outcome, 
        paste(variables, collapse = " + "), 
        sep = " ~ "))
print(f)
# mpg ~ cyl + disp + hp + carb
```

Example from Munibond project

```{r}
Y = 'ms_w_y_amount'
controls = c('competitive_d','taxable_d','go_d','credit_enhance_d','callable_d','s_f_m_rating','unrated_d','asset_backed_d', 'ln_ms_w_yrs_to_mat','ln_ms_w_size','Insured_Amount') %>% paste(collapse = ' + ')
diff = c('post2001_d * HighIPW5_d')
fixed = c('Dated_Year','statecode') %>% paste(collapse = ' + ') # year and state fixed effect
IV = '0'
clustered = c('Dated_Year','statecode') %>% paste(collapse = ' + ') # year and state clustered 

f = as.formula(
  Y %>% paste(diff, sep=' ~ ') %>% # diff
    paste(controls, sep=' + ') %>% # controls
    paste(fixed, sep = ' | ') %>% # fixed
    paste(IV, sep = ' | ') %>%  # IV reg
    paste(clustered, sep = ' | ') # se clustering
)
print(f)

```

## RMSE

Root Mean Squared Error (Residual Standard Error in R)

$$RMSE \equiv \sqrt{\frac{\Sigma_ie_i^2}{df}}$$

```{r}
mod = lm(y~x, data) 
sqrt(sum(residuals(mod)^2) /df.residual(mod)) # is the RMSE, average difference between actual vs fit
```

## Leverage and Influence

What about outliers?

`.hat` : Leverage in the `augment()` tells how much the explanatory variable is far from its own average.\
`.cooksd` : Cook's distance, tells how much that point influences the overall regression model.

## model.matrix()

R automatically generate matrices from formula. To see how it works in action, use `model.matrix`

```{r}
# verify how the formula works in R
my_size = c(1.1,1.2,1.3)
model.matrix(~my_size) # interpret it as numerical variable
```

```{r}
color = c('red','blue','black')
class(color) # character vector : automatically generate dummies with this variable
model.matrix(~color) # by alphabetical order : 'black' is the first one, and is subsumed to intercept
model.matrix(~color -1) # without intercept : explicit dummies
```








