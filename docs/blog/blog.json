[
  {
    "path": "blog/R Cookbook/",
    "title": "My R CookBook",
    "description": "My simple working R Code cookbook.",
    "author": [
      {
        "name": "Matthew Son",
        "url": {}
      }
    ],
    "date": "2022-09-03",
    "categories": [],
    "contents": "\n\nContents\nMACHINE\nLEARNING\nH2O\nInit and\nShutdown\nh2o.jar location installed by\nR\nRun java\ninstances\n\nSupervised ML: caret\ntrain-test\nsplit\ntrain()\n1 preProcess\n2 trControl\n3 tuneLength\n4 tuneGrid\n\nmodel\ncomparison\nMake\nprediction\nConfusion\nMatrix\nROC curve\nAUC\nglmnet Model\nrandom\nforest Model\ncustom\ntuning\n\nUnsupervised\nML\nkmeans\nhierarchical clustering\nDimensionality reduction\n\nDeep\nLearning\n\nNETWORK\nANALYSIS\nMy Functions\nRemove zero weight edges\n\nConcepts\nCentrality measures\nOverall structure of\nnetwork\nConnectivity\n\nigraph package\nConversion\nBasic\nfeatures\nadd\nattributes\n\ndirected\nnetwork\neigenvector centrality\n\ncensus\nNetwork\nStructure\nnetwork substructure\n\nRelationship measures\ncommunity detection\ncommunity comparison\n\nNetwork\nfeatures\nNetwork Visualization package\nigraph base\nggraph\nggnetwork\nvisNetwork\n\nClassifier\nrelational neighber\nclassifier\n\nAdjacency\nMatrix\nHierarchical clustering\nPredictive\nmodels\nExamples\n\nDATABASE & SERVER COMPUTING\n(HPC)\nSSH\nHypergator\nconn\nWRDS conn\n\nFilezilla\nSite Manager\n\nSCP\nSparklyr\nInstallation\nConfig\n\nWRDS\nSetup for\nWRDS\nInstall package : Rpostgres\n.pgpass file\nDBPLYR ON\nWRDS\nRPostgres\n\nBatch Job on\nWRDS\nBatch job SAS example\nBatch\njob R example\nSubmit batch\njob\nManaging\njobs\nrequest more computing\npower\n\nHiPerGator\nBase Info\nConnection &\nID\nConnect\nRStudio\nServer\nRun R\nSLURM\n\n\nUNIX SHELL\nHead\nWord count\nList files\nMove files\n\nLaTeX\nTinyTex\nBibTex\npackage\ninstall\n\nSweave\nKnitr\nStargazer\n\nGit (Github)\nBasic Commands\nstatus\ndiff\nadd\nreset\ncheckout\ncommit\nlog\nshow\nremove\n\nIgnore files\nConfig\n\nGmailR\nInstall\nUsage\n\nWEB DATA\nRead CSV\nfrom web\nDownload\nfiles\nUsing API\nGET, POST\nPage\nerror handling\nAPI\n\nWeb scraping\nRSelenium\nrvest\nCSS selector\n\n\nCUSTOM\nFUNCTIONS\nDescriptive Statistics\nSummary\nStatistics\nWinsorize\nStandardize\nLinear\ninterpolation\nFast\nWeighted Mean\nRowwise\nMedian\nrowSums\nStandard Error of mean\nCumsum\nignoring NA\nEdgelist\ngenerator\nFund\nflow generator\nLinear interpolation\nDecile\nfactorization\nMultiple\n.SDs\nTime Interval Grouping\nVector\nSplitting\nForce Time\nzone\nFast date/time conversion\n\nR\nCompile & Installation\nRenviron\nSetup\nRprofile\nSetup\nRstudio Setup\nRStudio\nIDE options\nView column\nlimit\n\nR/RStudio removal on MacOS\nOpenMP(data.table, fst) for\nMacOS\nRQuantlib\nRPostgres\nInstall Archived packages\nRtools40\n(Windows)\nDelete all User installed\npackages\nRemove Miniconda\ncompletely\nArrow with snappy\ncompression\nRETICULATE\nPython\nInstallation\nMac\nWindows\nConda setup\n\n\nR MARKDOWN\nBeamer\nEXAMPLES\nHide\nSection slide\nUse custom\nstyle\n\nArticle\nexample\nUse Custeom Pandoc\nTemplate\nImport LaTeX packages\nPrevent\nfloating\nSubfigures\n\nchunk options\nglobal\nchunk options\n\nYAML Header\nYAML example\nparameter\nstyle\n\nMarkdown\ntext\nhyperlink\nembed image\nLaTeX\n\nKable\n(KableExtra)\nTable digits and\nformatting\nInternal\nlinks\n\n\nBASE R /\nRStudio\nR Function\nTricks\nExpressions to string\nassign to string object\nstring\nevaluation\n\nOptions\n.Rprofile\ndata.table\noptions\nstr options\nReticulate\noptions\n\ndata.frame\ntranspose data.frame\n\nshQuote\nouter\nlist.files\nstr\ncut\nfactor\nUnordered\nOrdered\n\nError Handling\nexists()\nstop()\nstopifnot()\ntry()\ntryCatch()\n\nRStudioapi\nrun\ncode on terminal\nincrease View column limit\n\n\nBENCHMARKING\nsystem configurations\nbenchmarking\ntime\n\nPARALLEL\nPROCESSING\nDISK.FRAME\nSetup\nBasic\noperations\nExamples\n\nFUTURE\nBasics of\nFuture\nFuture with data.table\nExport\nglobals\nErrors\nin the future\nSet workers (cores) for each\nprocess\nExamples\ndoFuture\nBasics\nParallel\ndata.table\n\nfuture.apply package\nplan\navailableCores\nfuture_lapply\nreproducibility\n\nparallel\npackage\nclusterApply()\nclusterEvalQ()\nclusterExport()\nparApply()\nprogress bar\n\n\nProgress bar\nDoFuture Foreach\nprogressbar\nfor\nloop progress bar\napply\nprogressbar\n\nData I/O\nList of files in directory\nRead flat\nfiles\nRead excel\nfiles\nRead Stata\nfiles\nDatabase\nbackend\nRead parquet\n\nDATA.TABLE\ni\noperations\nREMOVE ROWS BY REFERENCE\norder rows\nfiltering\nrows\ncount unique\nshow\nduplicates\ndrop\nduplicates\nna.omit\nJoin\nMerge\nchoose\nrandom rows\n\nj\noperations\norder\ncolumns\nselecting\ncolumns\ndropping\ncolumns\nreplace\nvalues\nassigning\ncolumns\nrename\ncolumns\ncolumns class converting\ngrep\nfunction\nlapplying function calls\nlead & lag\ncolumns\n\nby\noperations\nenv\noperations\nmult opertations\nSpecial\nsymbols\n.N\n.SD\n.I\n.EACHI\n.GRP\n.GRPN\n\nFunction with data.table\nget(), ()\n\nRead multiple files as\ndata.tables\nCombinations\nSet\noperations\nMELT\nDCAST\nExample\n\nCJ\nexample\n\n\nSTRING\nMANIPULATION\nregular\nexpressions\nbase R\npaste\nsubstr\nstrsplit\ngrep, grepl\nsub\ngsub\nformat\nstrings\n\nstringr\nstr_view()\nstr_match()\nstr_detect()\nstr_which()\nstr_subset()\nstr_replace()\nstr_split()\n\nglue package\nstringdist\npackage\nmethods\n\nfuzzyjoin\npackage\n\nDATE and TIME\nPOSIX\nDatetime Conversions\nGenerate date / time\nvariable\nGenerate Time Sequence\n\ndata.table functions\nbase R\nnumeric to\ndate\ntime\ndifference\n\nanytime\nlubridate\nPOSIXct : base package\nConverting integer posix to\nposixct\nGenerating\nposix\n\nxts package\nSelect\nMutate\nSummarize : aggregation\nmethod\nJoining\ndataframes\n\nData concatenation\n(binding)\nCleaning column names\nType (format) conversion\n\nGGPLOT2\nMy samples\nError bar\nLegend\nScaling axis\n\n1 Aesthetics\nAbout\naesthetics (aes)\nAdjusting Aesthetics\n\n2 Geometrics\nDot plots\nBar plots\nLine Plots\nBox plots\nCrosshairs\n\n3 Facets\nfacet_grid()\nfacet_wrap()\nMultiple\nplots\n\n4 Statistics\n5 Coordinates\nxlim, ylim\ncoord_cartesian()\ncoord_fixed()\ncoord_expand()\ncoordinates vs scales\ncoord_flip()\nsec_axis()\npolar\ncoordinates\n\n6 Themes\naxis\nplot\npanel\nline\ntext\nrect\nlegend\nBuilt-in\nthemes\nupdate theme\nset default\ntheme\ncolor\narguments\n\nHelper\nfunctions\nreorder\n\nEXAMPLES\ndynamite\nplot\n\n\nREGRESSION\nLinear\nregression\nFixed Effects regression\n\nLogistic\nRegression\nprogramatic formula\nRMSE\nLeverage and Influence\nmodel.matrix()\n\n\nMACHINE LEARNING\nH2O\nInit and Shutdown\n\n\nh2o.init() # initialize, use all core and 25% of memory\nh2o.shutdown() # close connection\n\nh2o.init(max_mem_size = '800G')\n\n\nh2o.jar location installed by\nR\n\n\njar_path = system.file('java', 'h2o.jar', package='h2o')\n\n\nCopy to current working directory\n\n\nfile.copy(jar_path, './h2o.jar')\n\n\nRun java instances\n\n\nrequire(stringr)\nsystem(str_glue('java -Xmx3g -jar h2o.jar -name n1 -nthreads 3  -port 54321'), intern = F, wait =F)\nsystem(str_glue('java -Xmx3g -jar h2o.jar -name n2 -nthreads 3  -port 54322'), intern = F, wait =F)\n\n\nlibrary(h2o)\nh2o.init(startH2O = F)\nh2o.shutdown(prompt = F)\n\nh2o.init()\n\n\nSupervised ML: caret\ncaret package\n\n\nlibrary(caret)\nlibrary(mlbench)\ndata(Sonar)\n\n\ntrain-test split\ncreateDataPartition() function, p=0.7 stands for 70% of\ntrain set. The output is the rownames(index number).\nTo create an index with 70% of the breast_cancer_data to create a\ntraining set and stratify the partitions by the response variable\ndiagnosis:\n\n\nindex <- createDataPartition(breast_cancer_data$diagnosis, p = 0.7, list = FALSE)\n\n\ntrain()\n1 preProcess\nmissing values\nGenerate missing values for demo\n\n\nset.seed(42)\nmtcars[sample(1:nrow(mtcars), 10), 'hp'] = NA\nY = mtcars$mpg\nX = mtcars[,2:4]\n\n\nIf the data is missing at random, medianImpute is\ngood option\nIt missing is non-random, knnImpute should be\nconsidered\nit imputes with observations that are similar\n\n\n\n# medianImpute\nmedian_model <- train(\n  x = X, \n  y = Y,\n  method = \"glm\",\n  preProcess = \"medianImpute\" # knnImpute\n)\n\n\ncenter and scale\n\n\nmodel_prep = train(\n  x = X,\n  y = Y,\n  method = 'glm',\n  preProcess = c('medianImpute', 'center','scale') # combination of preprocessing\n)\nprint(min(model_prep$results$RMSE))\n\n\nlow-information variable\nLow-variance variables don’t contain much information\nThey cause computational problems and bugs, and makes fitting longer\n- good reason to drop them - preProcess = zv to drop\nconstant columns - preProcess = nzv to drop nearly constant\ncolumns - Also caret provides function\nnearZeroVar() to detect those columns - nearZeroVar() takes\nin data x, then looks at the ratio of the most common value to the\nsecond most common value, freqCut, and the percentage of distinct values\nout of the number of total samples, uniqueCut.\nExample: constant column\n\n\nX$bad = 1\ntrain(X,Y,method='glm', preProcess=c('medianImpute','center','scale','pca')) # PCA goes wrong because of constant variable\ntrain(X,Y,method='glm', preProcess=c('zv','medianImpute','center','scale','pca')) # now it works fine\n\n\n\n\nnearZeroVar(X, names=T, freqCut = 2, uniqueCut = 20) # gives the names of columns\n\n\n\n\n?setdiff\n\n\nPCA\nPCA combines low-variance and correlated variables into single set of\nhigh-variance perpendicular predictors. It also prevents collenearity.\nFirst component has highest variance, and so on.\nBest practice : drop only zero-variance, and use PCA to capture\nlow-variance variables.\n2 trControl\nTo choose the best model with the best fit, train-test split must be\nthe same, even in Cross-validation.\nCan do this by predefining trainControl object that is\nreusable for multiple models.\n\n\n# Create custom indices: myFolds\n# By saving the indexes in the train control, we can fit many models using the same CV folds.\nmyFolds <- createFolds(churn_y, k = 5)\n\n# Create reusable trainControl object: myControl\nmyControl <- trainControl(\n  summaryFunction = twoClassSummary,\n  classProbs = TRUE, # IMPORTANT!\n  verboseIter = TRUE,\n  savePredictions = TRUE,\n  index = myFolds\n)\n\n\nCross Validation\nTo do cross validation, need to modify trainContol()\nnumber : how many slices of train/test split.\nnumber = 5 means slice the data randomly into 5 chunks and\ndo 4:1 train/test split = 5 possible scenarios\nrepeats : how many repeats it will perform. Do another random slice\nand repeat the operation according to above number.\n10 fold CV\n\n\ntenfold_cv = trainControl(\n    method = \"cv\", \n    number = 10,\n    verboseIter = TRUE\n  )\n# Fit lm model using 10-fold CV: model\nmodel = train(\n  price ~ ., \n  diamonds,\n  method = \"lm\",\n  trControl = tenfold_cv\n)\nmodel\n\n\n5*5 fold CV\n\n\nrepeated_cv = trainControl(\n    method = \"repeatedcv\", \n    number = 5,\n    repeats = 5,\n    verboseIter = TRUE\n  )\n\n# Fit lm model using 5 * 5 fold CV\nmodel <- train(\n  price ~ ., \n  diamonds,\n  method = \"lm\",\n  trControl = repeated_cv\n)\n# Print model to console\nmodel\n\n\nsearch\nsearch = ‘grid’ or ‘random’\nSince searching all grid is computationally expensive, Randomly\nchoose hyperparameters within the specified grid. Random search is used\nespecailly when hyperparameter is given in continuous range\nvariable.\n\n\n# Train control with random search\nfitControl <- trainControl(method = \"repeatedcv\",\n                           number = 3,\n                           repeats = 5,\n                           search = \"random\")\n\n# Test 6 random hyperparameter combinations\ntic()\nnn_model_voters_big_grid <- train(turnout16_2016 ~ ., \n                   data = voters_train_data, \n                   method = \"nnet\", \n                   trControl = fitControl,\n                   verbose = FALSE,\n                   tuneLength = 6 # need to be specified in random search\n                   )\ntoc()\n\n\nAdaptive resampling\nGrid search : all hyperparameter combinations are searched Random\nsearch : random subsets of hyperparameter combinations are searched\n=> All of the combinations will be tested\nAdaptive resampling : hyperparameter combinations are resampled with\nvalues near combinations that performed well.\n=> combinations that are sub-optimal will not be tested - faster\nand efficient.\n\n\ntrainControl(\n  method = 'adaptive_cv',\n  search = 'random',\n  adaptive = list(\n    min = 2,\n    alpha = 0.05,\n    method = 'gls',\n    complete = TRUE\n    \n  )\n)\n\n\nmin : minimum number of resamples per\nhyperparameter\nalpha : confidence level for removing hyperparameters.\nDoesn’t change that much\nmethod : ‘gls’ for linear model and ‘BT’ for\nBradley-Terry (for large number of hyperparameters)\ncomplete : TRUE generates full resampling\nset / FALSE saves time and still get the optimal\ncombination\n3 tuneLength\nShould be specified especially with random search.\nAutomatically tries different values of hyperparameters E.g.\ntuneLength = 5 in SVM, it tries degree = 1,2,3,4,5, scale =\n0.001,0.01,0.1,1,10, c = 0, 0.25, 0.5, 1, 2, 4\nWhich argument do you need to set in combination with\ntrainControl(search = “random”) in order to perform random search?\n[Correct! Tune length defines the number of (randomly sampled) tuning\nparameter combinations to compare.]\n4 tuneGrid\nManual hyperparameter tuning : tuneGrid +\nexpand.Grid\n\n\n# Define Cartesian grid\nman_grid <- expand.grid(degree = c(1, 2, 3), \n                        scale = c(0.1, 0.01, 0.001), \n                        C = 0.5)\n\nsvm_model_voters_grid <- train(turnout16_2016 ~ ., \n                   data = voters_train_data, \n                   method = \"svmPoly\", \n                   trControl = fitControl,\n                   verbose= FALSE,\n                   tuneGrid = man_grid)\n\n\nmodel comparison\nHighest average AUC\nLowest std in AUC\nresamples() function\nNow that you have fit two models to the churn dataset, it’s time to\ncompare their out-of-sample predictions and choose which one is the best\nmodel for your dataset.\nYou can compare models in caret using the resamples() function,\nprovided they have the same training data and use the same trainControl\nobject with preset cross-validation folds. resamples() takes as input a\nlist of models and can be used to compare dozens of models at once\n(though in this case you are only comparing two models).\n\n\n# model comparison example\nmodel_list = list(\n  glmnet = model_glmnet,\n  rf = model_rf\n)\n# collect resamples from CV folds\nresamps = resamples(model_list)\nresamps \nsummary(resamps)\n\n\nYou can make this plot using the bwplot() function, which makes a box\nand whisker plot of the model’s out of sample scores. Box and whisker\nplots show the median of each distribution as a line and the\ninterquartile range of each distribution as a box around the median\nline. You can pass the metric = “ROC” argument to the bwplot() function\nto show a plot of the model’s out-of-sample ROC scores and choose the\nmodel with the highest median ROC.\n\n\n# Create bwplot\nbwplot(resamples, metric = \"ROC\")\n# Create xyplot\nxyplot(resamples, metric = \"ROC\")\n\n\nEnsembling models\nThat concludes the course! As a teaser for a future course on making\nensembles of caret models, I’ll show you how to fit a stacked ensemble\nof models using the caretEnsemble package.\n\n\n# Create ensemble model: stack\nstack <- caretStack(model_list, method = \"glm\")\n\n# Look at summary\nsummary(stack)\n\n\nMake prediction\n\n\niris_caret = iris[!(Species %in% 'versicolor')]\n# Get the number of observations\nn_obs <- nrow(iris_caret)\n\n# Shuffle row indices: permuted_rows\npermuted_rows <- sample(n_obs)\n\n# Randomly order data: iris_caret\niris_caret_shuffled <- iris_caret[permuted_rows, ]\n\n# Identify row to split on: split\nsplit <- round(n_obs * 0.6)\n\n# Create train\ntrain <- iris_caret_shuffled[1:split, ]\n\n# Create test\ntest <- iris_caret_shuffled[(split + 1):n_obs, ]\n\n\n# Fit glm model: model\nmodel <- glm(Species ~ ., family = \"binomial\", train)\n\n# Predict on test: p\np <- predict(model, test, type = \"response\")\n\n# If p exceeds threshold of 0.5, M else R: m_or_r\nm_or_r <- ifelse(p > 0.5, \"versicolor\", \"virginica\")\n\n# Convert to factor: p_class\np_class <- factor(m_or_r, levels = levels(test[[\"Species\"]]))\n\n\nConfusion Matrix\n\n\n# Create confusion matrix\nconfusionMatrix(p_class, test[[\"Species\"]])\n\n\nROC curve\nReceiver Operating Characteristic curve.\nAbout classification threshold : manually calculating and comparing\ndozens of confusion matrices is a tedious work.\nROC curve : plot true / false positive rate at\nEVERY POSSIBLE threshold.\nVisualize tradeoffs between two extremes :\n100% true positive vs 0% false positive\nSuppose a binary classification problem : which probability\nshould be used for classification? (50 pos 50 neg)\nIf 0% : then it classifies always positive.\n100% true positive but also 100% false positive.\nIf 100% : always classify negative. 100% false negative but also\n100% true negative (0% false positive)\n\n\nX axis : false positive (left better off)\nY axis : true positive (up better off)\nUse package caTools\n\n\nlibrary(caTools)\n\n# Predict on test: p\np <- predict(model, test, type = \"response\")\n\n# Make ROC curve\ncolAUC(p, test[[\"Class\"]], plotROC = TRUE)\n\n\nAUC\nArea Under the Curve : gives one statistic that\nsummarizes the model’s accuracy without having to specify all the\ndetails of the model!\n0.5 is random guessing\n1 is perfect model\n0 is model always wrong\nMost models lies between 0.5 and 1\nRule of thumb :\n0.9 is A\n0.8 is B\nand so forth\nYou can use the trainControl() function in caret to use AUC (instead\nof acccuracy), to tune the parameters of your models. The\ntwoClassSummary() convenience function allows you to do this easily.\nWhen using twoClassSummary(), be sure to always include the argument\nclassProbs = TRUE or your model will throw an error! (You cannot\ncalculate AUC with just class predictions. You need to have class\nprobabilities as well.)\n\n\n# Create trainControl object: myControl\nmyControl <- trainControl(\n  method = \"cv\",\n  number = 10,\n  summaryFunction = twoClassSummary,\n  classProbs = TRUE, # IMPORTANT!\n  verboseIter = TRUE\n)\n\n# Train glm with custom trainControl: model\nmodel <- train(\n  Class ~ ., \n  Sonar, \n  method = \"glm\",\n  trControl = myControl\n)\n\n# Print model to console\nmodel\n\n\nglmnet Model\nExtension of glm models with built-in variable selection. It is great\nbaseline model for any type of prediction, with several advantages: -\nquick fit - ignores noisy varibles - provides interpretable\ncoefficients\nHelps handle collinerity of regressors / small sample errors\nLasso regression : penalize number of non-zero coefficients\nRidge regression : penalize absolute magnitude of coefficients\nIt attempts to find parsimonious models, and pairs well with\nrandom forest models (it tends to yield different\nresults!)\nglmnet model has two tuning parameters :\nalpha [0, 1] : 0 is pure Ridge, and 1 is pure Lasso\nregression\nlambda (0, inf) : size of penalty, high being simpler\nmodels (penalize for complex)\nClassification problems are a little more complicated than regression\nproblems because you have to provide a custom summaryFunction to the\ntrain() function to use the AUC metric to rank your models. Start by\nmaking a custom trainControl, as you did in the previous chapter. Be\nsure to set classProbs = TRUE, otherwise the twoClassSummary for\nsummaryFunction will break.\n\n\n# Create custom trainControl: myControl\nmyControl <- trainControl(\n  method = \"cv\", \n  number = 10,\n  summaryFunction = twoClassSummary,\n  classProbs = TRUE, # IMPORTANT!\n  verboseIter = TRUE\n)\n# Fit glmnet model: model\nmodel <- train(\n  y ~ ., \n  overfit,\n  method = \"glmnet\",\n  trControl = myControl\n)\n\n# Print model to console\nmodel\n\n# Print maximum ROC statistic\nmax(model[[\"results\"]][[\"ROC\"]])\n\n\nglmnet tuning\n\n\n# make a custom tuning grid\nmyGrid  = expand.grid(\n  alpha=0:1,\n  lambda = seq(0.0001, 0.1, length=10)\n)\n# fit a model\nset.seed(42)\nmodel = train(\n  y ~ .,\n  data = overfit,\n  metric = 'ROC',\n  method = 'glmnet',\n  tuneGrid = myGrid,\n  trControl = myControl\n)\n# plot results\nplot(model)\n\n\ncaret automatically chooses best alpha and lambda\ncoefficient here, so no need to do further.\nrandom forest Model\nInstead of randomForest, ranger is faster,\nstable and use less memory.\nPros - Yield very accurate, non-linear models with no extra work -\nRobust against over-fitting - Requires little preprocessing (no\nlog-transform, normalize..) - Captures threshould effects and variable\ninteractions well\nCons - A bit slower than glmnet - Less interpretable - Needs\nhyperparameters before fitting the model, which might be a bit\narbitrary\n\n\nlibrary(caret)\nlibrary(mlbench)\ndata(Sonar)\nset.seed(42)\n\n\n\n\n# Fit random forest: model\nmodel <- train(\n  quality ~ .,\n  tuneLength = 1,\n  data = wine, \n  method = 'ranger',\n  trControl = trainControl(\n    method = \"cv\", \n    number = 5, \n    verboseIter = TRUE\n  )\n)\n\n# Print model to console\nmodel\n\n\nmtry is the most important hyperparameter : number of\nrandomly selected variables used at each split - lower mtry : more\nrandom - highter mtry : less random\nHigh value : potentially more accurate model, but at the expense of\nwaiting much longer for it to run.\ncaret does this hyperparameter selection\n::tuneLength() argument in train()\nfunction.\n\n\n# fit a model with deeper tuning grid\nmodel = train(\n  Class ~ .,\n  data = Sonar,\n  method = 'ranger',\n  tuneLength = 10 # hyperparameter selection \n)\nplot(model)\n\n\ncustom tuning\nCustom tuning grids to tuneGrid() argument\nPros : - Most flexible - Complete control Cons : - Knowledge in the\nmodel - Can dramatically increase the runtime\n\n\n# From previous step\ntuneGrid <- data.frame(\n  .mtry = c(2, 3, 7),\n  .splitrule = \"variance\",\n  .min.node.size = 5\n)\n\n# Fit random forest: model\nmodel <- train(\n  quality ~.,\n  tuneGrid = tuneGrid,\n  data = wine, \n  method = 'ranger',\n  trControl = trainControl(\n    method = \"cv\", \n    number = 5, \n    verboseIter = TRUE\n  )\n)\n\n# Print model to console\nmodel\n\n# Plot model\nplot(model)\n\n\nwith custom trainControl\n\n\n# Fit random forest: model_rf\nmodel_rf <- train(\n  x = churn_x, \n  y = churn_y,\n  metric = \"ROC\",\n  method = \"ranger\",\n  trControl = myControl\n)\n\n\nUnsupervised ML\nData\n\n\nset.seed(1)\nx = data.frame( a= rnorm(300, mean=0,sd=1), b= rnorm(300, mean=1, sd=1))\nx\n\n\nkmeans\nstats package\nk-means has random component : - randomly assign each points into\ngroups - calculate centers of groups - each point is assigned to cluster\nof nearest center (iteration 1) - calculate centers of groups - each\npoint is assigned to cluster of nearest center (iteration 2) - … - until\nit no longer changes any assignments of each point\nModel selection\nin R, kmeans uses total within cluster sum of squares\nwhich is the squared distance from point to center\n\nBest is minimum one\nnstart does multiple random assignment and give the\nbest(minimum SS) one\n\n\n# Fit a k-means model to x using 3 centers and run the k-means algorithm 20 times. Store the result in km.out.\nkm.out = kmeans(x , centers = 3, nstart =20) # 20 initial random assignemnts and give the best one\n# Inspect the result\nsummary(km.out)\n\n\n\n\nkm.out\n\n\n\n\nkm.out$cluster\n\n\n\n\nplot(x, col = km.out$cluster,\n     main = \"k-means with 3 clusters\", \n     xlab = \"\", ylab = \"\")\n\n\nModel selection\n\n\n# Set up 2 x 3 plotting grid\npar(mfrow = c(2, 3))\n\n# Set seed\nset.seed(1)\n\nfor(i in 1:6) {\n  # Run kmeans() on x with three clusters and one start\n  km.out <- kmeans(x, centers = 3, nstart = 1)\n  \n  # Plot clusters\n  plot(x, col = km.out$cluster, \n       main = km.out$tot.withinss, \n       xlab = \"\", ylab = \"\")\n}\n\n\nHow many clusters to choose?\nChoose one with highest elbow(kink)\n\n\n# Initialize total within sum of squares error: wss\nwss <- 0\n\n# For 1 to 15 cluster centers\nfor (i in 1:15) {\n  km.out <- kmeans(x, centers = i, nstart = 20)\n  # Save total within sum of squares to wss variable\n  wss[i] <- km.out$tot.withinss\n}\n\n# Plot total within sum of squares vs. number of clusters\nplot(1:15, wss, type = \"b\", \n     xlab = \"Number of Clusters\", \n     ylab = \"Within groups sum of squares\")\n\n# Set k equal to the number of clusters corresponding to the elbow location\nk <- 2  # 3 is probably OK, too\n\n\nhierarchical clustering\nNumber of cluster is not known ahead of time\nTwo approaches : bottom-up and top-down - bottom-up approach:\nhclust() function 1) assign all different cluster N 2) join\nclosest one pair to one group, now N-1 clusters - here to\nfind closest one, similarity measure should be used. Simple one is\neuclidean distance. 3) join next closest pair to one group, N-2 -\ncluster to clusters, 4 methods to determine. 1) and 3) makes balanced\none and popular. 1) complete : pairwise similarity between all\nobservations in cluster 1 and 2, pick largest similarity to gauge\ndistance between clusters - method = 'complete' 2) single :\npairwise but pick smallest one to gauge distance -\nmethod = 'single' 3) average : pairwise and pick average of\ndistances - method = 'average' 4) centroid : find center of\neach clusters and check distance between those centers - it can make\ninversion, undesirable and much less used 4) … 5) until all of them are\nclustered into one cluster\nPreprocessing\nDifference in units makes problem, standardizing is needed before\nclustering Use scale(x) for this step.\n\n\n# distance matrix, euclidean distance\n#  dist(x) \nhclust.out = hclust(dist(x))\nsummary(hclust.out)\n# dentrogram\nplot(hclust.out)\nabline(h = 6, col='red') # cut at height 6, two clusters\n\n\nGenerate vector of clustering outcome\n\n\n# Cut by height\ncutree(hclust.out, h = 7)\n# Cut by number of clusters\ncutree(hclust.out, k = 3)\n\n\nCluster linkage methods\n\n\n# Cluster using complete linkage: hclust.complete\nhclust.complete <- hclust(dist(x), method = \"complete\")\n\n# Cluster using average linkage: hclust.average\nhclust.average <- hclust(dist(x), method = \"average\")\n\n# Cluster using single linkage: hclust.single\nhclust.single <- hclust(dist(x), method = \"single\")\n\n# Plot dendrogram of hclust.complete\nplot(hclust.complete, main = \"Complete\")\n\n# Plot dendrogram of hclust.average\nplot(hclust.average, main = \"Average\")\n\n# Plot dendrogram of hclust.single\nplot(hclust.single, main = \"Single\")\n\n\nDimensionality reduction\nDimensionality reduction is needed for: - to find structure in\nfeatures - aid in visualization\nprincipal component analysis - linear combination of features - find\nlower dimensional representation that maintains and describes maximum\namount of variance from original data - maintain most variance in the\ndata - components are orthgonal (uncorrelated) by construction\n\n\nsetDT(iris)\npr.iris = prcomp(x = iris[,-5],\n                 scale = FALSE, # divide by std to set standard deviation to 1\n                 center = TRUE # demean the data, centered around zero\n                 )\nsummary(pr.iris) # variance explained by each component\n# here, PC1 explains 92% of variability of the data\n\n\n\n\npr.iris\n\npr.iris$center\npr.iris$scale\npr.iris$rotation\n# Rotation : the directions of the principal component vectors in terms of the original features/variables. This information allows you to define new data in terms of the original principal components\n\n\nVisualizing\nSee the factor loadings : biplot()\n\n\nbiplot(pr.iris) # Petal.Width and Petal.Length has similar loadings\n\n\nScreeplot\nA scree plot shows the variance explained as the number of principal\ncomponents increases. Sometimes the cumulative variance explained is\nplotted as well.\n\n\npr.var = pr.iris$sdev^2 # make it variance\npve = pr.var / sum(pr.var) # proportion of variance explained\n# Plot variance explained for each principal component\nplot(pve, xlab = \"Principal Component\",\n     ylab = \"Proportion of Variance Explained\",\n     ylim = c(0, 1), type = \"b\")\n\n# Plot cumulative proportion of variance explained\nplot(cumsum(pve), xlab = \"Principal Component\",\n     ylab = \"Cumulative Proportion of Variance Explained\",\n     ylim = c(0, 1), type = \"b\")\n\n\nDeep Learning\nNETWORK ANALYSIS\nMy Functions\nRemove zero weight edges\n\n\ndelete.edges(test_graph, which(E(test_graph)$weight == 0))\n\n\nConcepts\nCentrality measures\n: to see important and influential vertices\ndegree centrality\nout-degree : how many edges does a vertex send out\nin-degree : how mane edges does a vertex receives\ntotal-degree : sum of out and in degree (undirected network’s degree\ncentrality)\ndegree function\n\nstrength\nused in weighted networks (connection strength)\nsummation of weights of edges connected to each node\nstrength function\n\nbetweenness\na measure that quantifies how often a node lies on the shortest path\nbetween other nodes.\nIn other words, searching for hub\nBoth directed / non-directed\nbetweenness()\n\nweighted betweenness\nwhen edges are weighted, then shortest path is the MINUMUM of sum of\nweights.\nUsually weights need to be inversed to calculate correctly\nedge_betweenness(g, weight =dist_weight)\n\ncloseness\na measure that quantifies how close a node is to all other nodes in\nthe network in terms of shortest path distance.\ncloseness()\n\neigenvector centrality (undirected network)\nHow well connected a vertex is\nHighest eigenvector centrality : those that are connected to many\nothers but especially who themselves are highly connected to others\n\ncloseness centrality\nCounting how many steps are required to get to every other node\ncloseness()\n\ntransitivity (clustering coefficient)\nthe extent to which the nodes in the network are connected\n\n\\(\\frac{number\\ of\\ triangles}{number\\ of\\\ntriads}\\) at each node\n-   type global\n\n    -   computed for the whole network\n\n-   type local\n\n    -   computed for each node separately\npagerank centrality\npage.rank()\n\ndyads : two connected vertices(nodes)\ntriads : three connected verticies\ntriangle\n\nOverall structure of network\ndensity\nProportion of edges that actually do exist in the network out of all\npotentially could exist between every pair of vertices\nMeasure of how interconnected a network is\nedge_density() funciton\nNetwork connectance(density)\nNumber of edges in a fully connected network : \\(_nC_2 = \\frac{nodes(nodes-1)}2\\)\nNetwork connectance : \\(p = \\frac{number\\\nof\\ edges\\ connected}{_nC_2}\\)\n\naverage path length\nMean of length of shortest path between all pairs of vertices in the\nnetwork\nthe more connected overall, the shorter the shortest-length\nmean_distance() function\n\nConnectivity\n: Measures how densely connected the vertices of a graph are.\nVertex / Edge connectivity : how man vertices or edges need to be\nremoved to disconnect graph, creating two distinct graphs.\nvertex_connectivity(), edge_connectivity() :\nreturns integer\nMore details about - how many, which are cut, what are the partitions\nafter the cut? min_cut( value.only =FALSE), and\nstMincuts(graph, 'node1','node2')\nconnectivity randomizations\nigraph package\nSetup\n\n\nlibrary(igraph)\n# sample data\nfriends = structure(list(name1 = c(\"Jessie\", \"Jessie\", \"Sidney\", \"Sidney\", \n\"Karl\", \"Sidney\", \"Britt\", \"Shayne\", \"Sidney\", \"Sidney\", \"Jessie\", \n\"Donnie\", \"Sidney\", \"Rene\", \"Shayne\", \"Jessie\", \"Rene\", \"Elisha\", \n\"Eugene\", \"Berry\", \"Odell\", \"Odell\", \"Britt\", \"Elisha\", \"Lacy\", \n\"Britt\", \"Karl\"), name2 = c(\"Sidney\", \"Britt\", \"Britt\", \"Donnie\", \n\"Berry\", \"Rene\", \"Rene\", \"Sidney\", \"Elisha\", \"Whitney\", \"Whitney\", \n\"Odell\", \"Odell\", \"Whitney\", \"Donnie\", \"Lacy\", \"Lacy\", \"Eugene\", \n\"Jude\", \"Odell\", \"Rickie\", \"Karl\", \"Lacy\", \"Jude\", \"Whitney\", \n\"Whitney\", \"Tommy\")), .Names = c(\"name1\", \"name2\"), class = \"data.frame\", row.names = c(NA, \n-27L))\nprint(friends) # unordered network\nfriends.mat = as.matrix(friends)\n\n\nConversion\nMatrix to graph\n\n\n# edgelist accepts matrix, not dataframe\ng <- graph.edgelist(friends.mat, directed = FALSE)\n\n\nDataframe to graph\ngraph_from_data_frame\n\n\nfriends1_edges = structure(list(name1 = c(\"Joe\", \"Joe\", \"Joe\", \"Erin\", \"Kelley\", \n\"Ronald\", \"Ronald\", \"Ronald\", \"Michael\", \"Michael\", \"Michael\", \n\"Valentine\", \"Troy\", \"Troy\", \"Jasmine\", \"Jasmine\", \"Juan\", \"Carey\", \n\"Frankie\", \"Frankie\", \"Micheal\", \"Micheal\", \"Keith\", \"Keith\", \n\"Gregory\"), name2 = c(\"Ronald\", \"Michael\", \"Troy\", \"Kelley\", \n\"Valentine\", \"Troy\", \"Perry\", \"Jasmine\", \"Troy\", \"Jasmine\", \"Juan\", \n\"Perry\", \"Jasmine\", \"Juan\", \"Juan\", \"Carey\", \"Demetrius\", \"Frankie\", \n\"Micheal\", \"Merle\", \"Merle\", \"Alex\", \"Gregory\", \"Marion\", \"Marion\"\n), hours = c(1L, 3L, 2L, 3L, 5L, 1L, 3L, 5L, 2L, 1L, 3L, 5L, \n3L, 2L, 6L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 3L, 2L)), .Names = c(\"name1\", \n\"name2\", \"hours\"), class = \"data.frame\", row.names = c(NA, -25L\n))\n\nfriends1_nodes = structure(list(name = c(\"Joe\", \"Erin\", \"Kelley\", \"Ronald\", \"Michael\", \n\"Valentine\", \"Troy\", \"Jasmine\", \"Juan\", \"Carey\", \"Frankie\", \"Micheal\", \n\"Keith\", \"Gregory\", \"Perry\", \"Demetrius\", \"Merle\", \"Alex\", \"Marion\"\n), gender = c(\"M\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \n\"F\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"F\", \"F\")), .Names = c(\"name\", \n\"gender\"), class = \"data.frame\", row.names = c(NA, -19L))\n\nfriends1_edges\n\n\n\n\n# Create an igraph object with attributes directly from dataframes\ng1 <- graph_from_data_frame(d = friends1_edges, vertices = friends1_nodes, directed = FALSE)\n# d : symbolic edge list in first two columns\n\n\n\n\n# Subset edges greater than or equal to 5 hours\nE(g1)[[hours >= 5]] \n\n# Set vertex color by gender\nV(g1)$color <- ifelse(V(g1)$gender == \"F\", \"orange\", \"dodgerblue\")\n\n\n# Plot the graph\nplot(g1, vertex.label.color = \"black\")\n\nE(g1)[[hours]]\n\nas_adjacency_matrix(g1, attr = 'hours' )\n\n\nBasic features\nVertices and Edges\n\n\n# Subset vertices and edges\nV(g)\nE(g)\n\n# number of nodes ( 2 commands)\nvcount(g) # old\ngorder(g)\n# Count number of edges\necount(g) # old\ngsize(g)\n\n\nadd attributes\n\n\ngenders = c(\"M\", \"F\", \"F\", \"M\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"M\", \"F\", \n\"M\", \"F\", \"M\", \"M\")\nages = c(18, 19, 21, 20, 22, 18, 23, 21, 22, 20, 20, 22, 21, 18, 19, \n20)\nhours = c(1, 2, 2, 1, 2, 5, 5, 1, 1, 3, 2, 1, 1, 5, 1, 2, 4, 1, 3, 1, \n1, 1, 4, 1, 3, 3, 4)\n\n\nVertex attributes\n\n\n# Create new vertex attribute called 'gender'\ng <- set_vertex_attr(g, \"gender\", value = genders)\n\n# Create new vertex attribute called 'age'\ng <- set_vertex_attr(g, \"age\", value = ages)\n\n# View all vertex attributes in a list\nvertex_attr(g)\n\n# View attributes of first five vertices in a dataframe\nV(g)[[1:5]] \n\n\nEdge attributes\n\n\n# Create new edge attribute called 'hours'\ng <- set_edge_attr(g, \"hours\", value = hours)\n\n# View edge attributes of graph object\nedge_attr(g)\n\n\n# Find all edges that include \"Britt\"\nE(g)[[inc('Britt')]]\n\n# Find all pairs that spend 4 or more hours together per week\nE(g)[[hours>=4]]  \n\n\ndirected network\nCheck if the graph object is directed / weighted\n\n\nis.directed(g)\nis.weighted(g)\n\n\n\n\n# Is there an edge going from vertex 184 to vertex 178?\ng['184', '178'] # 1\n\n# Is there an edge going from vertex 18 to vertex 20?\ng['178', '184'] # 0\n\n# Show all edges going to or from vertex 184\nincident(g, '184', mode = c(\"all\"))\n\n# Show all edges going out from vertex 184\nincident(g, '184', mode = c(\"out\"))\n\n\nRelationships between vertices\n\n\n# Identify all neighbors of vertex 12 regardless of direction\nneighbors(g, '12', mode = c('all'))\n\n# Identify other vertices that direct edges towards vertex 12\nneighbors(g, '12', mode = c('in'))\n\n# Identify any vertices that receive an edge from vertex 42 and direct an edge to vertex 124\nn1 <- neighbors(g, '42', mode = c('out'))\nn2 <- neighbors(g, '124', mode = c('in'))\nintersection(n1, n2)\n\n\nDistances between vertices\n\n\n#- Which two vertices are the furthest apart in the graph ?\nfarthest_vertices(g) \n\n#- Shows the path sequence between two furthest apart vertices.(it is called a diameter)\nget_diameter(g)  \n\n# Identify vertices that are reachable within two connections from vertex 42\nego(g, 2, '42', mode = c('out'))\n\n# Identify vertices that can reach vertex 42 within two connections\nego(g, 2, '42', mode = c('in'))\n\n\n\n\n# Calculate the out-degree of each vertex\ng.outd <- degree(g, mode = c(\"out\"))\n\n# View a summary of out-degree\ntable(g.outd)\n\n# Make a histogram of out-degrees\nhist(g.outd, breaks = 30)\n\n# Find the vertex that has the maximum out-degree\nwhich.max(g.outd)\n\n\n\n\n# Calculate betweenness of each vertex\ng.b <- betweenness(g, directed = TRUE)\n\n# Show histogram of vertex betweenness\nhist(g.b, breaks = 80)\n\n# Create plot with vertex size determined by betweenness score\nplot(g, \n     vertex.label = NA,\n     edge.color = 'black',\n     vertex.size = sqrt(g.b)+1,\n     edge.arrow.size = 0.05,\n     layout = layout_nicely(g))\n\n\nOne issue with the measles dataset is that there are three\nindividuals for whom no information is known about who infected them.\nOne of these individuals (vertex 184) appears ultimately responsible for\nspreading the disease to many other individuals even though they did not\ndirectly infect too many individuals. However, because vertex 184 has no\nincoming edge in the network they appear to have low betweenness. One\nway to explore the importance of this vertex is by visualizing the\ngeodesic distances of connections going out from this individual. In\nthis exercise you shall create a plot of these distances from this\npatient zero.\n\n\n# Make an ego graph\ng184 <- make_ego_graph(g, diameter(g), nodes = '184', mode = c(\"all\"))[[1]]\n\n# Get a vector of geodesic distances of all vertices from vertex 184 \ndists <- distances(g184, \"184\")\n\n# Create a color palette of length equal to the maximal geodesic distance plus one.\ncolors <- c(\"black\", \"red\", \"orange\", \"blue\", \"dodgerblue\", \"cyan\")\n\n# Set color attribute to vertices of network g184.\nV(g184)$color <- colors[dists+1]\n\n# Visualize the network based on geodesic distance from vertex 184 (patient zero).\nplot(g184, \n     vertex.label = dists, \n     vertex.label.color = \"white\",\n     vertex.label.cex = .6,\n     edge.color = 'black',\n     vertex.size = 7,\n     edge.arrow.size = .05,\n     main = \"Geodesic Distances from Patient Zero\"\n     )\n\n\neigenvector centrality\n\n\n# Make an undirected network\ng <- graph_from_data_frame(gump, directed = FALSE)\n\n# Identify key nodes using eigenvector centrality\ng.ec <- eigen_centrality(g)\nwhich.max(g.ec$vector)\n\n# Plot Forrest Gump Network\nplot(g,\nvertex.label.color = \"black\", \nvertex.label.cex = 0.6,\nvertex.size = 25*(g.ec$vector),\nedge.color = 'gray88',\nmain = \"Forrest Gump Network\"\n)\n\n\ncensus\nDyads (two connected vertices)\n3 types\nNull : no edges\nAsymettric : one directional edge\nMutual : two directed edge back and forth\nTriads (three connected vertices)\n16 types\n\n\n# Perform dyad census\ndyad_census(amzn_g)\n\n# Perform triad census\ntriad_census(amzn_g)\n\n# Find the edge density\nedge_density(amzn_g)\n\n\nNetwork Structure\n\n\n# Get density of a graph\ngd <- edge_density(g)\n\n#Get the diameter of the graph g\ndiameter(g, directed = FALSE)\n\n# average path length\n\n#Get the average path length of the graph g\ng.apl <- mean_distance(g, directed = FALSE)\ng.apl\n\n\nGenerating random network with given parameters\nRandom graph & randomization tests - To determine if the network\nis particularly different from those random networks\nGenerate 1000 graphs based on original network, with same number of\nvertices and similar density\nCalculate average path length of original network\nCalculate average path length of 1000 random networks\ndetermine how many random networks have length greater or less than\noriginal average length path\nGenerate 1000 random graphs\n\n\ngl = vector('list', 1000)\n\nfor (i in 1:1000){\n  gl[[i]] = erdos.renyi.game(\n    n = gorder(g), # same number of vertices\n    p.or.m = edge_density(g), # similar density\n    type = 'gnp'\n  )\n}\n\n\n# calculate average mean distance of 1000 random graphs\n\ngl.apls = unlist(\n  lapply(glm ,mean_distance, directed = F)\n)\n\n# histogram of random mean distances\nhist(gl.apls, breaks =20)\n\n\nGenerate a random graph using the function erdos.renyi.game(). The\nfirst argument n should be the number of nodes of the graph g which can\nbe calculated using gorder(), the second argument p.or.m should be the\ndensity of the graph g which you previously stored as the object gd. The\nfinal argument is set as type=‘gnp’ to tell the function that you are\nusing the density of the graph to generate a random graph.\n\n\n# Create one random graph with the same number of nodes and edges as g\ng.random <- erdos.renyi.game(n = gorder(g), p.or.m = gd, type = \"gnp\")\n\ng.random\n\nplot(g.random)\n\n# Get density of new random graph `g.random`\nedge_density(g.random)\n\n#Get the average path length of the random graph g.random\nmean_distance(g.random, directed = FALSE)\n\n\n\n\n# Generate 1000 random graphs\ngl <- vector('list',1000)\n  \nfor(i in 1:1000){\n  gl[[i]] <- erdos.renyi.game(n = gorder(g), p.or.m = gd, type = \"gnp\")\n}\n\n# Calculate average path length of 1000 random graphs\ngl.apls <- unlist(lapply(gl, mean_distance, directed = FALSE))\n\n# Plot the distribution of average path lengths\nhist(gl.apls, xlim = range(c(1.5, 6)))\nabline(v = g.apl, col = \"red\", lty = 3, lwd = 2)\n\n# Calculate the proportion of graphs with an average path length lower than our observed\nmean(gl.apls < g.apl)\n\n\nnetwork substructure\nclosed triangles\nFor every three vertices there exists three potential edges. If all\nedges exist, then the triad is said to be closed.\nTo identify all closed triangles : triangles()\nfunction.\nGlobal Transitivity : probability that the adjacent verticies of a\ngiven vertex are connected. transitivity() function.\nLocal Transitivity : the proportion of closed triangles that a vertex\nis a part of, out of theoretical number of closed triangles it could be\na part of given its connections.\ncount_triangles() function yields the number of closed\ntriangle of given vertex.\nclique\nIn a clique, every vertex is connected to every other vertex.\nlargest_cliques()\nmax_cliques() : identify cliques of any size, 2+\n\n\n# Show all triangles in the network.\nmatrix(triangles(g), nrow = 3)\n\n# Count the number of triangles that vertex \"BUBBA\" is in.\ncount_triangles(g, vids = 'BUBBA')\n\n# Calculate  the global transitivity of the network.\ng.tr <- transitivity(g)\ng.tr\n\n# Calculate the local transitivity for vertex BUBBA.\ntransitivity(g, vids = 'BUBBA', type = \"local\")\n\n\n\n\n# Calculate average transitivity of 1000 random graphs\ngl.tr <- lapply(gl, transitivity)\ngl.trs <- unlist(gl.tr)\n\n# Get summary statistics of transitivity scores\nsummary(gl.trs)\n\n# Calculate the proportion of graphs with a transitivity score higher than Forrest Gump's network\nmean(gl.trs > g.tr) # 0 - randomized networks gave lower transitivity score than Gump's network\n\n\n\n\n# Identify the largest cliques in the network\nlargest_cliques(g)\n\n# Determine all maximal cliques in the network and assign to object 'clq'\nclq <- max_cliques(g)\n\n# Calculate the size of each maximal clique.\ntable(unlist(lapply(clq, length)))\n\n\nVisualize largest cliques\n\n\n# Assign largest cliques output to object 'lc'\nlc <- largest_cliques(g)\n\n# Create two new undirected subgraphs, each containing only the vertices of each largest clique.\ngs1 <- as.undirected(subgraph(g, lc[[1]]))\ngs2 <- as.undirected(subgraph(g, lc[[2]]))\n\n\n# Plot the two largest cliques side-by-side\n\npar(mfrow=c(1,2)) # To plot two plots side-by-side\n\nplot(gs1,\n     vertex.label.color = \"black\", \n     vertex.label.cex = 0.9,\n     vertex.size = 0,\n     edge.color = 'gray28',\n     main = \"Largest Clique 1\",\n     layout = layout.circle(gs1)\n)\n\nplot(gs2,\n     vertex.label.color = \"black\", \n     vertex.label.cex = 0.9,\n     vertex.size = 0,\n     edge.color = 'gray28',\n     main = \"Largest Clique 2\",\n     layout = layout.circle(gs2)\n)\n\n\nRelationship measures\nAssortativity\nThe preferential attachement of vertices to other verticies that are\nsimilar in numerical or categorical attributes\nE.g. political party, gender, age\nNetwork with high assortativity? how high is high? => by\nrandomization test\nReciprocity (directed network)\nequal to proportion of edges that are symmetrical. (A -> B) and\n(B -> A)\nProportion of outgoing edges that also have an incoming edge.\n\n\n# Convert the gender attribute into a numeric value\nvalues <- as.numeric(factor(V(g1)$gender))\n\n# Calculate the assortativity of the network based on gender\nassortativity(g1, values)\n\n# Calculate the assortativity degree of the network\nassortativity.degree(g1, directed = FALSE)\n\n\nUsing randomizations to assess assortativity\n\n\n# Calculate the observed assortativity\nobserved.assortativity <- assortativity(g1, values)\n\n# Calculate the assortativity of the network randomizing the gender attribute 1000 times\nresults <- vector('list', 1000)\nfor(i in 1:1000){\n  results[[i]] <- assortativity(g1, sample(values))\n}\n\n# Plot the distribution of assortativity values and add a red vertical line at the original observed value\nhist(unlist(results))\nabline(v = observed.assortativity, col = \"red\", lty = 3, lwd=2)\n\n\nReciprocity\n\n\n# Make a plot of the chimp grooming network\nplot(g,\n     edge.color = \"black\",\n     edge.arrow.size = 0.3,\n     edge.arrow.width = 0.5)\n\n\n# Calculate the reciprocity of the graph\nreciprocity(g)\n\n\nDyacity(Homophily measure)\nConnectedness between nodes with the same label\ncompared to what is expected in a random configuration of the\nnetwork\nExpected number of same label edges : All possible number of edges\nwithin the same label * connectance(density) of network \\(p\\)\n$n_{green}C_2 * p = $\ne.g. 9 white nodes 6 green nodes, 21 edges and p =0.2 - \\(_6C_2 * 0.2 = 3\\) - \\(D = \\frac{number\\ of\\ same\\ label\\\nedges}{expected\\ number\\ of\\ same\\ label\\ edges}\\)\nDyadic : D >1 D ~ 1 random , D < 1 anti-dyadic\nHeterophilicity\nConnectedness between nodes with different labels compared to what\nis expected for a random configuration of the network\nExpected number of cross label edges : \\(n_wn_gp\\)\ne.g. 9 white nodes 6 green nodes 21 edges and connectance \\(p=0.2\\)\nexpected number of cross label edges is 11 (\\(9*6*0.2\\))\n\\(H = \\frac{number\\ of\\ cross\\ label\\\nedges}{expected\\ number\\ of\\ cross\\ label\\ edges}\\)\nH > 1 : heterophilic / H = 1 : random / H < 1 :\nHeterophobic\nNetwork connectance\ncommunity detection\nSeveral algorithms are out there, and they have pros and cons, so all\nshould be tried out to determine the best fit.\nfastgreedy detection\nmodularity score based\nworks by tyring to build larger and larger commnunities by adding\nvertices to each community one by one and assessing modularity\nscore\nmodularity score : an index of how inter-connected edges are within\nvs between communities.\ncluster_fastgreedy\n\nedge-betweenness\nworks by dividing the network into smaller and smaller pieces until\nit finds edges that it perceives to be bridges between communities.\ncluster_edge_betweenness\n\nleading eigenvector\ncluster_leading_eigen\n\nlabel propergation\ncluster_label_prop\n\ninfomap\nlouvain (multilevel)\n\n\n# Perform fast-greedy community detection on network graph\nkc = fastgreedy.community(g)\n\n# Determine sizes of each community\nsizes(kc)\n\n# Determine which individuals belong to which community\nmembership(kc)\n\n# Plot the community structure of the network\nplot(kc, g)\n\n\n\n\n# Perform edge-betweenness community detection on network graph\ngc = edge.betweenness.community(g)\n\n# Determine sizes of each community\nsizes(gc)\n\n# Plot community networks determined by fast-greedy and edge-betweenness methods side-by-side\npar(mfrow = c(1, 2)) \nplot(kc, g)\nplot(gc, g)\n\n\ncommunity comparison\nHow to compare the communities found by each detection methods?\ncompare(method ='vi') to find similarity. -One possible\nuse : vi variation information metric. - How much variation\nis there in community membership for each vertex - The closer to zero,\nthe more likely any two vertices to be found in the same community\n\n\n# From previous step\nretweet_graph_undir <- as.undirected(retweet_graph)\ncommunities_fast_greedy <- cluster_fast_greedy(retweet_graph_undir)\ncommunities_infomap <- cluster_infomap(retweet_graph_undir)\ncommunities_louvain <- cluster_louvain(retweet_graph_undir)\n\ntwo_users <- c(\"bass_analytics\", \"big_data_flow\")\n\n# Subset membership of communities_fast_greedy by two_users\nmembership(communities_fast_greedy)[two_users]\n\n\n\n\ndata()\n\n\nCollective Inferencing\nCollective inferencing is a procedure to simultaneously label nodes\nin interconnected data to reduce classification error.\nIn this exercise you will perform collective inferencing and see the\neffect it has on the churn prediction using the AUC performance measure.\nAUC, or area under the ROC curve, is commonly used to assess the\nperformance of classification techniques.\nAUC = probability that a randomly chosen churner is ranked higher by\nthe model than a randomly chosen non-churner AUC = number between 0.5\nand 1, where a higher number means a better model\nNetwork features\ndegree() function\nnumber of nodes that are connected to each node (first order)\nneighborhood.size(g, order=2)\n2nd order neighberhood (connected within 2, including itself)\ncount_triangles()\nHow many triangles does a node form?\nCentrality measures - Betweenness - Closeness\nNetwork Visualization\npackage\n\n\nlibrary(ggnetwork)\nlibrary(ggraph)\n\n\nigraph base\n\n\n# See a list of possible layouts\nls(\"package:igraph\", pattern = \"^layout_.\")\n\n\n\n\n# Plot the graph object g1 in a circle layout\nplot(g1, vertex.label.color = \"black\", layout = layout_in_circle(g1))\n\n\n\n\n# Plot the graph object g in a Fruchterman-Reingold layout \nplot(g1, vertex.label.color = \"black\", layout = layout_with_fr(g1))\n\n\n\n\n# Plot the graph object g in a Tree layout \nplot(g1, vertex.label.color = \"black\", layout = layout_as_tree(g1))\n\n\n\n\n# Plot the graph object g using igraph's chosen layout \nm1 <- layout_nicely(g1)\nplot(g1, vertex.label.color = \"black\", layout = m1)\n\n\n\n\n# Create a vector of weights based on the number of hours each pair spend together\nw1 <- E(g1)$hours\n\n# Plot the network varying edges by weights\nm1 <- layout_nicely(g1)\nplot(g1, \n        vertex.label.color = \"black\", \n        edge.color = 'black',\n        edge.width = w1,\n        layout = m1)\n\n\nNetwork Filtering\n\n\n# Create a new igraph object by deleting edges that are less than 2 hours long \ng2 <- delete_edges(g1, E(g1)[hours < 2])\n\n\n# Plot the new graph \nw2 <- E(g2)$hours\nm2 <- layout_nicely(g2)\n\nplot(g2, \n     vertex.label.color = \"black\", \n     edge.color = 'black',\n     edge.width = w2,\n     layout = m2)\n\n\n\n\n# Add a node attribute called churn\nV(network)$churn <- customers$churn\n\n# Add a node attribute called color\nV(network)$color <- V(network)$churn\n\n# Change the color of churners to red and non-churners to white\nV(network)$color <- gsub(\"1\",\"red\",V(network)$color)\nV(network)$color <- gsub(\"0\",\"white\",V(network)$color)\n\n# Plot the network\nplot(network, vertex.label = NA, edge.label = NA,\n     edge.color = \"black\", vertex.size = 2)\n\n\nSubgraph with conditions\n\n\n# Create a subgraph with only churners\nchurnerNetwork <- induced_subgraph(network, \n                    v = V(network)[which(V(network)$churn == 1)])\n                    \n# Plot the churner network                     \nplot(churnerNetwork, vertex.label = NA, vertex.size = 2)\n\n\nggraph\nRegular graph(Karmada-Kawai layout)\n\n\nggraph(g, layout = \"with_kk\") +\n  geom_edge_link(aes(alpha = weight)) +\n  geom_node_point()  +\n  # Add a node text geometry, mapping label to id and repelling\n  geom_node_text(aes(label = id), repel = TRUE)\n\n\ncircular graph\n\n\n# Visualize the network in a circular layout\nggraph(g, layout = \"in_circle\") + \n  # Map tie transparency to its weight\n  geom_edge_link(aes(alpha = weight)) + \n  geom_node_point()\n\n\nGrid graph\n\n\n# Change the layout so points are on a grid\nggraph(g, layout = \"on_grid\") + \n  geom_edge_link(aes(alpha = weight)) + \n  geom_node_point()\n\n\nVisualize betweennesss\n\n\n# Plot with the Kamada-Kawai layout \nggraph(g, layout = \"with_kk\") + \n  # Add an edge link geom, mapping alpha to weight\n  geom_edge_link(aes(alpha = weight)) + \n  # Add a node point geom, mapping size to degree\n  geom_node_point(aes(size = degree))\n\n\nHighlight edges\n\n\n# Make is_weak TRUE whenever the tie is weak\nis_weak <- E(g)$weight == 1\n\nggraph(g, layout = \"with_kk\") +\n  # Add an edge link geom, mapping color to is_weak\n  geom_edge_link(aes(color = is_weak))\n\n\nSubgraph\n\n\nggraph(g, layout = \"with_kk\") + \n  # Map filter to is_weak\n  geom_edge_link(aes(filter = is_weak), alpha = 0.5) \n\n\nggnetwork\n\n\n# Call ggplot\n\nggplot(\n  # Convert retweet_samp to a ggnetwork\n  ggnetwork(retweet_samp), \n  # Specify x, y, xend, yend\n  aes(x = x, y = y, xend = xend, yend = yend)) +\n  # Add a node layer\n  geom_nodes() +\n  # Change the edges to arrows of length 6 pts\n  geom_edges(arrow = arrow(length = unit(6, \"pt\"))) +\n  # Use the blank theme\n  theme_blank()\n\n\nvisNetwork\nInteractive visualization package\n\n\n# convert igraph to visNetwork\ndata <- toVisNetworkData(g)\n\n# Visualize the network\nvisNetwork(\n  nodes = data$nodes, \n  edges = data$edges, \n  width = 300, \n  height = 300\n)\n\n\nIt is possible to change the layout of the visualization using the\nvisNetwork() and visIgraphLayout() function calls. The igraph package\ncontains several functions that provide algorithms to lay out the nodes.\nYou can pass the function name as a string to the layout argument of\nvisIgraphLayout() to use it.\n\n\n# Update the plot\nvisNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%\n  # Change the layout to be in a circle\n  visIgraphLayout(layout = \"layout_with_kk\")\n# on grid format\nvisNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%\n  # Change the layout to be on a grid\n  visIgraphLayout(layout = \"layout_on_grid\")\n\n\nHere, we will highlight the nearest nodes and ties when a node is\nselected.\n\n\n# Add to the plot\nvisNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%\n  # Choose an operator\n  visIgraphLayout(layout = \"layout_with_kk\") %>%\n  # Change the options to highlight the nearest nodes and ties\n  visOptions(highlightNearest = TRUE)\n\n\nNodes by ID\n\n\n# Update the plot\nvisNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%\n  visIgraphLayout(layout = \"layout_with_kk\") %>%\n  # Change the options to allow selection of nodes by ID\n  visOptions(nodesIdSelection = TRUE)\n\n\nNodes by group (cluster)\n\n\n# Update the plot\nvisNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%\n  visIgraphLayout(layout = \"layout_with_kk\") %>%\n  # Change options to select by group\n  visOptions(selectedBy = \"group\")\n\n\nClassifier\nrelational neighber\nclassifier\n\n\n# You are given two vectors: ChurnNeighbors and NonChurnNeighbors with each customer's number of neighbors that have churned and not churned, respectively.\n# Compute the churn probabilities\nchurnProb <- ChurnNeighbors / (ChurnNeighbors + NonChurnNeighbors)\n\n# Find who is most likely to churn\nmostLikelyChurners <- which(churnProb == max(churnProb))\n\n# Extract the IDs of the most likely churners\ncustomers$id[mostLikelyChurners]\n\n\nprobabilistic relational neighbor classifier\nAdjacency Matrix\nas_adjacency_matrix(g)\nWith weighted network,\nas_adjacency_matrix(g, attr = 'weight')\nHierarchical clustering\nThe basic idea behind hierarchical clustering is to define a measure\nof similarity between groups of nodes and then incrementally merge\ntogether the most similar groups of nodes until all nodes belongs to a\nunique cluster. The result of this process is called a dendrogram.\nSimilarity measure\npearson similarity is used\nsingle linkage : the similarity between groups is the max of\nsimilarities between nodes of different groups\ncomplete linkage : the similarity between groups is the min of\nsimilarities between nodes of different groups\naverage linkage : the similarity between groups is the average of\nsimilarities between nodes of different groups\nHierarchical clustering algorithm\nevaluate the similarity measures for all node pairs\nassign each node to a group of its own\nfind a pair of groups with the highest similarity and join them\ntogether into single group\ncalculate similarity between new composite groups and all\nothers\nrepeat 3),4) until all nodes are grouped into one.\nin R, hclust package\n\n\n# compute a distance matrix\nD <- 1-S\n\n# obtain a distance object \nd <- as.dist(D)\n\n# run average-linkage clustering method and plot the dendrogram \ncc <- hclust(d, method = \"average\")\nplot(cc)\n\n\n\n\n# Cut the dendrogram tree into 4 clusters\ncls <- cutree(cc, k = 4) # factor vector that classifies each into 1,2,3,4\n\n# Add cluster information to nodes\nnodes_with_clusters <- nodes %>%\n  mutate(cluster = cls)\n\n# See the result\nnodes_with_clusters\n\n\nVisualize cluster\n\n\n# From previous step\nV(g)$cluster <- nodes$cluster\n\n# Update the plot\nggraph(g, layout = \"with_kk\") + \n  geom_edge_link(aes(alpha = weight), show.legend=FALSE) +  \n  geom_node_point(aes(color = factor(cluster))) + \n  labs(color = \"cluster\")  +\n  # Facet the nodes by cluster, with a free scale\n  facet_nodes(~ cluster, scales=\"free\")\n\n\nPredictive models\nfrom graph object -> data.frame -> run ML\nLogistic regression : white-box, comprehensible Random forest :\nblack-box, powerful\n\n\n# Set the seed\nset.seed(7)\n\n# Create the index vector\nindex_train <- sample(1:nrow(studentnetworkdata), 2 / 3 * nrow(studentnetworkdata))\n\n# Make the training set\ntraining_set <- studentnetworkdata[index_train, ]\n\n# Make the test set\ntest_set <- studentnetworkdata[-index_train, ]\n\n\nLogit model\n\n\n# Make firstModel\nfirstModel <- glm(Future ~ degree + degree2 + triangles + betweenness + closeness + transitivity, family = 'binomial', data = training_set)\n# Build the model\nsecondModel <- glm(Future ~ ChurnNeighbors + RelationalNeighbor + ChurnNeighborsSecond + RelationalNeighborSecond + averageDegree + averageTriangles + averageTransitivity + averageBetweenness, \n                   family = 'binomial', data = training_set)\n# Build the model\nthirdModel <- glm(Future~., family = 'binomial', data=training_set)\n\n\nRandom forest model\n\n\n# Load package\nlibrary(randomForest) # slower than ranger package\n\n# Set seed\nset.seed(863)\n\n# Build model\nrfModel <- randomForest(as.factor(Future)~. ,data=training_set)\n\n# Plot variable importance\nvarImpPlot(rfModel)\n\n\nExamples\nGenerate a column with degree / strength from graph object\n\n\nnodes_with_centrality <- nodes %>%\n  mutate(\n    degree = degree(g),\n    # Add a column containing the strength of each node\n    strength = strength(g)\n  ) %>%\n  # Arrange rows by descending strength\n  arrange(desc(strength))\n\n# See the result\nnodes_with_centrality\n\n\nDATABASE & SERVER COMPUTING\n(HPC)\nSSH\nHypergator conn\nssh gunsu.son@hpg.rc.ufl.edu\nWRDS conn\nssh mgson633@wrds-cloud.wharton.upenn.edu\nFilezilla\nSite Manager\nGo to\nHypergator\n- host : hpg.rc.ufl.edu\n- port : 22\nWRDS\n- host : wrds-cloud.wharton.upenn.edu\n- port : 22\nSCP\nUpload and download files from server\n\n# Downloading\nscp mgson633@wrds-cloud.wharton.upenn.edu:/home/ufl/mgson633/SAS/ibes/suecars.sas7bdat /Users/matthewson/Desktop # download\n# scp user@server:/path/to/remotefile.zip /Local/Target/Destination\n\n\nrstudioapi::terminalExecute('scp ~/Dropbox\\ \\(UFL\\)/Nimal_Matthew/Projects/Options_ML/outputs/alldays_c.fst gunsu.son@hpg.rc.ufl.edu:/blue/nimalendran/matthewson/projects/optm/data')\n\nSparklyr\nInstallation\nInstall local spark\n\n\nlibrary(sparklyr)\nspark_install() #2.4.3 and 2.7 as default\n\n\nConfig\nFor SAS reading package : use saurfang’s Spark package for SAS\nreading\n\n\nconfig <- spark_config()\nconfig[\"sparklyr.shell.packages\"] = \"saurfang:spark-sas7bdat:2.0.0-s_2.11\"\nconfig[\"sparklyr.shell.repositories\"] = \"https://repos.spark-packages.org/\"\n\nsc = spark_connect(master = \"local\", config = config)\n\n\nWRDS\nSetup for WRDS\nFor more details, check wrds websitehttps://wrds-www.wharton.upenn.edu/pages/support/programming-wrds/programming-r/r-from-your-computer/\nInstall package : Rpostgres\n\n\ninstall.packages(\"RPostgres\")\n# install.packages(\"RPostgres\", repos = \"https://r-dbi.r-universe.dev\")\n\n\n.pgpass file\nFor local and WRDS cloud use option(alt) +\ncmd + enter to run in terminal\nnano ~/.pgpass\nSave below inside .pgpass file\nwrds-pgdata.wharton.upenn.edu:9737:wrds:wrds_username:wrds_password\nRun below to secure file\nchmod 600 ~/.pgpass\nFor Windows\n%APPDATA%\\postgresql\\pgpass.conf\nOn CMD:\necho wrds-pgdata.wharton.upenn.edu:9737:wrds:wrds_username:wrds_password > %APPDATA%\\postgresql\\pgpass.conf\nDBPLYR ON WRDS\n\n\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(RPostgres)\ncon <- dbConnect(Postgres(),\n                 host='wrds-pgdata.wharton.upenn.edu',\n                 port=9737,\n                 dbname='wrds',\n                 sslmode='require',\n                 user='mgson633'\n                 )\n\n\nWRDS table names\nhttps://wrds-www.wharton.upenn.edu/users/products/\nThe solution is to either by explicit with:\ncrsp_a_stock.dsf crsp_m_stock.dsf crsp_q_stock.dsf\nDsf = tbl(con, in_schema(‘tr_ibes’,‘act_epsus’))\nOR\ndbExecute(con, “SET search_path=‘crsp_a_stock’”) Dsf = tbl(con,\n‘dsf’)\nCRSP\nS&P500 Identifiers\nJoin dsf and dsenames (for ticker symbol) then query\n\n\ndsf %>% \n  select(cusip, permno, date, ret, retx) %>% \n  filter(year(date) >= 2018,) %>% \n  right_join(dsenames %>% select(permno,namedt,nameendt,permno,ticker), \n                   sql_on = '\"LHS\".date >= \"RHS\".namedt AND \"LHS\".date <= \"RHS\".nameendt AND \"LHS\".permno = \"RHS\".permno') %>% \n  filter(ticker %in% local(special_tickers$Ticker))\n\n\nS&P500 Constituent\n\n\n# crsp.dsp500list / crsp.msp500list\ndsp500list = tbl(con, in_schema('crsp_a_indexes', 'dsp500list')) # daily stock file S&P500 list\nsnp500 = dsp500list %>% filter(start <= '2019-01-01',  ending >= '2021-03-31') %>% collect %>% setDT \n# samples to be included during the sample period\n\nsnp500[,uniqueN(permno)]\n\n\nPermno to TICKER : use Company identifiers\n\n\n# crsp.dsenames / crsp.msenames\ndsenames = tbl(con, in_schema('crsp_a_stock', 'dsenames')) # daily names\ndsenames = dsenames %>% collect %>% setDT\n\n\nMerge\n\n\n# names that are valid at 2019-01-01, and registered as snp throughout the sample period (2019 ~ 2021 Mar)\nsnp500_tickers = dsenames[,.(permno, namedt, nameendt, ncusip, ticker, tsymbol)] %>%\n  .[nameendt >= '2019-01-01'] %>%\n  .[snp500, on = .(permno)] %>%\n  .[start <= '2019-01-01' & ending >= '2021-03-31'] %>% \n  .[,unique(ticker)]\n\n\nDSF,MSF\nDaily / monthly stock files\n\n\ndsf = tbl(con, in_schema('crsp_a_stock', 'dsf'))\ndsfhdr = tbl(con,'dsfhdr')\n\n\nMerge identifier (permno to cusip and ticker)\n\n\ndsenames = tbl(con, in_schema('crsp_a_stock', 'dsenames')) %>% collect %>% setDT\n\n\ndsf2[,date_copy := date]\ndsf2 = dsenames[,.(permno, ticker, namedt,nameendt)][dsf2, on = .(permno = permno, namedt <= date_copy, nameendt >= date_copy)][,.(permno,ticker,date, ret,retx)]\nsetnames(dsf2, c('PERMNO','TICKER','date','RET','RETX'))\n\n\n\n\nquery = dbSendQuery(con,\n            \"\n            select a.*, b.date, b.ret\n            from crsp.msp500list as a,\n            crsp.msf as b\n            where a.permno=b.permno\n            and b.date >= a.start and b.date<= a.ending\n            and b.date>='01/01/2000'\n            order by date;\n            \")\nsp500 = dbFetch(query)\nsp500\n\n\nTrace\n\n\n# connect to trace data\ntrace = tbl(con, 'trace')\n# lazy query\nquery = trace %>%\n  filter(year(trd_exctn_dt)<=2005) %>%\n  group_by(trd_exctn_dt) %>% \n  filter(row_number() ==n()) # last observations for each group\n# show example\nquery\n# show query\nquery %>% show_query\n# submit and retrieve\ndata = query %>% collect\n\n\nCOMPUSTAT\nDaily short interest file\n\n\ncomp = tbl(con, 'sec_shortint')\nquery = comp %>% head()\nquery\n\n\nFundamentals Annual (funda)\n\n\nfunda = tbl(con, 'funda')\ntemp = funda %>% \n  # select(gvkey, tic, cusip, fyr, dvt) %>%\n  head(10000) %>% collect\ntemp\n\n\nTAQ\nGeneral information for TAQ on WRDS https://wrds-www.wharton.upenn.edu/pages/support/manuals-and-overviews/taq/general/wrds-overview-taq/\nDue to the very large size of the data, each year is located in a\ndifferent physical directory: /wrds/taq.YYYY for Monthly and\n/wrds/nyse/taq_msecYYYY for Daily\nNBBOM\nMillisecond national best bid and offer (NBBO) file\n\n\nlibrary(tictoc)\n\n\n\n\nnbbom = tbl(con, 'nbbom_20200102')\nquery = nbbom %>% \n  select(date, sym_root, sym_suffix, time_m, best_bid,best_bidsiz, best_ask, best_asksiz ) %>%\n  filter(is.na(sym_suffix), hour(time_m) >= 9, hour(time_m)<=15) %>% \n  mutate(temp= floor(minute(time_m)/10)) %>%\n  group_by(sym_root, hour(time_m), temp) %>%\n  filter(row_number()==n())\nquery %>% head()\ntoc()\n# 451.146 sec elapsed # including download & memory loading\n# 371.793 sec elapsed\n\n\nWCT\nWRDS Consolidated Trades (WCT)\nThe NBBO’s Bid and Ask midpoints are matched to each trade at seconds\n0, -1, -2 and -5 relative to their trade time, and stored along with\ntrades in the same datasets.\nThese WRDS-generated trades files are labeled as\nwct_YYYYMMDD. This approach provides users with all\nnecessary components to infer the trade directions, regardless of what\nspecifications and assumptions are employed on the trade-quote lag or on\ntrade filters. Please see the Lee and Ready research application for\nmore information. Additionally the following article, “Matching TAQ\nTrades and Quotes in the Presence of Multiple Quotes”, may be useful for\nunderstanding more about WCT files.\nMillisecond stock trading volume and buy/sell indicator using Lee\nReady algorithm\nSAS code is here: https://wrds-www.wharton.upenn.edu/pages/support/applications/microstructure-research/lee-and-ready-1991-algorithm/\n\n\nwct = tbl(con, 'wct_20200102')\nquery = nbbom %>% \n  select(date, sym_root, sym_suffix, time_m, best_bid,best_bidsiz, best_ask, best_asksiz ) %>%\n  filter(is.na(sym_suffix), hour(time_m) >= 9, hour(time_m)<=15) %>% \n  mutate(temp= floor(minute(time_m)/10)) %>%\n  group_by(sym_root, hour(time_m), temp) %>%\n  filter(row_number()==n())\nquery %>% head()\ntoc()\n\n\nOptionMetrics\noptionmnames :\nIncludes 1) secid, symbol, optionid, root, suffix, effect_date,\ncusip, ticker, class, issuer, issue\n\n\nopnames = tbl(con,in_schema('optionm_all', 'optionmnames'))\nsample2 = opnames %>% head(100) %>% collect\nsample2\n\n\n\n\nopnames %>% filter(optionid == 130940651) %>% collect\nprint(opnames)\n\n\nTo match, should use ‘secid’ and ‘effect_date’\nopprcd : Option Prices data\nincludes 1) option price 2) delta gamma open_interest implied_vol\nvega theta\n\n\nopprcd = tbl(con, in_schema('optionm_all','opprcd2020')) # It includes year\nsample = opprcd %>% head(100) %>% collect\nsample\n\n\nsecprd2020 : Underlying Security price data by year\nincludes secid symbol optionid root suffix effect_date cusip ticker\nclass issuer issue\n\n\nsecprd2020 = tbl(con, in_schema('optionm_all','secprd2020'))\nprint(secprd2020)\n\n\nRPostgres\nConnection\n\n\nlibrary(RPostgres)\nwrds <- dbConnect(Postgres(),\n                  host='wrds-pgdata.wharton.upenn.edu',\n                  port=9737,\n                  dbname='wrds',\n                  sslmode='require',\n                  user='mgson633')\n\n\nQuery\nQuery is done through combinations of query + fetch + close\ndbSendQuery() : Send query to existing connection,\nwrds\ndbFetch() : Fetch data from resulting response\nn : number of rows fetching\ndbClearResult(res) : closes connection (response)\nAll libraries\nAll data libraries available at WRDS\n\n\nres = dbSendQuery(wrds, \"select distinct table_schema\n                   from information_schema.tables\n                   \n                   order by table_schema\")\n\ntemp = dbFetch(res)\ndbClearResult(res)\ntemp\n\n\nAll datasets within library\n\n\nres = dbSendQuery(wrds, \n                   \"\n                   select distinct table_name\n                   from information_schema.columns\n                   where table_schema='taqm_2021'\n                   order by table_name\n                   \"\n                   )\ntemp = dbFetch(res, n=-1)\ndbClearResult(res)\ntemp %>% View\n\n\nAll columns within dataset\n\n\nres = dbSendQuery(wrds, \n                   \"\n                   select column_name\n                   from information_schema.columns\n                   where table_schema='optionm'\n                   and table_name='opvold'\n                   order by column_name\n                   \")\ntemp = dbFetch(res, n=-1)\ntemp\ndbClearResult(res)\n\n\nCheck the table\n\n\nres = dbSendQuery(wrds, \n                   \"\n                   select *\n                   from taqm_2011.ctm_20110401\n                   limit 1000\n                   \")\ntemp = dbFetch(res, n=-1)\ndbClearResult(res)\ntemp %>% View\n\n\nres = dbSendQuery(wrds, \n                   \"\n                   select *\n                   from taqmsec.ctm_20201201\n                   limit 1000\n                   \")\ntemp = dbFetch(res, n=-1)\ndbClearResult(res)\ntemp %>% View\n\n\nPostgreSQL\nWildcards\n% matchs any numbers of characters _ matchs\nonly ne character\nIN, BETWEEN, LIKE\n\n  SELECT * \n  FROM baby_names\n  WHERE \n  state IN('CA', 'NY', 'TX') and\n  state NOT IN('FL', 'GA') and\n  year BETWEEN 2010 and 2014 and\n  name LIKE 'J%n'\n\nProgrammatic Query\n\n\nmytext = 'taqmsec.cqm_20201201'\nsymbols = c('AAPL','AA')\nsymbols1 = shQuote(symbols) %>% str_c(collapse = ',')\nres = dbSendQuery(wrds, \n                  str_glue(\n                  \"\n                  SELECT sym_root as symbol, date, time_m, bid, bidsiz, ask, asksiz\n                  FROM {mytext}\n                  WHERE \n                  bid !=0 and \n                  ask !=0 and \n                  sym_root IN ({symbols1})\n                  LIMIT 1000\n                  \"\n                  ))\n\n\nDates and Times\nUse extract function to extract year/month/hour etc.\n\n\nres = dbSendQuery(wrds, \n                   \"\n                   SELECT cusip_id, bond_sym_id, trd_exctn_dt\n                   FROM trace.trace_btds144a\n                   WHERE extract(year from trd_exctn_dt) <= 2005\n                   LIMIT 100\n                   \"\n                   )\n\n\nTRACE examples\nTRACE btds query\n\n\nres = dbSendQuery(wrds, \n                  \"\n                  SELECT *\n                  FROM trace.trace\n                  WHERE extract(year from trd_exctn_dt) <= 2005\n                  LIMIT 100\n                  \"\n)\n\ntrace2002.2005 = dbFetch(res)\ndbClearResult(res)\n\n\nTRACE btds 144a query\n\n\nres = dbSendQuery(wrds, \n                   \"\n                   SELECT cusip_id, bond_sym_id, trd_exctn_dt\n                   FROM trace.trace_btds144a\n                   WHERE extract(year from trd_exctn_dt) <= 2005\n                   LIMIT 100\n                   \"\n                   )\n\ntrace_btds2002.2005 = dbFetch(res)\n\n\nCompustat examples\nCompustat fundamental annual\n\n\nres = dbSendQuery(wrds,\n                  \"\n                  select gvkey, datadate, fyear,cusip, at,ch,mkvalt,revt,xint,oibdp,ppegt\n                  from compa.funda\n                  where\n                  consol ='C' and\n                  indfmt = 'INDL' and\n                  datafmt = 'STD' and\n                  popsrc = 'D' and\n                  curcd = 'USD'\n                  \"\n                  )\n\n\nCompustat fundamental quarterly\n\n\nres = dbSendQuery(wrds,\n                  \"\n                  select *\n                  from comp.fundq\n                  where\n                  consol ='C' and\n                  indfmt = 'INDL' and\n                  popsrc = 'D' and\n                  datafmt = 'STD' and\n                  curcdq = 'USD'\n                  \"\n)\n\n\nJoining two compustat\n\n\nres <- dbSendQuery(wrds, \n                   \"select a.gvkey, a.datadate, a.tic,\n                   a.conm, a.at, a.lt, b.prccm, b.cshoq\n                   from comp.funda a join comp.secm b\n                   on a.gvkey = b.gvkey\n                   and a.datadate = b.datadate\n                   where a.tic = 'IBM'\n                   and a.datafmt = 'STD'\n                   and a.consol = 'C'\n                   and a.indfmt = 'INDL'\")\ndata = dbFetch(res, n = -1)\ndbClearResult(res)\ndata\n\n\nCRSP examples\nCCM link table\n\n\nres = dbSendQuery(wrds,\"\n                  select * \n                  from crsp.ccmxpf_lnkhist\n                  \")\ncrspa.link = dbFetch(res)\ncrspa.link\n\n\nCRSP msenames\n\n\nres = dbSendQuery(wrds,\"\n                  select * \n                  from crsp.msenames\n                  \")\ntemp = dbFetch(res, n=1000)\ntemp\n\n\nCRSP daily stock file\n\n\nres <- dbSendQuery(wrds,\n                  \"select *\n                  from crsp.dsf\n                  \")\n\n\nCCM Linktable SAS Query\n\n\n# Code from Phil\n\n# 1. Merge permno on Compustat using linktable\n\n# proc sql;\n#   create table comp2 as\n#   select a.*, b.lpermno as permno\n#   from key_control as a left join crsp.ccmxpf_lnkhist as b\n#   on a.gvkey_string=b.gvkey and b.linkprim in ('P', 'C') and\n#   b.LINKTYPE in ('LU', 'LC') and\n#   a.datadate >= b.LINKDT and (a.datadate <= b.LINKENDDT or missing(b.LINKENDDT))\n#   order by gvkey, datadate;\n# quit;\n# \n \n# 2. Merge ncusip and others on Compustat using msenames\n\n# proc sql;\n#       create table comp3\n#       as select a.*, b.ncusip, b.COMNAM,b.siccd, b.ticker as ticker_crsp\n#       from comp2 as a left join (select distinct ncusip, COMNAM, permno, namedt, nameendt, ticker, siccd from crsp.msenames\n#     where not missing(ncusip)) as b\n#       on a.permno = b.permno and b.namedt<=a.datadate<=b.nameendt;\n# quit;\n\n\n# rsubmit;\n# Data funda;\n# Set comp.funda;\n# Where at~=. Or at>=0;\n# where sale~=. or sale>=0;\n# where ebitda~=.;\n# Keep gvkey datadate;\n# Run;\n# \n# proc sql; \n#   create table getf_3 as \n#   select a.*, b.lpermno as permno_all\n#   from funda a left join crsp.ccmxpf_linktable b \n#     on a.gvkey eq b.gvkey \n#     and b.lpermno ne . \n#     and b.linktype in (\"LC\" \"LN\" \"LU\" \"LX\" \"LD\" \"LS\") \n#     and b.linkprim IN (\"C\", \"P\")  \n#     and ((a.datadate >= b.LINKDT) or b.LINKDT eq .B) and  \n#        ((a.datadate <= b.LINKENDDT) or b.LINKENDDT eq .E)   ; \n# quit;\n# \n# proc sql;\n#       create table getf_4\n#       as select a.*, ncusip as ncusip_all \n#       from getf_3 as a left join (select distinct ncusip, COMNAM, permno, namedt, nameendt, ticker  from crsp.msenames\n#     where not missing(ncusip)) as b\n#       on a.permno_all = b.permno and b.namedt<=a.datadate<=b.nameendt;\n# quit;\n# \n# proc download data=getf_4 out=getf_4;run;\n# endrsubmit;\n#         \n# proc sql;\n#   create table comp2 as\n#   select a.*, b.lpermno as permno\n#   from key_control as a left join crsp.ccmxpf_lnkhist as b\n#   on a.gvkey_string=b.gvkey and b.linkprim in ('P', 'C') and\n#   b.LINKTYPE in ('LU', 'LC') and\n#   a.datadate >= b.LINKDT and (a.datadate <= b.LINKENDDT or missing(b.LINKENDDT))\n#   order by gvkey, datadate;\n# quit;\n# \n# \n# \n# proc sql;\n#       create table comp3\n#       as select a.*, b.ncusip, b.COMNAM,b.siccd, b.ticker as ticker_crsp\n#       from comp2 as a left join (select distinct ncusip, COMNAM, permno, namedt, nameendt, ticker siccd from crsp.msenames\n#     where not missing(ncusip)) as b\n#       on a.permno = b.permno and b.namedt<=a.datadate<=b.nameendt;\n# quit;\n# \n# rsubmit;\n# Data funda;\n# Set comp.funda;\n# Where at~=. Or at>=0;\n# where sale~=. or sale>=0;\n# where ebitda~=.;\n# Keep gvkey datadate;\n# Run;\n# \n# proc sql; \n#   create table getf_3 as \n#   select a.*, b.lpermno as permno_all\n#   from funda a left join crsp.ccmxpf_linktable b \n#     on a.gvkey eq b.gvkey \n#     and b.lpermno ne . \n#     and b.linktype in (\"LC\" \"LN\" \"LU\" \"LX\" \"LD\" \"LS\") \n#     and b.linkprim IN (\"C\", \"P\")  \n#     and ((a.datadate >= b.LINKDT) or b.LINKDT eq .B) and  \n#        ((a.datadate <= b.LINKENDDT) or b.LINKENDDT eq .E)   ; \n# quit;\n# \n# proc sql;\n#       create table getf_4\n#       as select a.*, ncusip as ncusip_all \n#       from getf_3 as a left join (select distinct ncusip, COMNAM, permno, namedt, nameendt, ticker  from crsp.msenames\n#     where not missing(ncusip)) as b\n#       on a.permno_all = b.permno and b.namedt<=a.datadate<=b.nameendt;\n# quit;\n# \n# proc download data=getf_4 out=getf_4;run;\n# endrsubmit;\n\n\nTAQ examples\nlibname scratch \"/scratch/ufl/mgson633/\";\n\n%macro loop;\n\n%let start_date=20200101;\n%let end_date=20200331;\ndata scratch.quotes_&end_date; stop; run;\n\n%do %while (&end_date >= &start_date);\n    %let date_id = &start_date;\n    \n    %if %sysfunc(exist(taqmsec.cqm_&date_id)) %then %do;\n    \n        data quotes / view=quotes;\n        set taqmsec.cqm_&date_id (  /* where=(sym_root in(\"AAPL\", \"MCD\", \"AACQ\", \"AAIC\")) */  keep=sym_root sym_suffix date time_m bid bidsiz ask asksiz);\n        if not missing(sym_suffix) then delete;\n        time = floor(time_m/600);\n        if (54 <= time <= 90) and bid NE 0 and ask NE 0 and bidsiz NE 0 and asksiz NE 0 then output;\n        run;\n    \n        data quotes1 (drop=sym_suffix);\n        set quotes;\n        by sym_root time;\n        where not missing(sym_root);\n        if last.time then output; \n        run;\n        \n        data scratch.quotes_&end_date; set scratch.quotes_&end_date quotes1; run;\n        \n    %end;\n    \n%let start_date = %eval(&start_date+1);\n%end;\n\n%mend loop;\n\n%loop;\nNational best bid and offer\ndata quotes;\n    set taqmsec.nbbom_20200102 (  where=(sym_root in(\"AAPL\", \"MCD\", \"AACQ\", \"AAIC\")) \n    keep=sym_root sym_suffix date time_m best_bid best_bidsiz best_ask best_asksiz);\n    if not missing(sym_suffix) then delete;\n    time = floor(time_m/600);\n    if (54 <= time <= 90) then output;\nrun;\n    \ndata quotes1 (drop=sym_suffix);\nset quotes;\nby sym_root time;\nwhere not missing(sym_root);\nif last.time then output; \nrun;\nBatch Job on WRDS\nInfo\nJob Limits Both interactive and batch jobs have the following\nlimits:\nYou can use up to 5 concurrent jobs (10 Core Maximum), and may queue\nany additional jobs. Additional jobs will wait in the queue until active\njobs have finished. You can use up to 150GB memory, and may queue any\nadditional jobs. Additional jobs will wait in the queue until your\ncurrent total memory allocation drops below the limit. Your subscribing\ninstitution can run up to 10 concurrent jobs across all users who are\nmembers of that institution, and may queue any additional jobs.\nAdditional jobs will wait in the queue until active jobs have finished.\nEach active batch job may run for one week (168 hours). Each active\ninteractive job may run for 3 days (72 hours). Jobs running after the\nlimit has elapsed will be automatically terminated. Batch jobs\nadditionally have the following CPU and RAM usage restrictions:\nBy default, a WRDS Cloud job is assigned to 2 cores (for a total of 4\nthreads) per job. Users may request up to 8 cores (for a total of 16\nthreads) per job. By default, a WRDS Cloud job is assigned 8GB per core\n(for a total of 16GB if left at the default core count). Users may\nrequest up to 48GB RAM total per job (calculated as RAM * cores\nrequested).\nConnect to wrds cloud system\nssh mgson633@wrds-cloud.wharton.upenn.edu\nFor interactive R jobs\nqrsh \\# connect to the WRDS interactive node R --no-save --no-restore\nBatch job SAS example\nWhen sourcing sas file directly\nqsas my_sample.sas\nWhen sourcing shell script\nqsub script.sh\n#!/bin/bash\n#$ -cwd\n#$ -m abe\n#$ -M gunsu.son@ufl.edu\n#$ -pe onenode 8\n#$ -l m_mem_free=6G\n\nsas my_program.sas\nBatch job R example\nMaximum example\n#!/bin/bash\n#$ -cwd\n#$ -m abe\n#$ -M gunsu.son@ufl.edu\n#$ -pe onenode 8\n#$ -l m_mem_free=6G\n\ncd /home/ufl/mgson633/Projects/Fund_centrality/codes\n\nR CMD BATCH run_batch2_17-21.R 17-21.out\nAbove MAXIMUM for 1 job: 8 core, 48 GB ram\nreturn log back to cwd, give email notification, 8 core, 6g mem per\ncore.\nDefault batch jobs: CPU Cores: 2 per job RAM Usage: 16GB per job\nHARD LIMIT : 5 jobs with 150G Memory / 10 cores maximum\nSubmit batch job\nqsub my_program.sh\nqsub -q all.q@wrds-sas24-h my_program.sh \nManaging jobs\nShow all currently-running and queued jobs submitted by all users\nqstat -u \\* \nShow all compute nodes, including number of processors and amount of\nRAM per node\nqhost\nqhost -j \nDelete your job #1234567\nqdel 1234567 \nrequest more computing power\nhttps://wrds-www.wharton.upenn.edu/pages/support/the-wrds-cloud/running-jobs/batch-jobs-wrds-cloud/\nRequesting Additional CPU / RAM By default, the WRDS Cloud imposes\nthe following limitations on resource consumption (CPU and RAM) to\nmaintain a fair use environment for all WRDS users:\nbatch jobs: CPU Cores: 2 per job RAM Usage: 16GB per job interactive\njobs: CPU Cores: 1 per job RAM Usage: 8GB per job\nIf you need more than this for your job, you may request more up to\nthe following hard limits:\nCPU Cores: 8 per job RAM Usage: 48GB Total per Job\n\\#!/bin/bash \n\\#\\$ -cwd \n\\#\\$ -pe onenode 8 \n\\#\\$ -l m_mem_free=6G\nHiPerGator\nLog-in nodes : limit 16 cores, 64GB ram, 10 mins\nDo not run programs on login nodes\nHiperGator 1 : 4 sockets, 16cores each - 64 cores on node , 250GB ram\n(retired) HiperGator 2 : 2 sockets, 16cores each - 32 cores on node ,\n120GB ram HiperGator 3 : 8 sockets, 16cores each - 128 cores on node,\n1024GB ram\nMore info : nodeInfo\nHome Area: /home/\n40GB limit\nscripts, code, small data\nDo NOT use for job input/output\n\nBlue storage : /blue// (hypergator 3.0) :\n/ufrc// is 2.0 ver\nALL input/output from jobs should go here\n/blue/nimalendran/matthewson\n\nBase Info\nTo see running jobs\nsacct\nslurm info\nslurmInfo\nTo browse all hypergator nodes specifiction : NodeInfo\nnodeInfo\nshow QoS info\nshowQos nimalendran\nshowQos nimalendran-b\nTo check the disk quota for blue Currently we have 4TB quota for blue\nstorage\nblue_quota\nConnection & ID\nConnecting from Mac (Connect to Log in nodes )\nssh <username>@hpg.rc.ufl.edu\nCheck group membership\nid\n\nuid=3670(gunsu.son) gid=3446(nimalendran) groups=3446(nimalendran)\nCheck Association of user\nshowAssoc gunsu.son\nConnect\nconnect to development\nserver\nFrom log in node to development, 60 mins\nsrundev -t 60\nnow node is from login to development\nInstead, interactive on SLURM (connect to compute server) is another\noption, see below SLURM INTERACTIVE srun\nconnect to to compute server\nsrun --partition=hpg-default --ntasks=1 --cpus-per-task=128 --qos=nimalendran-b --nodes=1 --mem=1024gb --time=96:00:00 --mail-type=END,FAIL --mail-user=gunsu.son@ufl.edu --pty bash -i\nRStudio Server\nSetup\nInstallation\nTake a look at the server info:\nlsb_release -a\nHPG is RedHatEnterpriseServer Release is 7.7\nRedHat/CentOS 7 is the right one for HPG.\nwget https://download2.rstudio.org/server/centos7/x86_64/rstudio-server-rhel-1.4.1717-x86_64.rpm\nsudo yum install rstudio-server-rhel-1.4.1717-x86_64.rpm\nUser Setup\nRun R\nmodule load R\nR\nCheck computing allocation of group\nslurmInfo nimalendran\nsacct\n\nshowQos nimalendran\nSFTP Setup\nport 22\nhpg.rc.ufl.edu\nSLURM\nSLURM INTERACTIVE\nYou can request resources for an interactive session (i.e. job) and\nstart a command shell (bash, for example). Within that command shell you\nwill have access to the resources you requested and can run whatever\ncommands and processes you wish. Consider the example below. It will\ngive you 4 GB of memory for 8 hours on a real compute host and present\nyou, once the job starts, with an interactive command shell (bash). From\nthat shell you can run commands and launch processes just as you would\nfrom any other host (login or otherwise).\nQOS\nsrun --partition=hpg2-compute --ntasks=1 --cpus-per-task=24 --qos=nimalendran --nodes=1  --mem-per-cpu=3gb --time=08:00:00 --pty bash -i\nQOS-b\nHypergator 2.0 : 1 node max = 32 cpu (2 socket 16 cores each) 125GB\nram\nsrun --partition=hpg2-compute --ntasks=1 --cpus-per-task=32 --qos=nimalendran-b --nodes=1  --mem-per-cpu=3gb --time=08:00:00 --pty bash -i\nHypergator 3.0 : 1 node max = 128 cpu (8 socket 16 cores each) 1TB\nram Check with nodeInfo\nsrun --partition=hpg-default --ntasks=1 --cpus-per-task=128 --qos=nimalendran-b --nodes=1 --mem-per-cpu=7gb --time=72:00:00 --pty bash -i\nHypergator 3.0 : 1 node 100 cpu 600 GB ram\nsrun --partition=hpg-default --ntasks=1 --cpus-per-task=100 --qos=nimalendran-b --nodes=1 --mem-per-cpu=6gb --time=08:00:00 --pty bash -i\n\n\n\n\n\nsrun --partition=hpg-default --nodes=10 --ntasks-per-node=1 --cpus-per-task=12 --qos=nimalendran-b --mem-per-cpu=6gb --time=08:00:00 --pty bash -i\nSuppose you need 16 cores. Here are some use cases:\nProcesses : think of a new terminal, or R instances\nyou use mpi and do not care about where those cores are distributed:\n–ntasks=16 you want to launch 16 independent processes (no\ncommunication): –ntasks=16 you want those cores to spread across\ndistinct nodes: –ntasks=16 and –ntasks-per-node=1 or –ntasks=16 and\n–nodes=16 you want those cores to spread across distinct nodes and no\ninterference from other jobs: –ntasks=16 –nodes=16 –exclusive you want\n16 processes to spread across 8 nodes to have two processes per node:\n–ntasks=16 –ntasks-per-node=2\nyou want 16 processes to stay on the same node: –ntasks=16\n–ntasks-per-node=16 you want one process that can use 16 cores for\nmultithreading: –ntasks=1 –cpus-per-task=16 you want 4 processes that\ncan use 4 cores each for multithreading: –ntasks=4 –cpus-per-task=4\nSlurm complicates this, however, by using the terms core\nand cpu interchangeably depending on the context and Slurm\ncommand. –cpus-per-tasks= for example is actually specifying the\nnumber of cores per task.\nSubmit batch jobs\nSave .sh script and run the script file with\nsbatch job_file.sh\n#!/bin/bash\n#SBATCH --job-name=slurm_r_similarity      # Job name\n#SBATCH --mail-type=END,FAIL         # Mail events (NONE, BEGIN, END, FAIL, ALL)\n#SBATCH --mail-user=gunsu.son@ufl.edu    # Where to send mail.  Set this to your email address\n#SBATCH --ntasks=1                  # Number of MPI tasks (i.e. processes)\n#SBATCH --cpus-per-task=64            # Number of cores per MPI task (multithreading)\n#SBATCH --qos=nimalendran-b         # QOS\n#SBATCH --nodes=1                    # Maximum number of nodes to be allocated\n#SBATCH --ntasks-per-node=1         # Maximum number of tasks on each node\n#SBATCH --ntasks-per-socket=1        # Maximum number of tasks on each socket\n#SBATCH --distribution=cyclic:cyclic # Distribute tasks cyclically first among nodes and then among sockets within a node\n#SBATCH --mem-per-cpu=3Gb          # Memory (i.e. RAM) per processor\n#SBATCH --time=24:00:00              # Wall time limit (days-hrs:min:sec)\n#SBATCH --output=R_run_log.log     # Path to the standard output and error files relative to the working directory\n#SBATCH --partition=hpg2-compute   # To which partition\n\n\ncd /blue/nimalendran/gunsu.son/projects/code\n\n\nmodule load R\n\nRscript --vanilla slurm_work.R\nto check the queue : my job running on the server\nsqueue -u username\nto cancel job\nscancel job_id_number\nhistorical info about jobs\nsacct\nMultiple batch jobs in\nparallel\nYou could ask for 29 tasks, 1 cpu per task (you will get from 29 cpus\non a node to 1 cpu in 29 different nodes), and in the slurm script you\nshould start your calculus with srun, telling srun to allocate one\ntask/cpu per chunk.\n\n#SBATCH --ntasks=29\n#SBATCH --cpus-per-task=1\n\nfor n in {1..29}\ndo\n    srun -n 1 <your_script> $n &\ndone\nwait\n\nMPI jobs\nTo run MPI script, must use srun\nsrun --mpi=pmix_v2 myApp\nYou must set –ntasks=1, and then set –cpus-per-task to the number of\nOpenMP threads you wish to use.\n1 node and multiple cores (say 64 cores with group-b QOS)\nMaximum 32 workers and workers has 1 core, 2 nodes (server\ncomputers), each nodes can have 16 max workers. Since server computers\nhave two sockets and each socket has 16 cores, setting ntasks per socket\nto be half of that for fun, 8\n#SBATCH --ntasks =32\n#SBATCH --cpus-per-task=1\n#SBATCH --nodes =2\n#SBATCH --ntasks-per-node=16\n#SBATCH --ntasks-per-socket=8\n#SBATCH --constraint=infiniband\nOther example\nUsing 4 server computer nodes. Each node can do 4 tasks in parallel\n(ntasks per node). Each task can have 4 workers (cores).\n#SBATCH --ntasks =16\n#SBATCH --cpus-per-task=4\n#SBATCH --nodes =4\n#SBATCH --ntasks-per-node=4\n#SBATCH --ntasks-per-socket=2\n#SBATCH --constraint=infiniband\n–ntasks=# : Number of “tasks” (use with distributed parallelism).\n–ntasks-per-node=# : Number of “tasks” per node (use with distributed\nparallelism).\n–cpus-per-task=# : Number of CPUs allocated to each task (use with\nshared memory parallelism).\nQ: if every node has 24 cores, is there any difference between these\ncommands?\nsbatch –ntasks 24 […] sbatch –ntasks 1 –cpus-per-task 24 […]\nAnswer:\nYes there is a difference between those two submissions. You are\ncorrect that usually ntasks is for mpi and cpus-per-task is for\nmultithreading, but let’s look at your commands:\nFor your first example, the sbatch –ntasks 24 […] will allocate a job\nwith 24 tasks. These tasks in this case are only 1 CPUs, but may be\nsplit across multiple nodes. So you get a total of 24 CPUs across\nmultiple nodes.\nFor your second example, the sbatch –ntasks 1 –cpus-per-task 24 […]\nwill allocate a job with 1 task and 24 CPUs for that task. Thus you will\nget a total of 24 CPUs on a single node.\nIn other words, a task cannot be split across multiple\nnodes. Therefore, using –cpus-per-task will ensure it gets\nallocated to the same node, while using –ntasks can and may allocate it\nto multiple nodes.\nSuppose you need 16 cores. Here are some use cases:\nyou use mpi and do not care about where those cores are distributed:\n–ntasks=16 you want to launch 16 independent processes (no\ncommunication): –ntasks=16 you want those cores to spread across\ndistinct nodes: –ntasks=16 and –ntasks-per-node=1 or –ntasks=16 and\n–nodes=16 you want those cores to spread across distinct nodes and no\ninterference from other jobs: –ntasks=16 –nodes=16 –exclusive\n–exclusive ensures srun uses distinct CPUs for each job step\nyou want 16 processes to spread across 8 nodes to have two processes\nper node: –ntasks=16 –ntasks-per-node=2 you want 16 processes to stay on\nthe same node: –ntasks=16 –ntasks-per-node=16 you want one process that\ncan use 16 cores for multithreading: –ntasks=1 –cpus-per-task=16 you\nwant 4 processes that can use 4 cores each for multithreading: –ntasks=4\n–cpus-per-task=4\nhttps://login.scg.stanford.edu/faqs/cores/\nH2O on SLURM\nmodule load R\nR\nhttps://docs.h2o.ai/h2o/latest-stable/h2o-docs/faq/clusters.html#how-can-i-create-a-multi-node-h2o-cluster-on-a-slurm-system\n\n\nlibrary(stringr)\n# Check on the nodes we have access to.\np\ncat(\"SLURM nodes:\", node_list, \"\\n\")\n\n# Loop up IPs of the allocated nodes.\nif (node_list != \"\") {\n  id = str_match(node_list, '(.*)\\\\[(.*)\\\\]')[1,2]\n  numbers = strsplit(str_match(node_list, '(.*)\\\\[(.*)\\\\]')[1,3], ',')\n  nodes = strsplit(node_list, \",\")[[1]]\n  ips = rep(NA, length(nodes))\n  for (i in 1:length(nodes)) {\n    args = c(\"hosts\", nodes[i])\n    result = system2(\"getent\", args = args, stdout = T)\n    # Extract the IP from the result output.\n    ips[i] = sub(\"^([^ ]+) +.*$\", \"\\\\1\", result, perl = T)\n  }\n  cat(\"SLURM IPs:\", paste(ips, collapse=\", \"), \"\\n\")\n  # Combine into a network string for h2o.\n  network = paste0(paste0(ips, \"/32\"), collapse=\",\")\n  cat(\"Network:\", network, \"\\n\")\n}\n\n# Specify how many nodes we want h2o to use.\nh2o_num_nodes = length(ips)\n\n# Options to pass to java call:\nargs = c(\n  # -Xmx30g allocate 30GB of RAM per node. Needs to come before \"-jar\"\n  \"-Xmx30g\",\n  # Specify path to downloaded h2o jar.\n  \"-jar ~/software/h2o-latest/h2o.jar\",\n  # Specify a cloud name for the cluster.\n  \"-name h2o_r\",\n  # Specify IPs of other nodes.\n  paste(\"-network\", network)\n)\ncat(paste0(\"Args:\\n\", paste(args, collapse=\"\\n\"), \"\\n\"))\n\n# Run once for each node we want to start.\nfor (node_i in 1:h2o_num_nodes) {\n  cat(\"\\nLaunching h2o worker on\", ips[node_i], \"\\n\")\n  new_args = c(ips[node_i], \"java\", args)\n  # Ssh into the target IP and launch an h2o worker with its own\n  # output and error files. These could go in a subdirectory.\n  cmd_result = system2(\"ssh\", args = new_args,\n                       stdout = paste0(\"h2o_out_\", node_i, \".txt\"),\n                       stderr = paste0(\"h2o_err_\", node_i, \".txt\"),\n                       # Need to specify wait=F so that it runs in the background.\n                       wait = F)\n  # This should be 0.\n  cat(\"Cmd result:\", cmd_result, \"\\n\")\n  # Wait one second between inits.\n  Sys.sleep(1L)\n}\n\n\n# Wait 3 more seconds to find all the nodes, otherwise we may only\n# find the node on localhost.\nSys.sleep(3L)\n\n# Check if h2o is running. We will see ssh processes and one java process.\nsystem2(\"ps\", c(\"-ef\", \"| grep h2o.jar\"), stdout = T)\n\nsuppressMessages(library(h2oEnsemble))\n\n# Connect to our existing h2o cluster.\n# Do not try to start a new server from R.\nh2o.init(startH2O = F)\n\n#################################\n\n# Run H2O commands here.\n\n#################################\nh2o.shutdown(prompt = F)\n\n\nUNIX SHELL\nHead\n\n\nterminalExecute('head -n 5 data.csv')\n\n\nWord count\nwc\nList files\n\nls folder\n\nlist files with pattern matching - using * (any)\n\nls folder/regex\n\nMove files\n\n\nterminalExecute()\n\n\n\nmv fromPath toPath\n\nwith pattern : use regex\n\nmv crust.etcMC* /home/out\n\nLaTeX\nTinyTex\nhttps://yihui.org/tinytex/\nInstall TinyTex\n\n\ninstall.packages('tinytex')\ntinytex::install_tinytex()\n\n\nIn terminal, make the folder writable\nsudo chown -R `whoami`:admin /usr/local/bin\nBibTex\nPut this codebox on top of the code to identify the workding\nfolder\n\n\n#Set Environment Variables\nTEXINPUTS=\"~/Dropbox (UFL)/My Projects/Fund_centrality/code/latex\" #Path to tex file in Windows\n\nSys.setenv(TEXINPUTS=\"~/Dropbox (UFL)/My Projects/Fund_centrality/code/latex\", BIBINPUTS=TEXINPUTS,BSTINPUTS=TEXINPUTS) \n\n#Path to texfiles in Windows, set BIB files and BST files the same\n\n#Run before clicking \"Compile PDF\"\n\n\npackage install\nInstall missing packages from log file :\n\n\n# put log file location\ntinytex::parse_install('latex/beamer/uf_theme.log')\n\n\nSearch package name by filename\n\ntlmgr search --global --file \"/authblk.sty\"\n\nThen the package name one level upper folder :\npreprint\n\ntlmgr install preprint\n\nUsually the filename identical to the package name.\n\n\ntinytex::tlmgr('install ae grfext')\n\n\nOr within the terminal\ntlmgr install ae grfext\nSweave\nPut this\n\\\\SweaveOpts{concordance=TRUE}\nKnitr\nUse package PatchSynctex for concordance, and setting working\ndirectory for .bib recognition.\n\\documentclass[11pt]{article}\n\n<<Setup, eval=T, echo=FALSE, include = FALSE, results='hide'>>=\nopts_knit$set(concordance=TRUE, self.contained=TRUE)##$\nrequire(patchSynctex)\nsetwd('~/Dropbox (UFL)/My Projects/Fund_centrality/code/latex')\nSys.setenv(TEXINPUTS=getwd(),\nBIBINPUTS=getwd(),\nBSTINPUTS=getwd())\n@\n\n\\usepackage[margin=1in]{geometry} % for margin set\n\\usepackage{setspace} % for double spacing\n\\usepackage{hyperref} % hyperlinks\nStargazer\nDescriptive statistics table\n\n\nstargazer::stargazer(iris, \n                     type='text', \n                     title ='Descriptive stat, iris',\n                     notes = 'This is note of the table.')\n\n\nT statistics instead of standard error\n\n\ntrace(stargazer:::.stargazer.wrap, edit = T)\n# .format.t.stats.left <- \"t = \" and .format.t.stats.right <- \"\" modify this, line at (7050~7060)\n\n\nMy example of regression table\n\n\nstargazer(list(spec1,spec2,spec3,spec4,spec5,spec6), \n          # dep.var.caption = 'Dependant Variables',\n          # initial.zero = F\n          # type='text',\n          \n          report='vc*t',\n          label = 'tab:table3',\n          digits=3,\n          title = \"Holding Distinctiveness and Alpha\",\n          dep.var.labels = c('$Alpha_{t}$','$Alpha_{t+1}$'),\n          covariate.labels = c('Centraltity','Concentration','ln(Size)','Age','Expense Ratio','Turnover Ratio','Flow','Centrality * Concentration'),\n          add.lines = list(c(\"Fund Fixed Effect\", \"Yes\",\"Yes\",\"Yes\",\"Yes\",\"Yes\",\"Yes\"),\n                           c(\"Month Fixed Effect\", \"Yes\",\"Yes\",\"Yes\",\"Yes\",\"Yes\",\"Yes\"),\n                           c(\"Fund SE Cluster\", \"Yes\",\"Yes\",\"Yes\",\"Yes\",\"Yes\",\"Yes\")),\n          omit.stat = c('rsq','ser'), # omit rsquared, standard error residual\n          omit.table.layout = \"n\" # omit notes below \n          ) \n\n\nGit (Github)\nDefault git installation (MacOS)\n/usr/bin/git\nGit : version control of folder and all files\nStaging area : waiting stage that will be committed finally in\nlater\nGit has a staging area in which it stores files with changes you want\nto save that haven’t been saved yet.\nBasic Commands\nstatus\nStatus of git (which files are modified but not not staged for\ncommitment?, or which files are staged to be committed?)\n\ngit status\n\ndiff\nShow the modified part of the files\n\n# See what changes have made\ngit diff # show all changes \ngit diff file1 # comapre previous filname1 and revised filename1 \n\ngit diff -r HEAD # -r : compare to particular revision, HEAD : the most recent commit\ngit diff -r HEAD~1 # HEAD~1 refers to a commit before most recent commit\n\n# compared to the most recent commitment, show all changes \ngit diff -r HEAD file1 # compare file1 with most recent commitment\n\nShow the difference between two commits\n\ngit diff ID1..ID2 # two hash IDs that are connected through dotdot\ngit diff abc123..def456\ngit diff HEAD..HEAD~2\n\nadd\nAdd files to be staged (modifications to be done)\n\n# Add files to the staging area so that it may be committed later\ngit add file1\n\nWhen another change in file1 was done, then it has to be staged again\nto be committed\nreset\nWhen unstaging should be done, use git reset.\n\ngit reset # unstage all\ngit reset HEAD # same\n\ngit reset file1 # undo staging file1 \ngit reset HEAD file1 # same\ngit reset HEAD -- file1 # same\n\ncheckout\nCheckout is like loading the (specific) saved version, discarding new\nchanges made, while committing is saving the current version. Undo the\nchanges that you made since the last commit. When modified file(s)\nshould not be staged, use git checkout.\nWhen modify a file but would not want to stage the modifications made,\nuse git checkout\n\ngit checkout file1 # remove unstaged modification and load the last committed version\ngit checkout -- file1 # be careful that -- should be spaced between \n\nIt can be used to go back further into the file’s history.\n\ngit checkout hash file1 # load the specific history version of the file, removing changes that were made after this.\n\ncommit\nMaking commitment to the github\n\n\ngit commit # commit it\ngit commit -m \"Commitment Message\" # add commitment message to it\ngit commit --amend -m \"Amend the message I just committed\" # to amend the commit just made\n\nlog\nBrowsing the log of git\n\ngit log # show the most recent git committments with commitment messages. The most recent one is on the top\ngit log file1 # show only git commits related to the file1\ngit log -2 file1 #show most recent 2 commits of file1\n\nshow\nBrowse specific commit by its hash\n\ngit show someHash #View specific commit by its hash, with some initial characters of hash\n\nSee the change of the file in detail: who changed when, what in a file\n\ngit annotate file1\n\nremove\nClean (remove) files that are not staged\n\ngit clean -n # list of files that are in the repository, but untracked - currently it does not have any history.\ngit clean -f # delete those untracked file in the repository\n\nIgnore files\ngenerate .gitignore file to make exceptions\nTo create list of files that git should ignore : generate .gitignore\nfile such as\nfoldername\n*.mpl\nConfig\nShow config setting\n\ngit config --list # lists all the custom configs\ngit config --list --local # for the specific project (repo)\ngit config --list --global # for every projects\ngit config --list --system # for the system, every user of this computer\n\nSet config\n\n# set name and email address globally\ngit config --global user.name John Doe # set user name as John Doe\ngit config --global user.email address@gmail.com # set user email address\n\nGmailR\nhttp://console.developers.google.com/\nand setup Gmail API & Credentials (OAuth -> get client ID and\npwd in json file)\nIt is TESTing mode when created -> Change it into production\nmode.\nInstall\n\n\ninstall.packages('gmailr')\n\n\nUsage\nclient_secret.json : downloaded from the google cloud platform\nconsole.\n\n\nlibrary(gmailr)\ngm_auth_configure(path = '~/Dropbox (UFL)/Data Workshop/R/client_secret.json')\n?gm_auth_configure\n\n\nSend email\n\n\nemail_message = \n  gm_mime() %>% \n  gm_to('gsist156@gmail.com') %>% \n  # gm_from('mgson633@gmail.com') %>%  #doesn't work actually.\n  gm_subject('Test message') %>% \n  gm_text_body('This is the body') %>% \n  gm_attach_file('~/Dropbox (UFL)/Data Workshop/R/R_CookBook.nb.html')\n\ngm_send_message(email_message)\n\n\nWEB DATA\nRead CSV from web\n\n\ncsv_url <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv\"\ncsv_data = read.csv(csv_url)\ncsv_data %>% head\n\n\nDownload files\ndownload.file() does the trick.\n\n\ncsv_url <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv\"\ndownload.file(url = csv_url, destfile = \"feed_data.csv\")\n\n\nUsing API\nThe first thing to check : whether there’s R package available for\nthe website.\nGoogle search CRAN [website address]\nIf no API wrapper found in CRAN then use below methods.\nPackage httr can be alternative of python’s\nBeautifulSoup\nGET, POST\n\n\nlibrary('httr')\n# Get the url, save response to resp\nurl <- \"http://www.example.com/\"\nresp <- GET(url)\n# Print resp\nresp\n\n# Get the raw content of resp: raw_content\nraw_content <- content(resp, as = \"text\") # Default => automatic JSON parsing -> list if JSON\n\n# Print the head of raw_content\nhead(raw_content)\n\n\n\n\n# Make a POST request to http://httpbin.org/post with the body \"this is a test\"\npost_result <- POST(url = 'http://httpbin.org/post',body = 'this is a test')\nprint(post_result)\n\n\nPage error handling\nhttp_error function returns TRUE if the web\npage is broken.\n\n\nfake_url <- \"http://google.com/fakepagethatdoesnotexist\"\n\n# Make the GET request\nrequest_result <- GET(fake_url)\n\n# Check request_result\nif(http_error(request_result)){\n  warning('The request failed')\n} else {\n  content(request_result)\n}\n\n\nAPI\nThere are two types of URLs:\nDirectory based URLs\nThis is very intuitive case, one can simply work on the string\nmanipulation to work on this stuff.\nParameter based URLs\nOne has to send query on the homepage, and httr’s\nGET function nicely does the job.\nDirectory based URL API\n\n\n# Construct a directory-based API URL to `http://swapi.co/api`,\n# looking for person `1` in `people`\ndirectory_url <- paste(\"http://swapi.co/api\", \"people\", \"1\", sep = '/')\n\n# Make a GET call with it\nprint(directory_url)\nresult <- GET(directory_url)\nprint(result)\n\n\nQuery based URL API\n\n\n# Create list with nationality and country elements\nquery_params <- list(nationality = 'americans', \n    country = 'antigua')\n    \n# Make parameter-based call to httpbin, with query_params\nparameter_response <- GET('https://httpbin.org/get', query = query_params)\n\n# Print parameter_response\nprint(parameter_response)\n\n\nUser agents\n\n\nurl <- \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Aaron_Halfaker/daily/2015100100/2015103100\"\n\n# Add the email address and the test sentence inside user_agent()\nserver_response <- GET(url, user_agent(\"my@email.address this is a test\"))\nserver_response\nSys.sleep(1) # sleep function\n\n\nJSON to dataframe\njsonlite package, and httr packages\nhttr’s content function uses\njsonlite at the backend.\n\n\n# jsonlite use case\nlibrary(jsonlite)\n# Definition of quandl_url\nquandl_url <- \"https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?auth_token=i83asDsiWUUyfoypkgMz\"\n# Import Quandl data: quandl_data\nquandl_data <- fromJSON(quandl_url)\nquandl_data # JSON is parsed into list object\n\n\n# httr use case\nlibrary(httr)\nresponse = GET(quandl_url)\ncontent(response) # Also parsed into list object\n\n\n\n\n# use dplyr package\n# Notice : content(resp_json) is list class\nhttp_type(resp_json) # check if the response is json type\nrevs %>%\n  bind_rows() # bind_rows function makes named list into dataframe\n\n?http_type\n\n\nXML to dataframe\n\n\nlibrary('xml2')\nrev_text = content(resp_xml, as = 'text') # as ='parsed' does the xml parsing but it is being explicit here\nrev_xml = read_xml(rev_text)\n# Examine the structure of rev_xml\nxml_structure(rev_xml)\n\n\n3\nNow using XPath to find elements\n\n\n# Find all nodes using XPATH \"/api/query/pages/page/revisions/rev\"\nxml_find_all(rev_xml, \"/api/query/pages/page/revisions/rev\")\n\n# Find all rev nodes anywhere in document\nrev_nodes <- xml_find_all(rev_xml, '//rev')\n\n# Use xml_text() to get text from rev_nodes\nxml_text(rev_nodes)\n# All rev nodes\nrev_nodes <- xml_find_all(rev_xml, \"//rev\")\n\n# The first rev node\nfirst_rev_node <- xml_find_first(rev_xml, \"//rev\")\n\n# Find all attributes with xml_attrs()\nxml_attrs(first_rev_node)\n\n# Find user attribute with xml_attr()\nxml_attr(first_rev_node, 'user')\n\n# Find user attribute for all rev nodes\nxml_attr(rev_nodes, 'user')\n\n# Find anon attribute for all rev nodes\nxml_attr(rev_nodes, 'anon')\n\n\nWeb scraping\nRSelenium\nInstallation\nJava Installation\nPrerequisite: Java has to be installed, so that hitting\njava on terminal should work. Installing openjdk from brew\ndoes not set the link well, so go to http://www.java.com and download (jre) Or below code\ndownloads the file & opens it\n\n\ndownload.file('https://javadl.oracle.com/webapps/download/AutoDL?BundleId=244576_d7fc238d0cbf4b0dac67be84580cfb4b', '~/Downloads/jre8.dmg')\nsystem('~/Downloads/jre8.dmg')\n\n\nRselenium\nFirst install RSelenium package\n\n\ninstall.packages('RSelenium')\n\n\nThen download Selenium Server binary, which can be found here http://selenium-release.storage.googleapis.com/index.html\nLook for selenium-server-standalone-x.xx.x.jar.\nThis command would do the trick for now.\n\n\ndownload.file('http://selenium-release.storage.googleapis.com/3.9/selenium-server-standalone-3.9.1.jar', '~/.R/selenium-standalone-3.9.1.jar')\n\n\nFor standalone server operation. I put that in ~/.R/\nfolder.\nRun .jar file from console on mac using\njava -jar ~/.R/selenium-standalone-3.9.1.jar\nFor Safari, enable developer menu > allow automation. For Chrome,\ndownload proper version of chrome driver and locate it in\n/usr/local/bin which is already in the path(then it is\ndone)\n\n\ndriver_file_zipped = '~/Downloads/chromedriver_mac64.zip'\ndownload.file('https://chromedriver.storage.googleapis.com/90.0.4430.24/chromedriver_mac64.zip', driver_file_zipped) \n# then move the file to the /usr/local/bin\n\n\nAuthrization problem\nIF Chromedriver is not authorized app by Apple, so needs to be\nun-quarantined. Navigate to chromedriver path (/usr/local/bin/) and\nsudo xattr -d com.apple.quarantine /usr/local/bin/chromedriver \n(this worked for me)\nIf Apple cannot identify the developer still,\nspctl --add --label 'Approved' <name-of-executable>\nBasic Commands\nStart selenium server\nOn terminal,\n\njava -jar ~/.R/selenium-standalone-3.9.1.jar\n\nOr by using rstudioapi package\n\n\nrstudioapi::terminalExecute('java -jar ~/.R/selenium-standalone-3.9.1.jar')\n\n\nRun driver\n\n\nlibrary(RSelenium)\nremDr = remoteDriver(\n  remoteServerAddr = \"localhost\",\n  port = 4444L,\n  browserName = \"chrome\"\n)\n\n\nBasic examples\n\n\nremDr$open() # fire up the browser\n\n\nrvest\nwith rvest package (similar to BeautifulSoup4 in python,\nfor simple web scraping only)\n\n\n# Load rvest\nlibrary('rvest')\n\n# Hadley Wickham's Wikipedia page\ntest_url <- \"https://en.wikipedia.org/wiki/Hadley_Wickham\"\n\n# Read the URL stored as \"test_url\" with read_html()\ntest_xml <- read_html(test_url)\n\ntest_xml\n\n\n\n\ntest_node_xpath = \"//*[contains(concat( \\\" \\\", @class, \\\" \\\" ), concat( \\\" \\\", \\\"vcard\\\", \\\" \\\" ))]\"\n# Use html_node() to grab the node with the XPATH stored as `test_node_xpath`\nnode <- html_node(x = test_xml, xpath = test_node_xpath)\n\n# Print the first element of the result\nnode[1]\n\n\nExtracting with rvest\nhtml_text get text contents\nhtml_attr get specific attribute\nhtml_name get tag name\nhtml_table HTML table structures can converted to\ndata.frame\n\n\nhtml_text(node)\nhtml_name(node)\nhtml_attr(node, 'class')\nhtml_table(node)\n\n\nCSS selector\nThe major difference between XPath and CSS is that CSS finds all of\nthe meeting condition.\nCSS is great for finding objects with id or\nclass.\nTo select a certain class, add .\nTo select a certain id, add #\n\n\n# Select the table elements\nhtml_nodes(test_xml, css = 'table')\n\n# Select elements with class = \"infobox\"\nhtml_nodes(test_xml, css = '.infobox')\n\n# Select elements with id = \"firstHeading\"\nhtml_nodes(test_xml, css = '#firstHeading')\n\n\nCUSTOM FUNCTIONS\nDescriptive Statistics\n\n\ncolms = c(\n  'Cust_vol','Proc_vol','MM_vol',\n  'MID_QUOTE','BA_SPREAD','PCT_SPREAD','EFF_SPREAD','PEF_SPREAD',\n  'IMPL_VOL','DELTA','GAMMA','THETA_E','VEGA_E'\n  )\n\nsummary_stat = qsu(CBOE, cols =  colms)\nsummary_stat = summary_stat %>% as.data.table(keep.rownames = T) %>% setnames(., old='rn', new='Variable')\n\n\nprobs = CBOE[,lapply(.SD, quantile, na.rm =T, probs = c(0.01,0.25,0.5,0.75,0.99)), .SDcols = colms ]\nprobs = transpose(probs) %>% as.data.table\n\nsummary_stat = cbind(summary_stat, probs)\nsummary_stat = summary_stat[,.(Variable, N,Mean, SD, P1 = V1,P25=V2,P50=V3,P75=V4,P99=V5)]\n\n\nadvanced : groupby summary\n\n\n#### Summary (10 min) of Spread by moneyness  ----\n\n# Summary of Spread and greeks by MONEYNESS\nnames(CBOE)\nvol_cols = c('PCT_SPREAD','PEF_SPREAD', 'ABS_DELTA','GAMMA','THETA_E','VEGA_E', 'MONEYNESS2')\n\n\ntemp = qsu(CBOE[,.SD,.SDcols = vol_cols], by = ~MONEYNESS2) %>% as.data.table\n\nprobs = CBOE[,lapply(.SD, quantile, na.rm =T, probs = c(0.01,0.25,0.5,0.75,0.99)), .SDcols = vol_cols[vol_cols !='MONEYNESS2'] , keyby=MONEYNESS2 ]\nprobs[, stat := rep(c('P1','P25','P50','P75','P99'),3)]\nprobs = melt(probs, id.vars = c('MONEYNESS2','stat'))\nsetnames(probs, new = c('V1','V2','V3','value'))\ntemp = rbind(temp,probs)\n\ntemp[, V3 := factor(V3, levels = vol_cols)]\ntemp = dcast(temp, V1 + V3 ~ V2 , value.var = 'value')\n\ntemp = na.omit(temp)\nsetcolorder(temp, c('V1','V3','N','Mean','SD','Min','Max'))\nsetnames(temp, old = c('V1','V3'), new = c('Moneyness','Variable'))\n\ntemp %>% select(-Min,-Max)\n\n\nSummary Statistics\nPackage collapse provides very handy and fast summary\nstat tool.\n\n\nsu = function(DT, quant = FALSE){\n  require(data.table)\n  require(tdigest)\n  setDT(DT)\n  \n  types = lapply(DT, class) %>% unlist\n  colnames = names(types)\n  \n  # include only numeric columns\n  \n  numeric_cols = colnames[sapply(types, function(x) x == 'numeric')]\n  \n  # N, mean, St.dev, Min, Median, Max\n  \n  N = DT[, lapply(.SD, function(x) sum(!is.na(x))), .SDcols =numeric_cols]\n  Mean = DT[, lapply(.SD, mean, na.rm = T), .SDcols =numeric_cols]\n  SD = DT[, lapply(.SD, sd, na.rm = T), .SDcols =numeric_cols]\n  Min = DT[, lapply(.SD, min, na.rm = T), .SDcols =numeric_cols]\n  Med = DT[, lapply(.SD, median, na.rm = T), .SDcols =numeric_cols]\n  Max = DT[, lapply(.SD, max, na.rm = T), .SDcols =numeric_cols]\n  \n  if (quant != TRUE){\n    result = as.data.table(t(rbindlist( list(N,Mean,SD,Min,Med,Max))), keep.rownames = T)\n    setnames(result, c('Variable', 'N','Mean','St.Dev','Min','Median','Max'))\n    return(result)\n  } else {\n    message('Working on Quantiles..')\n    for (i in seq_along(numeric_cols)){\n      td = tdigest(DT[, get(numeric_cols[i])])\n      quan = tquantile(td, c(0.1,0.25,0.75,0.9))\n      names(quan) = c('P10','P25','P75','P90')\n      \n    }\n  }\n  \n}\n\n\nWinsorize\nFrom package DescTools\n\n\nlibrary(DescTools)\n\n# my winsor function\nms_winsor = function(x, min=0.01, max = 0.99){\n  winsored_vec = Winsorize(x, minval = quantile(x, min, na.rm=T), maxval = quantile(x, max, na.rm=T), probs = c(min,max), na.rm = T)\n  return(winsored_vec)\n}\n\n\nStandardize\nMin-max normalization : scale into [0,1]\n\n\nstandardize <- function(x){(x-min(x))/(max(x)-min(x))}\n\n\nLinear interpolation\nUsage example : approxfun\n\n\nmonthly[,.(caldt, yearq, yearm, crsp_portno, avg_sim, test = approxfun(1:.N, avg_sim)(1:.N)), by=crsp_portno]\n\n\nFast Weighted Mean\nBy far, for weighted average collapse package gives the\nbest result. It gives fast groupby and fast collapsing(aggregating) with\nfmean\n\n\nmtcars %>%  fgroup_by(cyl,vs,am) %>%                 # Equivalent and faster !\n  fselect(mpg,hp) %>%  fmean(hp) # weight by hp\n\n\nMy example\n\n\n# weighted average of all variables fselected : weight by trade_size\nagg_sample = cboe_price %>% \n  fgroup_by(underlying_symbol,option_type, expiration, days_to_expire, strike, by10) %>% \n  fselect(trade_size, BA_SPREAD, MID_QUOTE, PCT_SPREAD, EFF_SPREAD, PEF_SPREAD) %>% \n  fmean(trade_size) %>% \n  fselect(-sum.trade_size)\n\n\n\n\n# Aggregation 2 : Selecting variables by regex expression\n\nagg_sample2 = vola_data %>% \n  fgroup_by(SYM_ROOT, option_type, DateTime) %>% \n  get_vars(\"^D_|GT\", regex=T) %>% \n  fsum() %>% \n  setDT\n\n\nRowwise Median\nPackage matrixStats is faster than\nrobustbase.\n\n\nlibrary(matrixStats)\ndata[, EndPrice := rowMedians(cbind(BID, ASK))]\n\n\nrowSums\n\ncboe_volume[1:100, rowSums(cbind(OPEN_INTEREST_MM, OPEN_INTEREST_MMBD))]\n\ncboe_volume[1:100, rowSums(cbind(.SD)), .SDcols = .SDcols = c('OPEN_INTEREST_MM', 'OPEN_INTEREST_MMBD')]\n\ncboe_volume[1:100, rowSums(as.matrix(.SD)), .SDcols = .SDcols = c('OPEN_INTEREST_MM', 'OPEN_INTEREST_MMBD')] # as.matrix is slightly faster\n\ncboe_volume[1:100, rowSums2(as.matrix(.SD)), .SDcols = .SDcols = c('OPEN_INTEREST_MM', 'OPEN_INTEREST_MMBD')] # rowSums2 is faster\n\ncboe_volume[1:100, do.call(rowSums, list( cbind(OPEN_INTEREST_MM, OPEN_INTEREST_MMBD) ))]\ncboe_volume[1:100, do.call(rowSums, list( cbind(.SD) )), \n            .SDcols = c('OPEN_INTEREST_MM', 'OPEN_INTEREST_MMBD') ]\n\nStandard Error of mean\n\n\nse_mean = function(x) {\n  sqrt( var(x, na.rm=T) / sum(!is.na(x)) )\n}\n\n\nCumsum ignoring NA\nSkip NA and leave NA as NA\n\n\nskipna_cumsum = function(x){\n  y = x\n  y[!is.na(x)] <- cumsum(y[!is.na(x)])\n  return(y)\n}\n\n\nMaking NA as 0\n\n\n# define cumsum that ignores NA\nna_cumsum = function(x){ # x is a vector\n  \n  miss <- is.na(x) # TRUE/FALSE placeholder\n  x[miss] <- 0 # convert NAs to 0\n  \n  cs <- cumsum(x)\n  cs[miss] <- NA # convert back to NA \n  return(cs)\n}\n\n\nEdgelist generator\n\n\ngen_edge = function(unique_vector){\n  d = data.table(V1 = unique_vector)\n  d[, `:=`(id1 = 1L, id2 = .I)]  ## add interval columns for overlaps\n  setkey(d, id1, id2)\n  olaps = foverlaps(d, d, type='within', which=T)[xid != yid]\n  ans = as.data.table(list(d$V1[olaps$xid], d$V1[olaps$yid]))\n  return(ans)\n}\n\n\n\n\nprep = function(dt){\n  setnames(dt, c('from','to'))\n  setorderv(dt, c('from','to'))\n  dt[,`:=`(unionN = integer(.N),\n           intersectN = integer(.N))]\n}\n\n\nFund flow generator\nUse this in the rolling regression setting with zoo::rollapply\n\n\n# generate monthly flow \nflow_generator_dt = function(DT){ \n  # Data table with two columns : first TNA, second RET\n  # needs to be applied in the zoo::rollapply function. \n  flow = (DT[2,1] - DT[1,1] * (1+DT[2,2])) / (DT[1,1])\n  return(flow)\n}\n\n\nLinear interpolation\n\n\n# linear interpolate fund age\nmonthly[, fund_age_port := approxfun(1:.N, fund_age_port)(1:.N), by=crsp_portno]\n\n\nDecile factorization\nUsing cut and quantile functions\n\n\ncut(monthly$avg_sim_imp, \n    breaks = quantile(monthly$avg_sim_imp,probs=c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1), na.rm=T),\n    labels = c('1st','2nd','3rd','4th','5th','6th','7th','8th','9th','10th')\n    )\n\n\nMultiple .SDs\n*Multiple .SDs\n\n\nd = as.data.table(iris)\nd[, \n  c(lapply(SD1, sum), lapply(SD2, mean)), \n  env = list(SD1 = as.list(c('Sepal.Length','Sepal.Width')), \n             SD2 = as.list(c('Petal.Length','Petal.Width'))),\n  by = Species]\n\nas.list(names(d))\n\n\nGForce Optimized with name set\n\n\nDT = as.data.table(iris)\n\ncols1 = c('Sepal.Length','Sepal.Width')\ncols2 = c('Petal.Length','Petal.Width')\n\nDT[, \n   .j, \n   env = list(.j = c(\n     lapply(lapply(setNames(nm=cols1), as.name), function(v) call(\"sum\", v)),\n     lapply(lapply(setNames(nm=cols2), as.name), function(v) call(\"mean\", v))\n   )),\n   by=Species\n] # GForce ON\n\n\n\n\ncols1 # vector\nsetNames(nm=cols1) # named vector that has name as its element\nlapply(setNames(nm=cols1), as.name) # named list that has name as its elemeent\n\na = lapply(setNames(nm=cols1), as.name)\n\na = as.list(cols1) # each element as separate list\nb = list(cols1) # one list with elements\n\n?as.name\nas.name(cols1)\nlapply(setNames(nm=cols1), as.name)\nsetNames\n?as.name\n\n\nTime Interval Grouping\nUsing lubridate::round_date\nMost clean for the purpose\n\n\nfloor_date(time, unit = \"5 min\") # round_date, ceiling_date\n\n\nUsing cut\ncut by breaks, but does not guarantee even (5 min cut could be 47,\n52, 57, 02, 07, 12, etc..)\n\n\ncut(dat$time, breaks=\"15 min\")\n\n\nVector Splitting\n\n\nspliter = function(x,n) split(x, cut(seq_along(x), n, labels = FALSE))\n\n\nForce Time zone\nlubridate::force_tz is a bit slower (ten times)\n\n\ndatetime=rep(as.POSIXct(\"2011-01-01 12:32:23.234\",tz=\"GMT\"),1e6)\nf <- function(x,tz) return(as.POSIXct(as.numeric(x), origin=as.POSIXct(\"1970-01-01\", tz=tz), tz=tz))\n\nlibrary(lubridate)\nsystem.time(datetime2 <- f(datetime,\"Europe/Paris\"))\nsystem.time(datetime3 <- force_tz(datetime,\"Europe/Paris\"))\nidentical(datetime2,datetime3)\n\n\nFast date/time conversion\nfasttime::fastPOSIXct with fixed = N (number of year digits)\n\n\nbzx[, trade_datetime := fastPOSIXct(trade_datetime, fixed=4, tz='UTC')]\nbzx[, trade_date := as.Date(trade_datetime)]\nbzx[, expiration_date := fastPOSIXct(expiration_date, fixed=4, tz='UTC') %>% as.Date]\n\n\nR Compile & Installation\nRenviron Setup\nR Environmental variables are set every time R session is\nlaunched.\nRstudio Error Error: vector memory exhausted (limit reached?)\necho \"R_MAX_VSIZE=1000Gb\" >> ~/.Renviron\necho \"R_DEFAULT_INTERNET_TIMEOUT=500\" >> ~/.Renviron\nWindows Powershell\nAdd-Content c:\\Users\\$env:USERNAME\\Documents\\.Renviron \"R_MAX_VSIZE=100Gb\"\nWindows CMD\necho R_MAX_VSIZE=500Gb >> %userprofile%\\Documents\\.Rinviron\nRprofile Setup\nRprofiles are run when R session begins. Usually when one needs to\nset different default options. It should be located at default “HOME”\nvariable path, which can be browsed by\nSys.getenv(\"HOME\")\nMac/Linux\nopen ~/.Rprofile\nWindows\nC:/Users/gunsu.son/Documents\n\n\noptions(reticulate.repl.quiet = TRUE) # disable reticulate message\nutils::memory.limit(1e9) # for windows machine\n\n\nRstudio Setup\nRStudio IDE options\nLike keybindings, dictionaries and other setups, it is saved in\nOn Mac\n~/.config/rstudio\nor\nOn Windows\nAppData/Roaming/RStudio\nView column limit\nIncrease column limit\n\n\nrstudioapi::writeRStudioPreference(\"data_viewer_max_columns\", 100L)\n\n\nR/RStudio removal on MacOS\nR, RStudio and miniconda removal\nOn Terminal,\nsudo rm -rf /Applications/R.app\nsudo rm -rf /Applications/RStudio.app\nsudo rm -rf /Library/Frameworks/R.framework\nsudo rm -rf /Library/Saved Application state/org.rstudio*\nsudo rm -rf ~/.config/rstudio\nsudo rm /usr/local/bin/{R,Rscript}\nsudo rm /private/var/db/receipts/org.R-project*\nsudo rm /private/var/db/receipts/org.r-project*\nsudo rm /private/var/db/receipts/org.rstudio*\nrm -rf ~/Library/Application Support/R\nrm -rf ~/.Renviron\n\nconda deactivate\nsudo rm -rf ~/Library/r-miniconda/\nrm -rf ~/.zshrc\nrm -rf ~/.bashrc\nRemove Rstudio config\nsudo rm -rf ~/.config/rstudio\nsudo rm -rf ~/.local/share/rstudio\nOpenMP(data.table, fst) for\nMacOS\nWiki\nR for MacOS doesn’t come with OpenMP support after R 4.0\nInstall Apple Xcode command line tools\nxcode-select –install\nInstall homebrew\n/bin/bash -c “$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)”\nInstall gcc, pkg-config\nbrew install gcc pkg-config\nCheck GCC version\nbrew info gcc\nGenerate Makevars\nmkdir ~/.R nano ~/.R/Makevars\nCopy & Paste\nLOC = /usr/local\n#/opt/homebrew for ARM Macs (M1 and its successors)\n#/usr/local for Intel Macs\n\nGCC_VER=11\nCC=$(LOC)/bin/gcc-$(GCC_VER) -fopenmp\nCXX=$(LOC)/bin/g++-$(GCC_VER) -fopenmp\nCXX11=$(LOC)/bin/g++-$(GCC_VER) -fopenmp # for fst package\n\nCFLAGS=-g -O3 -Wall -pedantic -std=gnu99 -mtune=native -pipe\nCXXFLAGS=-g -O3 -Wall -pedantic -std=c++11 -mtune=native -pipe\nLDFLAGS=-L$(LOC)/lib -Wl,-rpath, -I$(LOC)/lib\nCPPFLAGS=-I$(LOC)/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include\nAnd then install it from the type=‘source’ to install\n\n\ninstall.packages(\"data.table\", type = \"source\",\n    repos = \"https://Rdatatable.gitlab.io/data.table\")\ninstall.packages('fst', type = 'source')\n\n\nRQuantlib\nEither installing binary (intraday disabled) or install with brew\nbrew install boost\nbrew install quantlib\nDisable ~/.R/Makevars (especially with above gcc conflicts : complie\nwith mac default clang)\nThen install from source with configurations\n\n\ninstall.packages(\"RQuantLib\", configure.args = \"--with-boost-include=/usr/local/include/ --with-boost-lib=/usr/local/lib/\", type = \"source\")\n\n\nIt takes a while to compile them.\nThen ressurect Makevars file.\nRPostgres\nTemporary solution for error (now fixed)\n\n\ninstall.packages(\"RPostgres\", repos = \"https://r-dbi.r-universe.dev\")\n\n\nInstall Archived packages\nShould be compiled in the computer using archived package\ndirectory.\n\n\ninstall.packages(\"https://cran.r-project.org/src/contrib/Archive/lfe/lfe_2.8-5.1.tar.gz\", repos=NULL, type='source') \n\n\nFor ‘lfe’, gfortran was needed for compliling on Mac, which could be\ninstalled from R homepage tools section.\nRtools40 (Windows)\nStarting with R 4.0.0 (released April 2020), R for Windows uses a\ntoolchain bundle called rtools40. R 4.0 requires to install Rtools to\ninstall from source (C, fortran) for windows machines https://cran.rstudio.com/bin/windows/Rtools/\nIn .Renviron file located in the documents folder, paste\nPATH=\"${RTOOLS40_HOME}\\usr\\bin;${PATH}\"\nTo test if the installation & path is set, make\nshould work.\nSys.which(\"make\")\n## \"C:\\\\rtools40\\\\usr\\\\bin\\\\make.exe\"\nDelete all User installed\npackages\n\n\n# # create a list of all installed packages\n# ip <- as.data.frame(installed.packages())\n# head(ip)\n# # if you use MRO, make sure that no packages in this library will be removed\n# ip <- subset(ip, !grepl(\"MRO\", ip$LibPath))\n# # we don't want to remove base or recommended packages either\\\n# ip <- ip[!(ip[,\"Priority\"] %in% c(\"base\", \"recommended\")),]\n# # determine the library where the packages are installed\n# path.lib <- unique(ip$LibPath)\n# # create a vector with all the names of the packages you want to remove\n# pkgs.to.remove <- ip[,1]\n# head(pkgs.to.remove)\n# # remove the packages\n# sapply(pkgs.to.remove, remove.packages, lib = path.lib)\n\n\nRemove Miniconda completely\nNot r-reticulate miniconda\nrm -rf opt/miniconda3\nrm -rf ~/.condarc ~/.conda ~/.continuum\nArrow with snappy\ncompression\nBefore installing arrow run\n\n\nSys.setenv(ARROW_WITH_SNAPPY = \"ON\")\n\n\nThen\n\n\ninstall.packages('arrow')\n\n\nRETICULATE Python\nInstallation\nCheck default installation path :\n\n\nreticulate::miniconda_path()\n\n# Mac default installation path:\n# /Users/matthewson/Library/r-miniconda/\n\n# Windows default installation path :\n# \"C:/Users/gunsu.son/AppData/Local/r-miniconda\"\n\n\nMac\nsetup conda :\nsource /Users/matthewson/Library/r-miniconda/bin/activate\nInitialize conda - base will be activated all the time\nMac - zsh:\nrun conda init zsh\nlinux - bash:\nrun conda init bash\n.Renviron Setup\necho\n‘RETICULATE_PYTHON=“~/Library/r-miniconda/envs/r-reticulate/bin/python”’\n>> ~/.Renviron\nBrowse conda environments\nconda env list\nUse reticulate conda\nconda activate r-reticulate\nTo activate conda\nsource /Users/matthewson/Library/r-miniconda/bin/activate\nApple Sillicon\nDownload and install Miniforge3 for Apple silicon, default location\n~/miniforge3\nThen create a new environment, called r-reticulate with\npython installed in it.\nconda create -n r-reticulate python\nThen set environment variable for Rstudio:\necho 'RETICULATE_PYTHON=\"~/miniforge3/envs/r-reticulate/bin/python\"' >> ~/.Renviron\nWindows\nUse Anaconda Prompt, or\nMake Anaconda prompt from CMD\nCreate symlink (shortcut) file for cmd located at\n%windir%.exe\nand go properties and set target\n%windir%\\System32\\cmd.exe \"/K\" %userprofile%\\AppData\\Local\\r-miniconda\\Scripts\\activate.bat\nset Start in:\n%HOMEPATH%\nIn order it to be found in the start menu, paste it into\n%userprofile%\\AppData\\Roaming\\Microsoft\\Windows\\Start_Menu\\Programs\\Miniconda3\nConda setup\nEnvironments\nBrowse list of environments\nconda env list\nRemove environment\nconda env remove -n env_name\nChannel\nChange the priority channel\nconda config --prepend channels conda-forge\nGet channel names\nconda config --get channels\nR MARKDOWN\nBeamer EXAMPLES\n---\ntitle: \"FIN4453 : Financial Modeling\"\nsubtitle: \"Course Introduction\"\nauthor: \"Matthew Son\"\ndate: \"Week 1\"\noutput:\n  beamer_presentation:\n    includes:\n      in_header:\n        - ~/Dropbox (UFL)/Data Workshop/LaTeX/RMarkdown Templates/Mytheme.tex\n---\nHide Section slide\nhttps://stackoverflow.com/questions/38180441/dropping-frames-of-sections-and-subsections-titles-for-knitr-beamer-slides\nAdd this in YAML header.\nheader-includes: \n- \\AtBeginSubsection{}\n- \\AtBeginSection{}\nUse custom style\nUsing .sty file as external template for latex (beamer\nin this example)\noutput: \n  beamer_presentation:\n    includes:\n      in_header:\n        - Mytheme1.sty\nArticle example\nUse Custeom Pandoc Template\nBecause of adjustbox issue, this fixes the issue.\n---\ntitle: \"Testing\"\noutput: \n  pdf_document:\n    template: test.tex\ndate: '2022-02-16'\nheader-includes:\n  - \\usepackage{booktabs}\n  - \\usepackage{geometry}\n  - \\usepackage{tikz}\n  - \\usepackage{adjustbox}\n---\nThis is the code for test.tex (Gin is making issue with\nadjustbox)\nhttps://github.com/jgm/pandoc-templates/blob/master/default.latex\nComment out Gin part\nImport LaTeX packages\nThere are basically two ways to include external packages in YAML\nheader:\nUse extra_dependencies on pdf_document:\nUse header_includes and\n---\ntitle: \"Analysis\"\noutput: \n  pdf_document:\n    extra_dependencies: [\"float\"]\nheader-includes:\n  - \\usepackage{booktabs}\n  - \\usepackage{geometry}\n---\nPrevent floating\nIn order to prevent floating, package float must be\nimported\n---\noutput: \n  pdf_document:\n    extra_dependencies: [\"float\"]\n---\nThen make a default setup\n\n\nknitr::opts_chunk$set(fig.pos = \"!H\", out.extra = \"\")\n\n\nSubfigures\nIn header-includes: -usepackage{subfig} needs to be included\n\\begin{figure}[H]\n\\subfloat[Customer Volume\\label{fig:volume_trend-1}]{\\includegraphics[width=0.5\\linewidth]{plots/trend_customer} }\n\\subfloat[Professional Volume\\label{fig:volume_trend-2}]{\\includegraphics[width=0.5\\linewidth]{plots/trend_prof} }\n\\caption{Option Trade Volume Trend}\\label{fig:volume_trend}\n\\textit{Note:} The sample consists of all CBOE options trades on S\\&P500 stocks from October 2019 to March 2021. Figure on left display all customer option transaction volume by option types. Figure on the right is trend of professional customer volume by option types.\n\\end{figure}\nchunk options\necho=FALSE : no code chunk but output\n\n\n\nresults=‘hide’ : no text output results\n\n\nprint('text')\n\n\ninclude=FALSE : code and output not included, but\nexecuted\neval=FALSE : code chunk not executed but knitted\n\n\nprint('this chunk is not evaluated. It is good use to only show the code')\n\n\ncollapse=TRUE : image and text output are merged into a single\nblock.\nGood when warning message(text) is printed when the chunk is executed.\nUsually it is the case when plotting graphs.\nerror=TRUE : when error has to be reported.\nglobal chunk options\nYAML Header\nYAML example\n\n\ntitle: \"R CookBook\"\nauthor: \"Matthew Son\"\ndate: \"Last Updated : `r Sys.Date()`\"\noutput:\n  html_notebook:\n    toc: yes\n    toc_depth: 3\n    toc_float:\n      collapsed: false\n      smooth_scroll: yes\n\n\nparameter\nstyle\nStyle can be saved with .css file and be loaded at css\nDetailed options for stylish presentation\nbody : for entire body of the html\npre : for code chunks style\n#TOC : for table of contents box\n#header : for header\nh1.title\nh4.author\nh4.date\n\ncolor\nbackground-color\nfont-family\nfont-size\nopacity\nMarkdown\ntext\nbold\nitalic\nstrikethrough\nMessage in the Box : double tap\n\n\n- Double tap and hypen\nhyperlink\nThis is the hyperlink text\nembed image\nThis is a caption of the image put\nbelowLaTeX\nsubscript / superscript\nLimits\n\\[\\sum\\limits_1^N\\]\nCurly brackets up and down\nOverbrace and Underbrace\n\\[\\text{NPV} =\n\\overbrace{\\text{CF}_0}^{\\substack{\\text{Excel' NPV function} \\\\\n\\text{does not accont for this}}} +\n\\underbrace{\\sum_{t=1}^N\\frac{\\text{CF}_t}{(1+r)^t}}_{\\substack{\n\\text{This is what Excel's} \\\\ \\text{NPV function calculates}}\n}\\]\nKable (KableExtra)\nTable digits and formatting\nExample\n\n\nsummary %>% \n  kbl(digits = 2,\n      format.args = list(big.mark = \",\", \n                         scientific = FALSE)) %>% \n  kable_styling(latex_options = \"HOLD_position\")\n\n\n\n\nknitr::kable(iris %>% head, align = 'ccc', caption = 'This is caption')\n\n\nComparison : without kable\n\n\nprint(iris %>% head)\n\n\nkableExtra\nPackage that adds more features on kable.\n\n\nlibrary(kableExtra)\n\n\n\n\niris %>% \n  kbl() %>% \n  kable_classic_2() %>% \n  kable_paper() %>%\n  scroll_box(width = \"100%\", height = \"200px\")\n\n\nInternal links\nPandoc supports explicit and implicit section references for headers;\nsee the pandoc manual.\nexplicit: you give a custom name to a header ## Test {#test} and\nlater refer to it with a link syntax: see the relevant\nsection.\nimplicit: headers where you don’t set a custom name, like ##\nTest, can still be refered to: See the section called [Test].\nBASE R / RStudio\nR Function Tricks\nExpressions to string\n\n\ndeparse(substitute(iris))\ndeparse(quote(iris)) # similar but takes the function's argument instead\n\n\nassign to string object\nassign / .GlobalEnv\n\n\nx = 3\nassign('x', 3)\n.GlobalEnv$'x' = 3\n\n# .GlobalEnv gives accessibility \n\n.GlobalEnv$'x'[[2]] = 3 # automatically generate as list\n.GlobalEnv[['x']][[2]] = 3 # This one accepts functions\n\nvar = 'x'\n.GlobalEnv[[glue::glue('{var}')]][[2]] = 3\n\n\nstring evaluation\nTwo versions : evalparse and get\neval parse\n\n\neval(parse(text='iris'))\n\n\nget / mget\nget\nA string to point object in environment\n\n\n#example\na = 'Hello, world!'\nprint('a') # this is a string\nprint(get('a')) # get('a') == a\nprint(a)\n\n\nmget\nConvert a vector of string object names into list of objects\n\n\n# example\na = c(1,2,3)\nb = c(3,4,5)\nobj_names = c('a','b')\nmget(obj_names)\n\n\nOptions\n.Rprofile\n.Rprofle needs to be setup.\nOn Mac\necho \"options(datatable.prine.class =T)\" >> ~/.Rprofile\nOn Windows\necho options(reticulate.repl.quiet = T) > %userprofile%\\Documents\\.Rprofile\ndata.table options\nPrint options with data.table\n\n\n# change default options\noptions(datatable.print.nrows = 50)\noptions(datatable.print.topn = 10)\noptions(datatable.print.class= T)\n\n# change locally\nprint(DT, topn = 20)\n\n\nstr options\nPrint options with str function - list.len\n\n\noptions(str = strOptions(list.len = 1e5))\n\n\nReticulate options\n\n\noptions(reticulate.repl.quiet = T)\n\n\ndata.frame\ntranspose data.frame\nWhen data.frame has row names and column names\n\n\niris_transpose <- as.data.frame(t(as.matrix(iris_df)))\n\n\nshQuote\nAdding quotes to each characters\n\n\nshQuote(c('AAPL','AA'))\n# [1] \"'AAPL'\" \"'AA'\"\n\n\nouter\nOuter product of two arrays\n\n\nmonths= str_pad(9:12, 2, pad ='0')\ndays = str_pad(1:31,2,pad='0')\nmd = c(outer(months, days, FUN = str_c))\n\n\nlist.files\nMaking the list of file names\n\n\nlibrary(readxl)\nfile.list <- list.files(pattern='*.xlsx')\ndf.list <- lapply(file.list, read_excel)\n\n\nstr\nPrint all of the data.frame (data.table)\n\n\n# print more\nstr(df, list.len=ncol(df))\n\n\ncut\nCut and make groups. Useful for time cutting.\nMy usage example\n\n\ntest2[,by10 := fastPOSIXct(cut(quote_datetime, breaks='10 min'), tz='UTC') + 600]\n\n\nfactor\nUnordered\nlabeling makes the column as if character vector\n\n\nmtcars$fam <- factor(mtcars$am, labels = c(`0` = 'automatic',\n                                           `1` = 'manual'))\n\n\nOrdered\n: use level and label both\n\n\nmtcars$fam <- factor(mtcars$am,\n                     levels = c(1, 0),\n                     labels = c(\"manual\", \"automatic\"))\n\n\nError Handling\nexists()\nStop and error when condition is met. Object name is given in character.\n\n\nsimplefunc = function(x){\n  exists(x)\n  print('Success')\n}\nsimplefunc('iris')\n\n\nstop()\nstopifnot()\ntry()\nWhen error has to be skipped in the loop, use try\n\n\n# lapplying log function, error on the last\na = list(1,2,3,'oh',4)\n# lapply doesn't report the interim output because of error\nlapply(a, log)\n\n\n\n\nk = lapply(a, function(x) try(log(x))) # it does work, and shows error\nclass(k[[4]]) # try-error\n\n\nWhen ran in data.table, errors are recorded with NA\n\n\nlibrary(data.table)\nirist = as.data.table(iris)\nirist[3, a := 5]\nirist\nfor (i in 2:nrow(irist)){\n  irist[i, a := try(sqrt(Species), silent = TRUE)] # data.table coerces to NA for numeric column\n}\n\n\ntryCatch()\nMore on error handling : what should do when it generates error?\nIt should be given in functional form. Give anonymous function for\nsimple operations.\n\n\ntryCatch(sqrt('a'),\n         error = function (x) print('You silly, put number instead')\n         )\n?tryCatch\n\n\nRStudioapi\nrun code on terminal\nRun code on the terminal from R script\n\n\nrstudioapi::terminalExecute('ls')\n\n\nincrease View column limit\n\n\nrstudioapi::writeRStudioPreference(\"data_viewer_max_columns\", 1000L)\n\n\nBENCHMARKING\nsystem configurations\nFor bug reporting, etc.\n\n\nsessionInfo()\n\n\nbenchmarkme package : checking out the specification of\nthe local.\n\n\n# Load the package\nlibrary(benchmarkme)\n# Assign the variable ram to the amount of RAM on this machine\nget_ram()\n# Assign the variable cpu to the cpu specs\nget_cpu()\n# Load the parallel package\nlibrary(parallel)\n# Store the number of cores in the object no_of_cores\ndetectCores() # number of cores\n?detectCores\n\n\nmicrobenchmark package : for efficiency and speed\ncomparision\n\n\nlibrary(microbenchmark)\nmicrobenchmark(\n  print('ak'),\n  print('abcdefg'),\n  times=100L\n)\n\n\nbenchmarking time\nUsing Sys.time()\n\n\ntic = Sys.time()\nprint('haha')\ntoc = Sys.time()\ntoc - tic\n\n\nUsing system.time()\n\n\nsystem.time(\n  print('hahaha')\n)\n\n\nPARALLEL PROCESSING\nBasic introduction :\nparallel, foreach,\nfuture.apply packages uses master / worker.\nThey are good for single machine and medium size data.\nsparklyr utilizes Spark, which uses Map-Reduce.\nDISK.FRAME\ndisk.frame is an future object that connects to a folder that\ncontains multiple .fst files. Those multiple fst files are chunks from\nthe original file.\n\n\niris.df = as.disk.frame(iris) # without path specification, it saves to temp foler\niris.df # nchunks 8\n\n\nSetup\n\n\nsetup_disk.frame(workers = 8)\noptions(future.globals.maxSize = Inf,  # Unlimited data communication\n        future.rng.onMisuse = 'ignore') # Ignore random seeds\n\n\nBasic operations\nRead disk.frame\ndisk.frame()\nLinking the file folder path as disk.frame. It expects .fst files\nwithin the folder. Folder itself is the disk.frame.\n\n\ndisk.frame(path = 'folderpath')\n\n\ncsv_to_disk_frame()\nWhen csv is to large to read, read in chunks\n\n\nflights.df <- csv_to_disk.frame(\n  csv_path, \n  outdir = df_path, \n  in_chunk_size = 1e7, # chunk size by number of rows\n  nchunks = 8, # number of chunks\n  .progress = TRUE,\n  select = c('colnames') # passed onto fread\n  )\n\n\nJIT transformation\nThis approach is faster than reading csv and transform and save.\n\n\ndf = csv_to_disk.frame('filepath', \n                       inmapfn = function(chunk) {\n                         # convert to date_str to date format and store as \"date\"\n                         chunk[, date := as.Date(date_str, \"%Y-%m-%d\")]\n                         chunk[, date_str:=NULL]\n                       })\n\n\nzip_to_disk.frame()\nzip file to directly read to disk.frame. Arguments are same as\ncsv_to_disk.frame\n\n\nzip_to_disk.frame()\n\n\nWrite disk.frame\nwrite_disk.frame()\n\n\nwrite_disk.frame() # useful for rechunking as well!\n\n\nPrompt evaluation\nCurrenlty, data.table opertaions are evaluated promptly, for each\nchunk of .fst files.\n\n\niris.df[] # collect all, warning!!\niris.df[1] # 1st row of all chunks\niris.df[1:5] # first 5 rows of all chunks\n\n\nLazy evaluation\nManipulating disk.frame with lazy evaluation\nUse evaluation with cmap Warning : since it is not in\nmemory, can’t be overwritten on the same disk.frame! When\ncmap is called without object name definition, it is\nevaluated promptly.\n\n\n# prompt function performance & write disk.frame\ncmap(iris.df, function(chunk){chunk[,newval2:= 1]},\n     # keep = c('col1','col2'),\n     outdir = 'df_name',\n     lazy=FALSE, # prompt writing\n     overwrite = T,\n     compress = 100)\n\n\nTo do lazily, save into an object and use with\nwrite_disk.frame.\nWarning : since it is not in memory, can’t be overwritten on the same\ndisk.frame!\n\n\nk = cmap(iris.df, function(chunk){chunk[,newval2 := 2]})\nwrite_disk.frame(k,\n                 outdir = 'df_name2', # shoulf be different folder than original disk frame\n                 compress=100,\n                 overwrite=TRUE\n                 )\n\n\nAccess chunks\nget_chunk()\nGet chunk with integer id number. Does not guarantee the integer is\nin sequence when hased (shardby) !!\n\n\nget_chunk(iris.df, 1) # needs \n\n\nget_chunk_ids()\n\n\nget_chunk_ids(iris.df)\n\n\nRechunk\nTwo approaches : using write_disk.frame and provide\nexplicit nchunks / use rechunk function\n`write_disk.frame\nExamples\nshardkey\nFUTURE\n\n\nlibrary(future)\nplan(multisession) \nplan(multicore) # forking, no RStudio, no windows\n\n\nOptions\n\n\noptions(future.gc=TRUE) # garbage collection\noptions(future.globals.maxSize = Inf)\n\n\nBasics of Future\nBasics : when future is called, the process is done behind the scene\n- opens up a new session and do the job then returns the value to the\noriginal session.\n\n\n# a slow process that takes 20 secs\nslow_sum = function(x){\n  Sys.sleep(length(x))\n  sum(x)\n}\nslow_sum(1:20) # has to wait 20 secs to get the answer!\n\n\nNow use future to run it behind the scene.\n\n\nfa = future(slow_sum(1:10)) # does job behind the scene, 10sec\nfb = future(slow_sum(11:20)) # does job behind the scene, 10 sec\n\n\nTo take out the calculated value, call value(). If it is\ncalled too early, then it waits until the future is complete!\n\n\na = value(fa) # when called within 10 sec, it waits!\nb = value(fb)\na + b\n\n\nIn order to do it in one step,\n\n\na %<-% {slow_sum(1:10)} # future and store value when resolved\na # waits if it is not resolved\n\n\nTo know if the future was resolved,\n\n\nresolved(fa)\n\n\nGarbage collection\nCan be set in the beginning\n\n\nplan(cluster, workers = cl, gc = TRUE)\n\n\nIf garbage collection needed after value was retrieved;\n\n\nx <- future({ expr }, gc=TRUE)\n\n\nFuture with data.table\n\n\nlibrary(data.table)\ndt_iris = as.data.table(iris)\ndt_iris\nf = future(dt_iris[, mean(Sepal.Length)], packages='data.table')\nv = value(f)\n\n\nExport globals\nFrom future documentation :\nIn most cases, such automatic collection of globals is sufficient and\nless tedious and error prone than if they are manually specified.\nHowever, for full control, it is also possible to explicitly specify\nexactly which the globals are by providing their names as a character\nvector. In the above example, we could use\n\n\na <- 42\nf <- future({ b <- 2; a * b }, globals = \"a\")\nf <- future({ b <- 2; a * b }, globals = list(a = a)) # same\n\n\nwe can disable the automatic search for globals by using\n\n\nf <- future({ a <- 42; b <- 2; a * b }, globals = FALSE)\n\n\n\n\na <- 42\nb <- 2\nf <- future({ a * b }, globals = FALSE)\nf2 <- future({a*b}, globals = 'a') # only a\nv = value(f) # varible a not found!\nv = value(f2) # b not found\n\n\nExample : dt_iris and “a” defined in the global environment, but I’m\nonly exporting subset of globals\n\n\ndt_iris = as.data.table(iris)\na = 30\nf = future({print(dt_iris); print(a); iris[Sepal.Length == 5.1]}, globals= list(iris = dt_iris[Species=='setosa']), package='data.table')\nv = value(f)  # prints first error message, and does not do the job afterwards! \nv\n\n\nErrors in the future\nExample : dt_iris and “a” defined in the global environment, but I’m\nonly exporting subset of globals\n\n\nf = future({message('working');print(dt_iris); print(a); iris[Sepal.Length == 5.1]; message('work done here')}, globals= list(iris = dt_iris[Species=='setosa']), package='data.table')\nv = value(f) # prints first error message, and does not do the job afterwards! \nv # no object assigned\n\n\nI can igore the error and just proceed with tryCatch with empty\nfunction error = function() {NULL}\n\n\nf = future(\n  tryCatch(\n    {message('working');print(dt_iris); print(a); dt_iris[Sepal.Length == 5.1]; message('work done here') },\n    error = function() {NULL}\n  ),\n  globals= list(iris = dt_iris[Species=='setosa']), \n  package='data.table')\nv=value(f)\nv\n\n\n\n\ntryCatch(\n    {message('working');print(dt_iris); print(a); dt_iris[Sepal.Length == 5.1]; message('work done here') },\n    error = function() {NULL}\n  )\n\n\nSet workers (cores) for each\nprocess\nMultisession + Sequential\n\n\navailableCores()\navailableWorkers()\n\nplan(list(\n  tweak(multisession, workers = 4), # 4 R sessions\n  tweak(multisession, workers = 2)) # nested future use 2 separate sessions\n)\nplan(multisession)\n\n\nH2O example for this\n\n\nlibrary(h2o)\nlibrary(future)\n\nplan(tweak(multisession, workers = 2)) # only two R processes\n\n# initiate multiple h2o clusters and match them to separate R processes\njar_path = system.file('java', 'h2o.jar', package='h2o')\n\nrequire(stringr)\nsystem(str_glue('java -Xmx4g -jar {jar_path} -name n1 -nthreads 4  -port 54321'), intern = F, wait =F)\nsystem(str_glue('java -Xmx4g -jar {jar_path} -name n2 -nthreads 4  -port 54322'), intern = F, wait =F)\n\n\nf1 = future({\n  # h2o.shutdown(prompt = F)\n  h2o.init(startH2O = F, port = 54321)\n  Sys.getpid()\n}, lazy =T)\nf2 = future({\n  # h2o.shutdown(prompt = F)\n  h2o.init(startH2O = F, port = 54322)\n  Sys.getpid()\n}, lazy =T)\n\n\nFuture + H2O\nSplit data by ticker, about equal size\n\n\nmy_tickers = alldays[,unique(TICKER)][1:20] # first 20 tickers for example\nnum_tickers = seq_along(my_tickers)\neach = 10\nchunks = split(my_tickers, ceiling(num_tickers/each))\nchunks %>% class #list\n\n\n\n\nplan(list(tweak(multisession, workers = 2), tweak(multisession, workers = 4)))\n\n\nSimple example\n\n\nplan(list(\n  tweak(multisession, workers = availableCores() %/% 4),\n  tweak(multisession, workers = 4)\n))\n\n\n\n\nv <- {\n  cat(\"Hello world!\\n\")\n  3.14\n}\nv\n\n\n\n\nlibrary(future)\nv %<-% {\n  cat('hello\\n')\n  3.14\n}\nv # evaluates when it is called!\n\nplan(multisession)\nfuture::availableWorkers()\nfuture::availableCores()\n\nv %<-% {\n  cat('hello\\n')\n  3.14\n}\nv # evaluates when it is called!\n\n\nExplicit future\n\n\nf = future({\n  cat('hello\\n')\n  3.14\n})\nv = value(f)\n\n\n\n\nplan(sequential)\navailableWorkers()\na = 1\nx %<-% {\n  a = 2\n  2 * a\n}\nx \na\n\n\n\n\nplan(sequential)\npid = Sys.getpid()\npid #1] 24919\n\na %<-% {\n  pid = Sys.getpid()\n  cat('Future \"a\" ...\\n')\n  3.14\n}\nb %<-% {\n  rm(pid)\n  cat('Future \"b\" ...\\n')\n  pid\n}\nc %<-% {\n  cat('Future \"c\" ...\\n')\n  2 * a\n}\n\n\n\n\nplan(multisession)\npid = Sys.getpid()\npid #1] 24919\na %<-% {\n  pid = Sys.getpid()\n  cat('Future \"a\" ...\\n')\n  3.14\n}\nb %<-% {\n  rm(pid)\n  cat('Future \"b\" ...\\n')\n  Sys.getpid()\n}\nc %<-% {\n  cat('Future \"c\" ...\\n')\n  2 * a\n}\nb\npid\nc\n\n\nNested future\n\n\nplan(list(sequential,multisession))\npid = Sys.getpid()\na %<-% {\n  cat('Future \"a\" ...\\n')\n  Sys.getpid()\n}\n\nb %<-% {\n  cat('Future \"b\" ...\\n')\n  b1 %<-% {\n    cat('Future \"b1\" ...\\n')\n    Sys.getpid()\n  }\n  b2 %<-% {\n    cat('Future \"b2\" ...\\n')\n    Sys.getpid()\n  }\n  c(b.pid = Sys.getpid(), b1.pid =b1, b2.pid = b2)\n}\n\npid\n\na # [1] 25887 \nb # 25943  25943  25943 \n\n\n\n\nlibrary(future)\nlibrary(listenv)\nx = listenv()\n\nfor (i in 1:3){\n  x[[i]] %<-% {\n    y = listenv()\n    for (j in 1:3){\n      y[[j]] %<-% { i + j /10}\n    }\n    y\n  }\n}\n\nunlist(x)\n\n\nExamples\nExport small global object\n\n\nf1 = future(\n  {\n    message(\"Working on \", \"firm_open_buy_qty\");\n    slim_cboe\n  }, \n  globals = list(slim_cboe = cboe[,.SD, .SDcols = c(key_cols, \"firm_open_buy_qty\")]),\n  packages = c('data.table')\n)\nvalue(f1) # OK\n\n\nFuture with Function wrap\n\n\ntest_func = function(colname, whole_data){\n  f = future(\n    {\n      tic();\n      slim_cboe[, test := frollapply(get(colname), n=2, FUN = function(x) {x[2]-x[1]})]\n      toc();\n      slim_cboe[,test]\n    }, \n    globals = list(slim_cboe = whole_data[,.SD, .SDcols = c(key_cols, colname)],\n                   colname = colname), # colname needs to be exported again\n    packages = c('data.table', 'tictoc')\n  )\n  return(f)\n}\n\nhmm = test_func('firm_open_buy_qty', cboe)\nresolved(hmm) #OK!\n\n\ndoFuture\n\n\nlibrary(doFuture)\noptions(future.globals.maxSize = Inf)\noptions(future.rng.onMisuse = \"ignore\") # ignore RNG warning\nregisterDoFuture()\nplan(multisession)\n\ngetDoParWorkers()\n\nlibrary(progressr)\nhandlers(global = TRUE)\nhandlers(\"progress\")\n\n\nBasics\nExporting objects\nWhen it needs to be manual, it can be turned off with\n\n\noptions(doFuture.foreach.export = \".export\")\n\n\nNote that when using doFuture, arguments .packages and\n.export in foreach() are not necessary, as the package\ndeals with the exports automatically.\n\n\nlibrary(doFuture)\nregisterDoFuture()\nplan(multisession, workers =4)\ngetDoParWorkers()\n\nforeach(n = c(3,4,2,5,1,5,6,2,6)) %dopar% {\n  print('Sleeping 1 sec...')\n  Sys.sleep(1)\n  rnorm(n)\n}\n\n\ndoFuture.foreach.export: Specifies to what extent the .export\nargument of foreach() should be respected or if globals should be\nautomatically identified.\nIf “.export”, then the globals specified by the .export argument will\nbe used “as is”.\nIf “.export-and-automatic”, then globals specified by .export as well\nas those automatically identified are used.\nThe “.export-and-automatic-with-warning” is the same as\n“.export-and-automatic”, but produces a warning if .export lacks some of\nthe globals that the automatic identification locates\nthis is helpful feedback to developers using foreach().\nParallel data.table\nFor data.table to be applied in parallel, all DTs should be splitted\nand saved into a list. split function is a possible use, or\nusing disk.frame to split and read.fst is a way.\n\n\nforeach(dt = splited_dt, .packages='data.table') %dopar% {\n    dt[, str_replace(DATE, '(\\\\d{4})(\\\\d{2})(\\\\d{2})', '\\\\1-\\\\2-\\\\3')]\n  }\n\n\nfread in chunks\n\n\ndata=NULL\nchunk.size = 1e8\nfor (i in 0:20){\n    data[[i+1]] = fread(\"my_data.csv\", nrow = chunk.size, skip = chunk.size*i )\n}\n\n\nGlobal assignment\nhttps://github.com/HenrikBengtsson/doFuture/issues/29\n\n\nregisterDoFuture()\nplan(multisession)\nx <- data.frame(a=1:5,b=NA,c=NA)\nforeach(i = 1:nrow(x)) %dopar% {\n  x$b[i] <- runif(1)\n  x$c[i] <- runif(1)\n}\nx # doesn't work!\n\n\nThe reason for this not working is that the parallel code (what’s\ninside the foreach { … } expression) is evaluated in a separate R\nprocess that the main R session from where it is called. It is not\npossible for other R processes on your machine to update the variables\nin your main R session. An analogue is when you run two separate R\nsessions manually and you do x <- 42 in one of them - then you\nwouldn’t expected x to be assigned in the other R session. That’s how\nparallel processing in R works too. Instead, all values must be\n“returned” at the end of a parallel evaluation.\nInstead, treat foreach() as you treat lapply(). If you do, then\nyou’re example is effectively equal to:\n\n\nlapply(1:nrow(x), FUN = function(i) {\n  x$b[i] <- runif(1)\n  x$c[i] <- runif(1)\n}\n)\n\n\nInstead, you should always “return” values. That is, make sure all of\nyour foreach() calls are assigned to a variable, cf.  y <- lapply(…).\nSo, in your case you can to do something like:\n\n\nx <- data.frame(a=1:5,b=NA,c=NA)\ny <- foreach(i = 1:nrow(x), .combine = rbind) %dopar% {\n  data.frame(b = runif(1), c = runif(1))\n}\nx$b <- y$b\nx$c <- y$c\n\n\nIn the ideal case, all it takes to start using futures in R is to\nreplace select standard assignments (<-) in your R code with future\nassignments (%<-%) and make sure the right-hand side (RHS)\nexpressions are within curly brackets ({ … }). Also, if you assign these\nto lists (e.g. in a for loop), you need to use a list environment\n(listenv) instead of a plain list.\nfuture.apply package\n\n\nlibrary(future)\nlibrary(future.apply)\n\n\nplan\nHow to plan future :\nsequential : sequentially, not parallel\nmultisession : PSOCK (both MAC/LINUX + WINDOWS)\n2-1. multicore : resolve futures asyncronously in separate FORKED R\nsessions. (UNSTABLE with Rstudio & MAC/LINUX only)\ncluster : resolve futures asyncronously in separate R sessions -\none or more machines.\nremote : separate machine - in different network\n\n\nplan(multisession)\n## Explicitly close multisession workers by switching plan\nplan(sequential)\n\n\navailableCores\n\n\navailableCores()\n\n\nfuture_lapply\n\n\nplan(sequential)\nfuture_lapply(1:10, rnorm)\n\n\nreproducibility\nput future.seed argument for reproducibility.\n\n\nplan(sequential)\nres1= future_lapply(1:5, rnorm, future.seed = 1234)\nplan(multiprocess)\nres2= future_lapply(1:5, rnorm, future.seed = 1234)\nidentical(res1,res2)\n\n\nparallel package\n\n\nlibrary(parallel)\n\n\nbase R : parallel package.\nWith this package, users can use current R session as the master\nprocess, while each worker is a separate R process.\nTo detect number of cores (hyperthreaded) and make a cluster of nodes\n:\n\n\ndetectCores()\ndetectCores(logical=FALSE) # physical cores only, no hyperthreading\nncores = detectCores()\n\n\nMake clusters & stop clusters\n\n\n# start a cluster\ncl <- makeCluster(ncores) # 'PSOCK' is the default\n\n# cl = makeForkCluster(ncores)\n# cl = makePSOCKcluster(ncores)\n\n# stop connection to cluster\nstopCluster(cl)\n\n\nSocket vs Fork\nIn a fork cluster, each worker is a copy of the master process,\nwhereas socket workers start with an empty environment.\n\n\n# A global variable and is defined\na_global_var <- \"before\"\n\n# Create a socket cluster with 2 nodes\ncl_sock <- makePSOCKcluster(2) # empty environment\n\n# Evaluate the print function on each node\nclusterCall(cl_sock, print, a_global_var) # a_global_var is not defined\nstopCluster(cl_sock)\nstopCluster(cl)\n\n\nWhat is the process id for each cluster?\n\n\nSys.getpid() # gives current R process id\n\nclusterCall(cl, Sys.getpid) # clusters has different, independent R process\n\n\nclusterApply()\n\n\n# single node processing : lapply\nlapply(ncores:1, rnorm)\n\n# parallel processing : clusterApply\nclusterApply(cl, x=ncores:1 , fun = rnorm, sd=1:100) # x : how many times function is called\n\n# x = c(8,7,6,5,4,3,2,1)\n# rnorm(8) for first worker\n# rnorm(7) for second worker\n# ...\n# rnorm(1) for eighth worker\n\n\nExample : more tasks than worker\n\n\nmean_of_rnorm = function(n){\n  random_numbers <- rnorm(n)\n  mean(random_numbers)\n}\nn_replicates <- 50\nn_numbers_per_replicate <- 10000\nx = rep(n_numbers_per_replicate, n_replicates)\nx # 10000 10000 ... 10000\n\n# Parallel evaluation on n_numbers_per_replicate, n_replicates times\nmeans <- clusterApply(cl, \n                      x = x, \n                      fun = mean_of_rnorm)\nunlist(means)\n\n\nload balancing\nInstead of waiting for the results from nodes in specific order, it\naccepts results from the first worker that finished and immediately send\nback new task.\nImproves speed if the task is imbalanced, but for small tasks\noverhead costs outweighs speed gains.\n\n\nset.seed(1)\ntasktime = sqrt(abs(rnorm(16)))\ncl = makeCluster(8)\nlibrary(future.apply)\nplan(multisession)\n\nmicrobenchmark::microbenchmark(\n  clusterApply(cl, tasktime, Sys.sleep),\n  clusterApplyLB(cl, tasktime, Sys.sleep),\n  future_lapply(tasktime, Sys.sleep), # does automatic load balancing\n  times=3\n)\n\n\nPlotting the job\n\n\nplot_cl_apply = function(cl, x, fun) \n    plot(snow::snow.time(snow::clusterApply(cl, x, fun)),\n            title = \"Cluster usage of clusterApply\")\nplot_cl_applyLB = function(cl, x, fun) \n    plot(snow::snow.time(snow::clusterApplyLB(cl, x, fun)),\n            title = \"Cluster usage of clusterApplyLB\")\n\nplot_cl_apply(cl, tasktime, Sys.sleep)\nplot_cl_applyLB(cl, tasktime, Sys.sleep)\n\n\nclusterEvalQ()\nEvaluate the literal expressions on all workers (nodes).\n\n\ncl = makeCluster(2)\nclusterEvalQ(cl, {\n  library(tidyverse)\n})\nclusterCall(cl, function() iris %>% head) # pipe operator works, because tidyverse is imported\n\n\nclusterExport()\nExports objects from master to all workers (nodes).\n\n\nthis <- 'hello'\nclusterEvalQ(cl,{\n  print(this)\n}) # not found\n\n\n\n\nclusterExport(cl, 'this')\nclusterEvalQ(cl,{\n  print(this)\n}) \n\n\nparApply()\nWrapper for clusterApply().\nparLapply(), parSapply() andparLapplyLB(), parSapplyLB() for load\nbalancing,\nMac/Linux has shared memory system, while windows does not.\nFor both Mac / Linux and Windows.\n\n\nparLapply(cl, c(1,1), rnorm) \nclusterApply(cl, c(1,1), rnorm) # same, output is list\n\nparSapply(cl, c(1,1), rnorm)\n\n\nFunctions : cluster’s do not know custom functions without explicit\nexport.\nUse clusterExport()\n\n\n# If function is defined, then it should be exported to cluster to be used.\nplay <- function() {\n    total <- no_of_rolls <- 0\n    while(total < 10) {\n      total <- total + sample(1:6, 1)\n  \n      # If even. Reset to 0\n      if(total %% 2 == 0) total <- 0 \n      no_of_rolls <- no_of_rolls + 1\n    }\n    no_of_rolls\n  }\n\n# Export the play() function to the cluster\nclusterExport(cl, \"play\")\n\n# Re-write sapply as parSapply\nres <- parSapply(cl, 1:100, function(i) play())\n\n\nprogress bar\n\n\n# parallel appraoch\nlibrary(parallel)\nlibrary(pbapply)\nmy_cl = makeCluster(8)\n\nsplit_max = function(col){\n  split = strsplit(col,'|',fixed=T)\n  int_split = lapply(split, as.integer)\n  maxed_col = suppressWarnings(sapply(int_split, max, na.rm=T))\n  maxed_col[is.infinite(maxed_col)] = -1\n  return(maxed_col)\n}\n\n\npaste0(names(sdc)[256:ncol(sdc)],'_MAX')\n\n\nlapply(strsplit(a$q, '|', fixed=T),as.integer) %>% sapply(max, na.rm=T)\nsdc[,S_P_I_LT] %>% head(50) %>% strsplit('|',fixed=T) %>% lapply(max)\n\n\nProgress bar\nDoFuture Foreach progressbar\n\n\nlibrary(doFuture)\nregisterDoFuture()\nplan(multisession)\n\nlibrary(progressr)\nhandlers(global = TRUE)\nhandlers(\"progress\")\n\n# Example\nmy_fcn <- function(xs) {\n  p <- progressor(along = xs)\n  y <- foreach(x = xs) %dopar% {\n    Sys.sleep(6.0-x)\n    p(sprintf(\"x=%g\", x))\n    sqrt(x)\n  }\n}\n\nmy_fcn(1:5)\n\n\nMy usage\n\n\nmy_fcn = function(split_dt){\n  p = progressor(along = split_dt)\n  result_dtlist = foreach(x = split_dt, j = seq_along(split_dt)) %dopar% {\n    p(sprintf(\"x=%g\", j))\n    x[, mflow_fundno := c(NA,rollapply(x[,.(mtna,mret)],2, flow_generator_dt, by.column=F))]\n  }\n  return(rbindlist(result_dtlist))\n}\n\nmon_ret_flow = my_fcn(seq_along(split_ret),split_ret)\n\n\nUsage 2\n\n\nmy_fcn <- function(filelist) {\n  p <- progressor(along = filelist)\n  list_df <- foreach(x = seq_along(filelist)) %dopar% {\n    p(sprintf(\"x=%g\", x))\n    fread(cmd = filelist[[x]])\n  }\n  return(list_df)\n}\n\na = my_fcn(bzx_files[1:20])\n\n\nfor loop progress bar\nprogress package\nUse for for loop or foreach loop for\nparallel\n\n\nlibrary(progress)\n\n# basic\n\npb <- progress_bar$new(total = 100) \nfor (i in 1:100) {\npb$tick()\nSys.sleep(1 / 100) }\n\n\n\n\n# percent and eta\npb <- progress_bar$new(\n      format = \"  downloading [:bar] :percent eta: :eta\",\n      total = 100, clear = FALSE, width= 60)\nfor (i in 1:100) {\npb$tick()\nSys.sleep(1 / 100) }\n\n\n\n\n# Elapsed time\npb <- progress_bar$new(\n  format = \"  downloading [:bar] :percent in :elapsed\",\n  total = 100, clear = FALSE, width= 60)\nfor (i in 1:100) {\npb$tick()\nSys.sleep(1 / 100) }\n\n\napply progressbar\npbapply package\npbLapply, pbSapply are often used\n\n\nlibrary(pbapply)\n\n\nData I/O\ngetwd() to get the current working directoryfile.path('~','data') to make a path in programmatic form\n(os independent)\nList of files in directory\nlist.files()\n\n\nlist.files()\n\n\nRead flat files\nbaseR utils package : read.csv,\nread.delim, read.table(ultimate)\nTidyverse readr package : read_csv,\nread_tsv, read_delim(ultimate)\ndata.table package : fread\n\n\n# read some cols only\n# Column names\nproperties <- c(\"area\", \"temp\", \"size\", \"storage\", \"method\",\n                \"texture\", \"flavor\", \"moistness\")\n\npotatoes <- read_tsv(\"potatoes.txt\", col_names = properties)\n\n# Import all data, but force all columns to be character: potatoes_char\npotatoes_char <- read_tsv(\"potatoes.txt\", col_types = \"cccccccc\", col_names = properties)\n\n\nRead excel files\nTidyverse readxl package\nexcel_sheets : names of sheets in the fileread_excel : read the sheets\n\n\n# To read all excel sheets with in a file\npop_list <- lapply(excel_sheets(\"urbanpop.xlsx\"), read_excel, path = \"urbanpop.xlsx\") # a list of dfs\n\n\nRead Stata files\nTidverse haven package\nread_stata or read_dta\nR also reads labeled columns well.\n\n\nlibrary(haven)\nsugar <- setDT(read_dta(\"http://assets.datacamp.com/production/course_1478/datasets/trade.dta\"))\nglimpse(sugar)\n\nsugar$Import # label and format are also shown\n\n\nDatabase backend\n\n\n# Load the DBI package\nlibrary(DBI)\n\n# Edit dbConnect() call\ncon <- dbConnect(RMySQL::MySQL(), \n                 dbname = \"tweater\", \n                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\", \n                 port = 3306,\n                 user = \"student\",\n                 password = \"something\")\n\n# Build a vector of table names: tables\ntables <- dbListTables(con) #\"comments\" \"tweats\"   \"users\"\n\n# Display structure of tables\nstr(tables)\n\n# Import the users table from tweater: users\nusers <- dbReadTable(con, \"users\")\n\n# Get table names\ntable_names <- dbListTables(con)\n\n# Import all tables\ntables <- lapply(table_names, dbReadTable, conn = con) # list of dfs\n\n\nUse SQL Query to process on the server and retrieve the data\n\n\n# Import tweat_id column of comments where user_id is 1: elisabeth\nelisabeth = dbGetQuery(con, \n                       \"SELECT tweat_id FROM comments WHERE user_id =1\"\n                       )\n\n\nSend-Fetch-Clear combo : what dbGetQuery automatically\ndoes at once\n\n\n# Send query to the database\nres <- dbSendQuery(con, \"SELECT * FROM comments WHERE user_id > 4\")\n\n# Use dbFetch() twice\ndbFetch(res, n = 2)\ndbFetch(res)\n\n# Clear res\ndbClearResult(res)\n\n\nRead parquet\n\n\n# parquet file : fast to read, but makes it small (1.3GB csv -> 78 MB)\nlibrary(arrow)\npqdata = read_parquet('~/Dropbox (UFL)/Gunsu_Phil/data/SDC munibond data/sdc_toxic.parquet')\npqdata = pqdata[1:50,1:10] # first 50 rows \nhead(pqdata)\n\n\nDATA.TABLE\nDT[i,j,by] i : for filter and order\nrows\nj : for select/drop, add and\nsummerize columns\nby : for grouping operations\ni operations\nIMPORTANT: i operations are executed the first thing\nbefore j=, by= arguments. If this should be performed later, than should\nuse [] chain.\nWhen ordering : use DT[order(col1),] for ordering and\nsetorder for in-place operation.\nREMOVE ROWS BY REFERENCE\n\n\ndelete <- function(DT, del.idxs) {           # pls note 'del.idxs' vs. 'keep.idxs'\n  keep.idxs <- setdiff(DT[, .I], del.idxs);  # select row indexes to keep\n  cols = names(DT);\n  DT.subset <- data.table(DT[[1]][keep.idxs]); # this is the subsetted table\n  setnames(DT.subset, cols[1]);\n  for (col in cols[2:length(cols)]) {\n    DT.subset[, (col) := DT[[col]][keep.idxs]];\n    DT[, (col) := NULL];  # delete\n  }\n  return(DT.subset);\n}\n\n\nExample\n\n\niris_dt = as.data.table(iris)\ndelete(iris_dt, iris_dt[,which(Species=='setosa')])\n\n\n\n\ncboe = delete(cboe, cboe[,which(security_type==1)])\n\n\norder rows\nsetorder() function\nsetorder() and setorderv() : fast inplace\nordering function\nsetorder() : supports unquoted varaible names\n\n\ngapminder %>% head\nsetorder(gapminder, country, continent, -year) # - for descending order\ngapminder %>% head\nsetorderv(gapminder, c('country', 'continent','year'), order=c(1,1,-1)) # ascending, descending order for character vector\n\n\nBracket syntax - not by reference\n\n\ngapminder\ngapminder[order(country,continent,year)] # not inplace, new \n\n\nsetkey\nsetkey() and setkeyv()\nWhen the data.table’s key is set, then the original data.table is\nsorted by the key variable(s).\nsetting key is useful for :\n1. efficient filtering\n2. efficient joining\nThis performance is achieved binary search algorithm, which is only\navailable when it is sorted.\nTo undo setting key, setkey(data, NULL)\nWhen keyby= was used, then the data.table gets those\nvairables as its key automatically (therefore the return are also\nordered by keyby= variables)\n\n\nsetkey(gapminder,NULL) # remove key\nhaskey(gapminder) # FALSE\ngapminder[,.SD,keyby =continent] # this is a new object\ngapminder[,.SD,keyby =continent] %>% haskey\n\n\nCheck if data.table has key\n\n\nhaskey(x) # logical output\nx_key = key(x) # assign the key to object\n\nsetkeyv(y, x_key) # set key programmatically to y\n\n\nfiltering rows\nHelper functions\n%in% : if element is equal to one of several\nvalues%chin% : fast character checker instead of\n%in%%between% : numeric check if between%like% : string like somthing. accepts regex\n\n\n# 1. %in%\ngapminder[year %in% c(1952,1957)]\n# 2. %between%\ngapminder[lifeExp %between% c(28,40)]\n# 3. %chin%\ngapminder[,continent := as.character(continent)] # %chin% only works with character vector, and continent was a factor variable\ngapminder[continent %chin% c('Asia', 'Europe')]\n# 4. %like%\ngapminder[continent %like% '^[AE]'] # Asia, Africa, America, Europe\n\n\nadvanced filtering\nCJ and SJ are convenience functions to create a data.table to be used\nin i when performing a data.table ‘query’ on x.\ncount unique\nuniqueN() function for counting unique specified\nvariables.\nSimilarly DT[, .N, by = var] does similar thing (value\ncounts).\n\n\nuniqueN(mtcars, 'cyl')\n\n\nshow duplicates\nTwo approaches\nGenerating duplicated column\n\n\nstate_county[, duplicated := .N >1 , by=c('State','County')]\nstate_county[duplicated==T]\n\n\nuse duplicated(x) +\nduplicated(x, fromLast= T)\nusing base::duplicated\nUse the column name (one and only one column) for duplicate check\n\n\n# single column duplicates : input single column\naddon[duplicated(fyear) | duplicated(fyear, fromLast=T)]\n\n\nusing data.table S3 method Mutiple columns duplicated rows\n\n\n# multiple column duplicates : input the data.table\naddon[duplicated(addon[,.(fyear, gvkey)]) | duplicated(addon[,.(fyear, gvkey)], fromLast = T)]\n\n# or use plyr syntax \naddon %>% .[duplicated(.,by=c('fyear','gvkey'))|duplicated(.,by=c('fyear','gvkey'), fromLast=T)]\n\n\ndrop duplicates\nunique() function to drop duplicates.\nBy default, the first observation survives and others are dropped.\nRefer to duplicated() function.\nunique(x, incomparables=FALSE, fromLast=FALSE, by=seq_along(x),\n…)\nFor faster way, one can first set keys with setkey() and\nthen call unique() to drop duplicates by those keys.\n\n\ndim(mtcars)\nunique(mtcars) # drop by all variables duplicate\nunique(mtcars, by=c('cyl','gear')) # drop by duplicates with cyl and gear\n\n\nAs shown below, the first occurrence is marked FALSE and others are\nTRUE. Those are dropped when unique() function was\ncalled.\n\n\nduplicated(mtcars, by='cyl') \n\n\nna.omit\nna.omit() function for fast dropping rows that contain\nNAs.\nDrops any NAs with columns specified.\n\n\na <- data.table(V1 = c(NA, 1, 2),\n                V2 = c(1, NA, 2))\nna.omit(a,c('V1','V2'))\n\n\nJoin\nRight / Left (by reference) Inner joins are possible.\nTo use data.table syntax : use on in bracket to connect\ntwo data.tables\nTIP : remember to use on for two data.tables.\nby is already being used for group by\noperations!\nAddtiional arguments:\nnomatch = 0\nit drops non_matching, resulting inner join\n\nroll = {TRUE, FALSE, Inf, -Inf, ‘nearest’ } - rolling\njoin\nmult = {‘last’} - keep only the last row of join result\nRight join\n\n\n# on data.table x, work on rows that matches with data.table y\n# on keys column \nx[y, on = .(keys), nomatch= 0] # basically RIGHT JOIN x to y. to make inner join put nomatch = 0\nx[y, on = .(xcol = ycol, xcol2 = ycol2)] # when colnames are different \n\n\nAnti join\n\n\nx[!y, on = .(keys)]\n\n\nLeft join\ni. prefix to refer to columns that are located in the\nright data.table (i part of the data.table)\n\n\n# left join\nip[link_t, c('new_added_state','new_added_county') := .(i.statecode, i.countycode), on=.(GeoFIPS = FIPS)] # i.statecode and i.countycode is from link_t data\n\n\nSet keys to join : makes joins a lot more efficient and faster\n\n\nsetkey(x, colname)\nsetkey(y, colname)\nx[y] # automatically use on = colname\n\n\nSubset left join\nIn cases it may be required to perform left join on subset of left\ndata.table.\nUsing i. with fifelse does the trick.\nExample use I had before\n\n\ntemp[cpon[,.(Issue_id,Coupon)], 'Coupon' := fifelse(Coupon_type=='F', i.Coupon, NA_real_), on = .(Issue_id)] # only joined when Coupon type is 'F'\n\n\nRolling join\nRolling join is only supported for = matches. It is not\nequal to non-equi join that only closest matching will survive. Default\nrolling join is right join with forward rolling. The time on the right\ntable will be fixed, and the closest time on left table will be\nmatched.\n\n\nDT <- data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)], \n                 B = c(5, 4, 1, 9, 8, 8, 6), \n                 C = 6:12, \n                 key = \"A,B\")\nDT\n#    A B  C\n# 1: a 4  7\n# 2: a 8 10\n# 3: b 1  8\n# 4: b 5  6\n# 5: b 8 11\n# 6: c 6 12\n# 7: c 9  9\n\n\nWhen roll=TRUE, then it tries to match value less than\ninput. Same as roll=+Inf. In above example, since A\nmatches, but B does not match 4, so it matches with B = 1 instead.\nWhen roll='nearest' then it matches nearest one, which is B\n= 5 in this case.\nroll = +Inf : last observation carried forwards\n(locf)roll = -Inf : next observation carried backwards\n(nocb)\n\n\nDT[.('b',4)]\nDT[.('b',4), roll=TRUE] # matches .('b', 1) -> C value 4 is chosen to be match\nDT[.('b',4), roll= +Inf] # same\n\nDT[.('b',4), roll= -Inf] # matches .('b', 5)\nDT[.('b',4), roll='nearest'] # matches .('b', 5) in this case\n\n\nWhen mathcing limits are needed : roll = 2\n\n\nDT[.('b',4), roll=2] # search .('b', 2:4) - NA if no matches within\nDT[.('b',4), roll=-2] # search .('b', 4:6) - matches ('b', 5)\n\n\nFilling in ends : rollends = argument\nWhen ends of data.table are NAs, it can filled in by using\nrollends\nc(TRUE, TRUE) : fill above and below\n\n\nDT[.('b',-2:10)] # non matched are NAs\n#     A  B  C\n#  1: b -2 NA\n#  2: b -1 NA\n#  3: b  0 NA\n#  4: b  1  8\n#  5: b  2 NA\n#  6: b  3 NA\n#  7: b  4 NA\n#  8: b  5  6\n#  9: b  6 NA\n# 10: b  7 NA\n# 11: b  8 11\n# 12: b  9 NA\n# 13: b 10 NA\n\nDT[.('b',-2:10), roll=T] # last observation carried forward (locf), and top is NA\n#     A  B  C\n#  1: b -2 NA\n#  2: b -1 NA\n#  3: b  0 NA\n#  4: b  1  8\n#  5: b  2  8\n#  6: b  3  8\n#  7: b  4  8\n#  8: b  5  6\n#  9: b  6  6\n# 10: b  7  6\n# 11: b  8 11\n# 12: b  9 11\n# 13: b 10 11\n\nDT[.('b',-2:10), roll=T, rollends = T]# fill the top with rollends = T\n#    A  B  C\n#  1: b -2  8\n#  2: b -1  8\n#  3: b  0  8\n#  4: b  1  8\n#  5: b  2  8\n#  6: b  3  8\n#  7: b  4  8\n#  8: b  5  6\n#  9: b  6  6\n# 10: b  7  6\n# 11: b  8 11\n# 12: b  9 11\n# 13: b 10 11\n\n\nExample\nMerge\nUsing merge() function for left, right, inner(default),\nfull join\nmerge (x, y, by = c('')) : inner join\nmerge (x, y, x.by = , y.by = , all = T ) : all =T means\nfull, F means inner\nmerge (x, y, x.by = , y.by = , all.x = T ) : all.x = T\nmeans Left join\n\n\n# Example code\nmerge (x, y, x.by = , y.by = , all = TRUE ) # all = TRUE means full, FALSE means inner by default\n\n\nmerge indicator\n\n\ncrsp_mstr = merge(test1[,l:=.I],test2[,r:=.I], by.x= 'ncusip', by.y = 'CUSIP', all=T)\ncrsp_mstr[, merge_ :=fcase(is.na(l), 'right_only',\n                          is.na(r), 'left_only',\n                          default='both')]\ncrsp_mstr[,.N,merge_]\n\n\nmultiple merge\nwith Reduce()\nReduce() function reduces argument by putting outcome as\ninput until it is exhausted.\n\n\nReduce(\n  f = function(x,y){paste0(x,y,\"|\")}, # f takes two arguments\n  x = c('a','b','c') # x has three \n)\n# evaluate with a, b first and then evaluate with outcome of f(a,b) with c\n\n\nWhen merging multiple dataset repeatedly with same column, it can be\nuseful.\n\nReduce(\n  f = function(x,y){merge(x,y,by='obstime')}\n  x = list(DT1,DT2,DT3)\n)\n# merges DT1 and DT2 and then merge DT3\n\nchoose random rows\n\n\nDT[sample(.N, 3)]\n\n\nj operations\nDT[i,j,by]\ni : for filter and order rows\nj : for select,assign/drop, and\nsummerize columns\nby : for grouping operations\nMultiple Assignments :\nUse :=(col_name = max(col1), col_name2 = min(col2)) form\ninstead, or,\nuse\nc('col_name','col_name2') := .(max(col1), min(col2))\norder columns\nsetcolorder() function\nIf not all variables are explicitly mentioned, then the columns\nmentioned come front.\n\n\nsetcolorder(gapminder, c('year')) # bring year to first\n\n\nselecting columns\nbasics\nMultiple ways to selecting columns\nUse list of unquoted variable names list() or\n.()\nUse character vector (supported in later version of\ndata.table)\nWhen using name object with character vector : use ..prefix\nor with=F\nWhen := is used within j, then () for name\nobject\nWhen selecting EXCEPT columns, use - or !\nin front of character vector\n\n\n# When only one colname -> returns vector\niris[1:6,Species] # returns vector, but seldomly used...\n# using get() function : get evaluates string as variable\niris[1:6, get('Species')] # equivalent, vector returned\n\n# Notice that selecting with character vector returns data.table \niris[1:6, .(Species)] # returns dt\niris[1:6, 'Species']   # same\n\n\n\n\n# unquoted names or character vector\niris[,.(`Species`,Sepal.Length)]  # USE backtick ` ` to include column names with special characters or whitespace\niris[,c('Species','Sepal.Length')] # also returns data table \n\n# When DEselecting columns, must use character vector\niris[, !c('Species','Sepal.Length')] # deselecting Species and Sepal.Length \niris[, -c('Species','Sepal.Length')] # deselecting Species and Sepal.Length\n\n\nSelecting columns with outside named objects\n\n\n# using prefix or with=F\nmy_cols = c('Species','Sepal.Length')\niris[, my_cols, with=FALSE] # with argument to recognize character vector OR\n\n# or use dotdot (but not much generalizable)\niris[, ..my_cols] # check 'upper' environment, not in gapminder DT env\neval(parse(text = 'Species'))\n\n\n\n\n# DO NOT put list of characters\ngapminder[,.('country','Sepal.Length')] # generates new data table and assigns those values \n\n# in order to evaluate string as column reference : use get() function\ngapminder[,.(get('country'))]\ngapminder[,.(country)] # equivalent \n\n# 2nd method : using .SDcols argument to input character vector (or incides)\nmy_cols = c('country','year')\ngapminder[, .SD, .SDcols = c('country','year')] # .SDcols expects character vectors \n\n\nselect columns in sequence\nSelect columns in sequence by its name : use year:pop\ncolon notation.\n\n\nnames(gapminder) # [1] \"country\"   \"continent\" \"year\"      \"lifeExp\"   \"pop\"       \"gdpPercap\"\ngapminder[, .SD, .SDcol=year:pop]\n\n\nselect columns with name\npatterns\ngrep() is the fastest. https://stackoverflow.com/questions/30189979/select-columns-of-data-table-based-on-regex\nWhen using name patterns to select columns,\nUse grep() function ::  ,\nUse %like% and with=F argument\nUse patterns() function inside .SDcols\nargument\nUse %like% in .SDcols argument\n\n\n# 1. using %like% with = F \ngapminder[, names(gapminder) %like% '^co', with=F]\ngapminder[,!names(gapminder) %like% '^co', with=F] # negation\n\n# 2. using grep\ngapminder[, grep('^co', names(gapminder)), with=F]\ngrep('^co', names(gapminder)) # gets indices\n\n# 3. using patterns()\nmydt[, .SD, .SDcols = patterns(\"<regex pattern>\")]\n\n# 4. using %like% on .SD (slow)\ngapminder[, .SD, .SDcols= names(gapminder) %like% '^co']\ngapminder[, .SD, .SDcols= !names(gapminder) %like% '^co'] # negation\n\n\nBenchmark result is on below\n\n# Benchmarking\nn <- 100000 d\nfoo_cols <- paste0(\"foo\", 1:30)\nbig_dt <- data.table(bar = rnorm(n), baz = rnorm(n))\nbig_dt[, (foo_cols) := rnorm(n)]\nhead(big_dt)\n\n# Methods\nsubsetting <- function(dt) {\n    subset(dt, select = grep(\"bar|baz\", names(dt)))\n}\n\nusingSD <- function(dt) {\n    dt[, .SD, .SDcols = names(dt) %like% \"bar|baz\"]\n}\n\nusingWith <- function(dt) {\n    cols <- grep(\"bar|baz\", names(dt), value = TRUE)\n    dt[, cols, with = FALSE]\n}\n\nusingDotDot <- function(dt) {\n    cols <- grep(\"bar|baz\", names(dt), value = TRUE)\n    dt[, ..cols]\n}\n\nusingPatterns <- function(dt) {\n  dt[, .SD, .SDcols = patterns(\"bar|baz\")]\n}\n\nusingLike <- function(dt) {\n  dt[, names(dt) %like% \"bar|baz\", with=FALSE]\n}\n# Benchmark\nmicrobenchmark(\n    subsetting(big_dt), usingSD(big_dt), usingWith(big_dt), usingDotDot(big_dt), usingPatterns(big_dt), usingLike(big_dt),\n    times = 5000\n)\n\n# Unit: microseconds\n#                   expr     min        lq     mean    median        uq       max neval\n#     subsetting(big_dt) 370.582  977.2760 1194.875 1016.4340 1096.9285  25750.94  5000\n#        usingSD(big_dt) 554.330 1084.8530 1352.039 1133.4575 1226.9060 189905.39  5000\n#      usingWith(big_dt) 238.481  832.7505 1017.051  866.6515  927.8460  22717.83  5000\n#    usingDotDot(big_dt) 256.005  844.8770 1101.543  878.9935  936.6040 181855.43  5000\n#  usingPatterns(big_dt) 569.787 1128.0970 1411.510 1178.2895 1282.2265 177415.23  5000\n#      usingLike(big_dt) 262.868  852.5805 1059.466  887.3455  948.6665  23971.70  5000\n\ndropping columns\nDrop columns from a data.table : two methods\nDrop explicitly using := NULL\nAnti-Select\nDrop explicitly\nWhen dropping variables programatically by calling an object name, it\nshould be () wrapped.\n\n\ndrop_cols = c('continent','year')\ngapminder[, (drop_cols) := NULL ] # should be wrapped when := operation is used with\ngapminder[, drop_cols := NULL, with=F] # this is deprecated\n\n\nAnti-Select\n\n\ngapminder[,!c('pop','year')]\n\ndrop_cols = c('pop','year')\ngapminder[,!drop_cols, with=F] \ngapminder[,!..drop_cols] # same\n\n\n# appendix\ngapminder[,(drop_cols)] # paranthesis does not behave as expected\n\n\nreplace values\nBelow replaces all values inplace, which is memory efficient and\nfast.\nEmpty strings to NA, all DT\n\n\nfor (colnum in seq_along(sdc)){\n  set(sdc, i=which(sdc[[colnum]]==''), j=colnum, value=NA)\n}\n\n\nNA to -1 conversion, chosen columns\n\n\nfor (colnum in seq_along(sdc[,S_P_I_LT_MAX:Moody_s_U_LT_MAX])){\n  set(sdc, i=which(is.na(sdc[[colnum]])), j=colnum, value=-1)\n}\n\n\nNA to 0 conversion, all DT\n\n\nf_dowle2 = function(DT) {\n  for (i in names(DT))\n    DT[is.na(get(i)), (i):=0]\n}\n\n\nnafill imputation\nCurrently only numeric vectors work well\n\n\ncolnames = c('col1','col2')\nDT[, (colnames) := lapply(.SD, function(x) nafill(x, fill = mean(x, na.rm=T))),\n   .SDcols = colnames ,\n   by = .(TICKER, Date)] # filling NAs with groupby means\n\n\nUse for only simple cases\n\n\n# last observation carried forward (locf)\n\n# next obseravation carried backward (nocb)\n\n?nafill\nx = 1:10\nx[c(1:2, 5:6, 9:10)] = NA\n# [1] NA NA  3  4 NA NA  7  8 NA NA\nnafill(x, \"locf\")\n# [1] NA NA  3  4  4  4  7  8  8  8\n\ndt = data.table(v1=x, v2=shift(x)/2, v3=shift(x, -1L)/2)\n#     v1  v2  v3\n#  1: NA  NA  NA\n#  2: NA  NA 1.5\n#  3:  3  NA 2.0\n#  4:  4 1.5  NA\n#  5: NA 2.0  NA\n#  6: NA  NA 3.5\n#  7:  7  NA 4.0\n#  8:  8 3.5  NA\n#  9: NA 4.0  NA\n# 10: NA  NA  NA\n\nnafill(dt, \"nocb\") %>% as.data.table\n#     V1  V2  V3\n#  1:  3 1.5 1.5\n#  2:  3 1.5 1.5\n#  3:  3 1.5 2.0\n#  4:  4 1.5 3.5\n#  5:  7 2.0 3.5\n#  6:  7 3.5 3.5\n#  7:  7 3.5 4.0\n#  8:  8 3.5  NA\n#  9: NA 4.0  NA\n# 10: NA  NA  NA\n\nsetnafill(dt, \"locf\", cols=c(\"v2\",\"v3\"))\n\n\nzoo::na.locf, na.nocb\nassigning columns\nAssigning one column : col_name := max(col1) can be\nused.\nAssigning multiple columns : := operator cannot be\ninside of list.\nUse :=(col_name = max(col1), col_name2 = min(col2))\nor,\nuse\nc('col_name','col_name2') := .(max(col1), min(col2))\nlapply .SD assign\nTo programmatically write multiple functions with\nlapply\n\ncolnames = c('name1','name2')\n# !doesn't work\nDT[, colnames := lapply(.SD, mean), .SDcols = colnames] # does not work : LHS should be a character vector\n\n# Correct\nDT[, (colnames) := lapply(.SD, mean) .SDcols = colnames] # works\n\nrename columns\nsetnames() is data.table function that works\ninplace.\n\n\n# Ex 1\nsetnames(gapminder, old=c('year','pop'),new=c('YEAR','POP'), skip_absent=T)\n\n\nAdd suffixes by reference\n\n\nsetnames(gdi_us_w, names(gdi_us_w)[3:13], paste0(names(gdi_us_w)[3:13],'_us'))\n\n\nExample : tagging function\n\n\ntag_names <- function(DT,cols){\n  setnames(DT, old = cols, new= paste0(cols, '_mytag'), skip_absent = T)\n}\ntag_names(gapminder, c('country','invalid')) \nnames(gapminder)\n\n\ncolumns class converting\nWhen looping over, pass if it generates error\n\n\ndata[,grep('Date', names(data), value=T) := \n       lapply(.SD, \n              function(x) tryCatch(\n                as.Date(x, tryFormats = c('%Y-%m-%d','%Y-%m-%d %H:%M:%S')), error= function(e) NULL)\n              ) ,\n     .SDcols = grep('Date', names(data), value=T)]\n\n\ngrep function\ngrep(pattern='^engine', x = names(gapminder)) returns\nindecies that are matching, and argument value=T returns\nvalues instead.\n\n\ncol_pattern = grep('^co', names(gapminder), value=T)\nprint(col_pattern)\ngapminder[,.SD,.SDcol = col_pattern]\n\n\nHowever, more intuitive and beautifuly way:\n\n\ngapminder[, names(gapminder) %like% '^co']\n\n\nlapplying function calls\nlapply and sapply works slightly\ndifferently : lapply returns DT, sapply\nreturns a vector.\n\n\ngapminder[,lapply(.SD,function(x){mean(is.na(x))})] # returns DT\ngapminder[,sapply(.SD,function(x){mean(is.na(x))})] # returns vector\n\n\nlead & lag columns\nshift(x, n=1L, fill=NA, type=c(\"lag\", \"lead\", \"shift\"), give.names=FALSE)\nTo generate a column with lead or lag.\n\n\ngapminder[,.(\n  lag1_year = shift(year, n=1, type='lag'),\n  lifeExp,\n  pop), by=country]\n\n\nby operations\nenv operations\nmult opertations\nThe default is mult = all, which means all mathcing\nresults would be retrieved. Other optionss are first or\nlast.\nSpecial symbols\n.N\n.N refers to the number of rows for each group\n(if by argument was there)\nUse cases :\nValue counts : NAs would also be counted.\n\n\nmtcars[,.N, by=cyl] # calculate nrow (.N) for each unique value of cyl\n\n\nLast row number :\n\n\n# last row\ngapminder[,.SD[.N], by=continent]\n# first row\ngapminder[,.SD[1], by=continent]\n\n\n\n\ngapminder[,.SD[which.min(lifeExp)], by=continent]\n\n\nGenerating sequence numbers to the members of each group\nwith seq_len() function\n\n\ngapminder[, seq_len(.N), by=.(continent, country )]\n\n\n.SD\n.SD is useful especially when\nlapplying same function over all or specific\ncolumns(.SDcols) or\nsubgroup operations\nmentioning data.table itself within the bracket\n\n\n# Example for 1) \ngapminder[, lapply(.SD, sum), .SDcols= c('lifeExp', 'pop'), by= continent][,setnames(.SD,'lifeExp','LIFEEXP')]\n\ngapminder[,.SD, .SDcols = 'year']\n\n\n*Multiple .SDs\n\n\nd = as.data.table(iris)\nd[, \n  c(lapply(SD1, sum), lapply(SD2, mean)), \n  env = list(SD1 = as.list(c('Sepal.Length','Sepal.Width')), \n             SD2 = as.list(c('Petal.Length','Petal.Width'))),\n  by = Species]\n\nas.list(names(d))\n\n\n.I\n.I is useful to get the row number.\n.EACHI\nUsed on by part of operations.\n.GRP\nAssigns integer numbers to each groups\nAssign Chunk group\n\n\nn = 8 # number of chunks\nDT[, rep(1:n, each = round(.N/8), length.out = .N)]\n\n\n.GRPN\nAssigns the number of observations to each groups\nFunction with data.table\nget(), ()\nget() and () are very useful notations when\ndefining a function with data.table\nget() function to evaluate string to refer column\nnames.\n() to assign a column with pre-defined name object\nWhen assigning new column name, get(colname) doesn’t\nwork properly. Use (colname) instead.\n\n\n# add 10 and assign new column with suffix _plus10\n\n# ERROR \nadd10 <- function(DT, cols){ # DT is data.table, cols is character vector \n    \n  for (col in cols){\n    new_name <- paste0(col, \"_plus10\")\n    \n    \n    # Here gets Error : data.table assigns colname as new_name literally, not as intended as .._plus10 \n    DT[, new_name := get(col) + 10] # assigns column name literally as new_name (!) which is not intended behavior\n\n        # ANSWER: use () for generating new column names\n    DT[, (new_name) := get(col) + 10] # works correctly\n  }\n}\nadd10(gapminder,\"year\") # no error returned\nprint(gapminder)\n\n\nRead multiple files as\ndata.tables\nUse rbindlist() function together with\nlist.files()rbindlist is similar to rbind, yet more\nefficient and faster.\nrbindlist takes a list of data.tables and rbind it.\n\n\n# Example workflow of reading multiple csv files and concatenating them\ntable_files = c('sales2000.csv','sales2001.csv')\nlist_tables = lapply(table_files, fread) # list of data.tables\nrbindlist(list_tables)\n\n\n\n\n# fill T fills NA if columns does not match each other, idcol = specifies which file it\nrbind (x, y , fill=T, idcol='id_name' ,use.names = TRUE)\nrbindlist(some_list, use.names=\"check\", fill=FALSE, idcol=NULL) # use.names='check' => by default it warns if position does not match names\n\n\nCombinations\nfoverlaps() is fast way to implement. My use case was to\ngenerate an edgelist for network analysis.\n\n\nas.data.table(t(combn(d$id, 2))) # slow\n\n\n\n\nd <- data.table(id=as.character(paste0(\"A\", 10001:15000))) \nd[, `:=`(id1 = 1L, id2 = .I)] ## add interval columns for overlaps\nsetkey(d, id1, id2)\n\n\n\n\nfoverlaps(d, d, type='within', which=T)[xid != yid]\nolaps = foverlaps(d, d, type='within', which=T)[xid != yid]\nans = setDT(list(d$id[olaps$xid], d$id[olaps$yid]))\n\n\nSet operations\nWhen working with multiple and duplicated data.tables\nfintersect() : choose UNIQUE set of rows that are common\non two data tables. If all=TRUE, then numbers of rows\nduplicated that intersect will be also returned.funion() : choose UNIQUE set of UNION rowsfsetdiff() : choose UNIQUE set of rows that are EXCLUSIVELY\nexisting on the left\nMELT\ndata.table melting : m to 2 columns\nmelt() function\nMultiple columns (measure.vars) -> two columns (variable,\nvalue)\none with name of column(variable.name),\nand one with values(value.name)\nid.vars = columns that are kept from melting\n\n\nmelt(ebola_wide, id.vars=\"Location\", measure.vars = c(\"Week_50\", \"Week_51\"), \n     variable.name = \"period\", value.name = \"cases\"\n     ) \n\n# except for Location, \n# Week_50 - 51 columns will be stacked and other columns will be dropped, \n# generating key (variable) and value pair\n\n\n\n\n?melt\n\n\nDCAST\ndata.frame casting.\ndcast function : multiple casting is available.\nFormula expression\nLHS = Identifing variables that should stay the same\nRHS = The separator variable\nvalue.var = values that are filling in the new columns\nif multiple columns :\nSpecial variables : . and .... : no variable... : all variables not mentioned\nExample\n\n\n# The name of new columns : year (2000, 2001) are there\n# (one column) => many columns\n\n# the value that will be filling those new columns : value.var\ndcast(gdp_oceania, formula = country + continent ~ year, value.var = c(\"gdp\", \"population\"))\n# year as separator variable : 2000, 2001, 2002 ,... will be spreaded to the right hand side\n# Keep country and continent as identifier variables, cast year variable and values are gdp (generate 2000_gdp, 2001_gdp etc) and population (2000_population, 2001_population etc.)\n\n# when identifiers are NOT UNIQUE, then it tries to aggregate with some functions. \n# When identity needs to be filled, make sure the identifiers uniquely match each observations.\n\n\nCJ\nCross-Join function\n\n\na <- c(\"ABC\", \"ABC\", \"DEF\", \"GHI\")\nb <- c(\"2012-05-01\", \"2012-05-02\", \"2012-05-03\", \"2012-05-04\", \"2012-05-05\")\nCJ(a,b) # All are treated as unique\nCJ(a,b,unique=T) # only Unique ids are used\n\n\nexample\n\n\ntimes = seq(ISOdate(2011,1,1,9,00, tz='UTC'), ISOdate(2011,12,31,16,00, tz='UTC'), by = '30 min')\ntimes = times[9 <= hour(times) & hour(times) <= 16 ]\n\n\nSTRING MANIPULATION\nregular expressions\nCharacters . : . means any character\\\\. : escape, literaly dot\\\\s : one spacebar[0-9] : one numeric\\\\d : single digit[a-zA-z] : one alphabet chracter\\\\w : word character including digit (alphanumeric)[:punct:] : punctuation ., !,\n,\nMultipliers\n* : 0, 1, ore more numbers of prefix+ : 1 or more numbers of prefix? : 0 or 1 (maybe there or not)*? : makes multiplier lazy instead of greedy^ : starts with$ : ends with\nCustom pattern [] : custom pattern[^a-z] : inverse of custom pattern\nRepetitions\n\\\\w{2} : exactly two characters\\\\w{2,3} : two to three characters\\\\w{2,} : minimum 2 to unlimited max\\\\w+ : 1 or more\\\\w* : 0, 1, or more\nGroups\n() : grouping for replacement and extraction etc.(?:) : non-capturing groups\\\\1,\\\\2, : backreference = referring the\nfirst, second, … capture group\nNegations\n\\\\D : all but digits\\\\W : all but alphanumerics[^a-zA-Z] : all but alphabet\nGreedy vs Lazy\n\n\nstr_match(\"Toy story 3 in digital 3d is really good\", '.*3')  # Greedy : \"Toy story 3 in digital 3\"\nstr_match(\"Toy story 3 in digital 3d is really good\", '.*?3') # Lazy : \"\"Toy story 3\"\n\n\nRegular expressions with base r package:\ngrep : Give indices that mathes patterngrepl : Give TRUE / FALSE result list that matches\npatternsub : Substitute first matching resultgsub : Substitute ALL\n\n\nawards <- c(\"Won 1 Oscar.\",\n  \"Won 1 Oscar. Another 9 wins & 24 nominations.\",\n  \"1 win and 2 nominations.\",\n  \"2 wins & 3 nominations.\",\n  \"Nominated for 2 Golden Globes. 1 more win & 2 nominations.\",\n  \"4 wins & 1 nomination.\")\n# use sub : substitute first occurence\nsub(\".*\\\\s([0-9]+)\\\\snomination.*$\", \"\\\\1\", awards)\n\n\n\n\ntest = c('koala','panda','monkey')\ngrep('.a$', test) # gives indecies that are TRUE\ngrepl('.a$',test) # gives logical back (TRUE/FALSE)\nsub('a','A',test) # substitutes first matches\ngsub('a','A',test) # substitue ALL matches\n\n\nbase R\npaste\n\n\n# use of paste() function to attach string\npqdata = pqdata %>%\n  mutate(new_state = paste('new_', State, sep='')) # mutate\npqdata %>% select(State, new_state)\n\n\nsubstr\nextract substring of the string\n\n\nsubstr('This is the example text',1,6) \n# \"This i\"\n\n\nstrsplit\nsplits each character elements and returns list\n\n\nstrsplit(c('I_My_Me_Mine','You_Your_You_Yours'), '_')\n# [[1]]\n# [1] \"I\"    \"My\"   \"Me\"   \"Mine\"\n# \n# [[2]]\n# [1] \"You\"   \"Your\"  \"You\"   \"Yours\"\n\n\ntstrsplit\nFrom data.table package, transpose and strsplit\nsplit a column into several, and generate columns\n\n\ndt[, c(\"PX\", \"PY\") := tstrsplit(PREFIX, \"_\", fixed=TRUE)]\n#    PREFIX VALUE PX PY\n# 1:    A_B     1  A  B\n# 2:    A_C     2  A  C\n# 3:    A_D     3  A  D\n# 4:    B_A     4  B  A\n# 5:    B_C     5  B  C\n# 6:    B_D     6  B  D\n\n\ngrep, grepl\ngrep : globally search for regular expression and print\n\n\nstr_match('Payload : \"Adam, 5, 3\", headers: h', \n          pattern = '([A-Za-z]+),\\\\s(\\\\d),\\\\s(\\\\d)') # full matched and then group\ngrep('([A-Za-z]+),\\\\s(\\\\d),\\\\s(\\\\d)',\n     'Payload : \"Adam, 5, 3\", headers: h', value=T)\n\n\ngrepl : print logical of grep\nsub\nsubstitute\ngsub\nglobal substitute\nGroup Pattern extraction\nwith gsub\n\n\ngsub(\"\\\\((.*?) :: (0\\\\.[0-9]+)\\\\)\",\n     \"\\\\1 \\\\2\",\n     \"(sometext :: 0.1231313213)\")\n\n\nformat strings\n\n\n?format\n# Define the names vector\nincome <- c(72.19,1030.18,10291.93,1189192.18)\nincome_names <- c(\"Year 0\", \"Year 1\", \"Year 2\", \"Project Lifetime\")\n\n# Create pretty_income\npretty_income <- format(income, digits=2, big.mark=',')\nprint(pretty_income) # character vector now\n\n\n\n\n# Create dollar_income\ndollar_income <- paste('$', pretty_income,sep='')\nprint(dollar_income)\nclass(dollar_income)\ndollar_income\n# Create formatted_names\nformatted_names <- format(income_names, justify = 'right')\nformatted_names\n# Create rows\nrows = paste(formatted_names,dollar_income, sep='   ')\nprint(rows)\n# Write rows\nwriteLines(rows)\n\n\nstringr\nstr_c : concatenate, similar to paste\nfunction\nstr_count : number of times pattern occured on each\ncharacter elementsstr_remove : remove a pattern\nstr_trim : stripping whitespaces beginning and the\nendstr_to_upperstr_to_lower\nstr_length : calculate length of character for all\nelements, works on factors as wellstr_sub : substring access to each character elementsstr_pad : pad numbers to match the width like leading zeros\n000470\nstr_view : shows how (regex) patterns match to the\nstring vector\n\n\npqdata %>% \n  mutate(test1 = str_detect(County, 'as')) %>%  # detect if word 'as' is found within column\n  mutate(test2 = str_replace(County, 'as','AS')) %>% \n  mutate(test3 = str_remove(County,'as')) %>% \n  mutate(test4 = str_to_upper(County)) %>% \n  select(County, test1:test4)\n\n\n\n\n# str_split : returns list, for the number of split outcome is not determined with character vector input\nstr_split('Tom & Jerry & Albin' , pattern = ' & ')\nstr_split('Tom & Jerry & Albin' , pattern = ' & ', n=2) \n\nstr_split(c('Tom & Jerry', 'Tom & Jerry & Albin'), pattern = fixed(' & '))  # fixed() means do not take this as regex pattern \nstr_split(c('Tom & Jerry', 'Tom & Jerry & Albin'), pattern = ' & ', simplify = T) # returns matrix to match \n\n\n\n\n# str_view example\nx <- c(\"cat\", \"coat\", \"scotland\", \"tic toc\")\nstr_view(x, pattern = 'ca')\n\n\nstr_view()\nShows which patterns match\n\n\nstr_view(c('ab', 'aab', 'b'), '(a)?b')\n\n\nstr_match()\n: extract matched groups from a string\n\n\nstr_match(c('The tiger','The dragon'), 'tiger')\n#     [,1]   \n# [1,] \"tiger\"\n# [2,] NA     \n\nstr_match('Payload : \"Adam, 5, 3\", headers: h', \n          pattern = '([A-Za-z]+),\\\\s(\\\\d),\\\\s(\\\\d)') \n#      [,1]         [,2]   [,3] [,4]\n# [1,] \"Adam, 5, 3\" \"Adam\" \"5\"  \"3\" \n\n\nstr_detect()\n: Detect string and yield logical vector\nequivalent to grepl\n\n\nstr_detect(c('The tiger','The dragon','cat'), '^The')\n# [1]  TRUE  TRUE FALSE\ngrepl('^The', c('The tiger','The dragon','cat'))\n\nmicrobenchmark::microbenchmark(\n  str_detect(c('The tiger','The dragon','cat'), '^The'),\n  grepl('^The', c('The tiger','The dragon','cat')),\n  times= 10000L\n  )\n\n\nstr_which()\ndetect pattern and yield the value that maching\nstr_subset()\nstr_subset : detect and filter only those matches the\npattern   Equivalent to grep(pattern, x, value =T)\nstr_replace()\n: replace the first occurrence that matches a pattern to\nspecified string\nsimilar to sub\nstr_replace_all : replace all if pattern occurs multiple\ntimes\nsimilar to gsub\n\n\n# simple replacement\n\n# group match and replacement\nstr_replace(\n  \"Payload : 'Adam, 5, 3', headers: h\", \n  pattern = '([A-Za-z]+),\\\\s(\\\\d),\\\\s(\\\\d)',# matches (Adam),(5),(3)\n  replacement = '\\\\1 tried to log in \\\\2 times'\n  )\n\nsub(\n  pattern = '([A-Za-z]+),\\\\s(\\\\d),\\\\s(\\\\d)',\n  replacement = '\\\\1 tried to log in \\\\2 times',\n  x = \"Payload : 'Adam, 5, 3', headers: h\"\n)\nmicrobenchmark(\n  str_replace(\n  \"Payload : 'Adam, 5, 3', headers: h\", \n  pattern = '([A-Za-z]+),\\\\s(\\\\d),\\\\s(\\\\d)',# matches (Adam),(5),(3)\n  replacement = '\\\\1 tried to log in \\\\2 times'\n  ),\n  sub(\n  pattern = '([A-Za-z]+),\\\\s(\\\\d),\\\\s(\\\\d)',\n  replacement = '\\\\1 tried to log in \\\\2 times',\n  x = \"Payload : 'Adam, 5, 3', headers: h\"\n  )\n)\n\n\nstr_split()\n: split strings apart by separator specified\nsimilar to base strsplit\n\n\nstrsplit(c('I_My_Me_Mine','You_Your_You_Yours'), '_')\nstr_split(c('I_My_Me_Mine','You_Your_You_Yours'), '_')\n\n\nglue package\nWhen r code string needs to be evaluated within the string, glue\npackage can be useful.\n\n\nlibrary('glue')\nanimal <- \"shark\"\nverb <- \"ate\"\nnoun <- \"fish\"\nstring=\"Sammy the {animal} {verb} a {noun}.\" # when called by glue, animal, verb, noun are executed in r\nglue(string)\n\n\nCan be used to glue (paste or attach) two or more strings as\nwell.\n\n\nglue(animal,verb,noun, .sep = ' ')\n\n\nTemporary variable\n\n\nglue(\n  'Hi {username}!',\n  username = 'Matthew'\n)\n\n\nstringdist package\nString distance\n\n\nlibrary(stringdist)\nstringdist(\"saturday\", \"sunday\", method='lv')\n\n\nFinding a match\n\n\namatch(\n  x = 'sonday',\n  table = c('friday','sunday','saturday'),\n  maxDist =1,\n  method='lv'\n)\n\n\nmethods\n-lv : Levenshtein distance -dl : Damerau-Levenshtein\n-qgram : qgrams distance (n-grams) -jaccard : sum of not shared\ndivided by shared -cosine : cosine difference between two vectors\n\n\nqgrams('Honolulu','Hanolulu', q=2) # the more shared, the more close\n\n\n\n\nstringdist('Honolulu','Hanolulu', method='qgram', q=2) # sum of not shared qgrams\nstringdist('Honolulu','Hanolulu', method='jaccard') # more similar, close to zero\nstringdist('Honolulu','Hanolulu', method='cosine') # more similar, less theta ( close to zero)\n\n\nfuzzyjoin package\n\n\nlibrary(fuzzyjoin)\n\n\nstringdist_join()\n\n\nstringdist_join(\n  frame_a,\n  frame_b,\n  by = c(\"frame_a_col\" = \"frame_b_col\"),\n  max_dist = 3,\n  distance_col = \"distance\",\n  ignore_case = TRUE\n)\n\n\nCustom fuzzy matching\n\n\nfunc_1 = function(left,right){\n  stringdist(left,right) <= 5 # string distance less than 5\n}\nfunc_2 = function(left,right){\n  abs(left-right) <=3  # number distance less than 3\n}\n\nfuzzy_left_join(\n  a,\n  b,\n  by = c('a_col1' = 'b_col1',\n         'a_col2' = 'b_col2'),\n  match_fun = c( 'a_col1' = func_1, \n                 'a_col2' = func_2 )\n)\n\n\nDATE and TIME\nPOSIX\nISO Time format :\n'2021-06-02 12:00:00'\n'%Y-%m-%d %H:%M:%S'\nDate\n%Y: 4-digit year (1982)%y: 2-digit year (82)%m: 2-digit month (01)%d: 2-digit day of the month (13)\n%A: weekday (Wednesday)%a: abbreviated weekday (Wed)%B: month (January)%b: abbreviated month (Jan)\nhttps://stackoverflow.com/questions/45549449/transform-year-week-to-date-object/45587644#45587644\n%U: Week of year as a decimal\nnumber (00-53), first Sunday of the year as day 1 of week 1\n%u: Weekday as a decimal number (1–7, Monday is 1)\nTime ?week %H: hours as a\ndecimal number (00-23)%I: hours as a decimal number (01-12)%M: minutes as a decimal number%S: seconds as a decimal number%T: shorthand notation for the typical format\n%H:%M:%S%p: AM/PM indicator\nDatetime Conversions\nPOSIXct is basically an integer number, second after 1970-01-01.\nTAKEAWAY : USE as.POSIXct!\nDate objects for date, and POSIXct objects\nfor datetime. POSIXct is most preferred and universal -\ndata.table forces to use as.POSIXct instead of\nstrptime.\n\n\nas.POSIXct('2017-12-25 12:34:02') # automatic identification of ISO format\nstrptime('2017-12-25 12:34:02', format = '%Y-%m-%d %T') %>% class # POSIXlt\n\nstrftime('2017-12-25 12:34:02') # basic format is recognized\nstrftime('2020-05-18', format = '%Y-%m-%d') \n\n\nBenchmark : Integer to Date\n\n\n# integer format to date\nmicrobenchmark::microbenchmark(\n  as.Date(as.character(880904), format = '%y%m%d'),\n  strptime(as.character(880904), format = '%y%m%d'),\n  strptime(880904, format = '%y%m%d'),\n  as.POSIXct(as.character(880904), format = '%y%m%d'),\n  as.POSIXct(strptime(880904, format = '%y%m%d'))\n)\n\n# Unit: microseconds\n#                                                 expr    min      lq     mean  median      uq    max neval\n#     as.Date(as.character(880904), format = \"%y%m%d\") 11.630 12.5580 13.25445 12.9200 13.3545 24.865   100\n#    strptime(as.character(880904), format = \"%y%m%d\")  9.883 10.4540 11.16121 10.9035 11.1555 21.288   100\n#                  strptime(880904, format = \"%y%m%d\")  9.863 10.2970 11.17324 10.7460 11.0925 37.027   100\n#  as.POSIXct(as.character(880904), format = \"%y%m%d\") 26.980 27.7465 29.88938 28.4275 29.2475 80.642   100\n#      as.POSIXct(strptime(880904, format = \"%y%m%d\")) 20.241 20.7970 23.07031 21.2615 21.8460 99.860   100\n     \n# string format to date\nmicrobenchmark::microbenchmark(\n  as.Date('880904', format = '%y%m%d'),\n  strptime('880904', format = '%y%m%d'),\n  as.POSIXct('880904', format = '%y%m%d'),\n  as.POSIXct(strptime('880904', format = '%y%m%d'))\n)\n\n# Unit: microseconds\n#                                               expr    min      lq     mean  median      uq     max neval\n#               as.Date(\"880904\", format = \"%y%m%d\")  7.748  8.2855  9.84746  8.5780  8.9335  32.249   100\n#              strptime(\"880904\", format = \"%y%m%d\")  6.442  6.8290  7.56529  6.9935  7.2230  29.866   100\n#            as.POSIXct(\"880904\", format = \"%y%m%d\") 20.680 21.3785 24.82438 21.6185 22.0085  94.032   100\n#  as.POSIXct(strptime(\"880904\", format = \"%y%m%d\")) 14.944 15.5050 18.69356 15.7945 16.0575 117.387   100\n\n\nGenerate date / time variable\nas.Date, as.POSIXct,\nISOdate\n\n\nmicrobenchmark::microbenchmark(\n  as.Date('2011-01-01'),  # generates date\n  as.POSIXct('2011-01-01'), # POSIXct\n  ISOdate(11,1,1) # POSIXct\n)\n\n# Unit: microseconds\n#                      expr     min       lq      mean   median       uq     max neval\n#     as.Date(\"2011-01-01\")  32.563  37.0975  41.44812  39.8715  43.1170 143.752   100\n#  as.POSIXct(\"2011-01-01\") 104.800 114.0320 121.04771 117.5525 123.0000 242.566   100\n#         ISOdate(11, 1, 1)  61.747  67.3165  71.76522  70.1420  73.3855 129.363   100\n\n\nGenerate Time Sequence\nGenerate simple time sequence\n\n\nseq(from = ISOdate(2011,1,1), to = ISOdate(2012,1,1), by='30 min')\n\n\nGenerate day sequence with time frame\n\n\nTIMESLOTS = seq(from = ISOdate(2011,1,1,9,30), to = ISOdate(2011,1,1,16), by='30 min')\ntemp = TIMESLOTS\n\nfor (i in 1:364){\n  add1 = TIMESLOTS + as.difftime(i, units='days')\n  temp = c(temp, add1)\n}\n\n\nOnly business days (weekdays)\n\n\ndays = seq(from = ISOdate(2011,1,1), to = ISOdate(2011,1,15), by = 'day')\ndays[!weekdays(days) %chin% c('Saturday','Sunday')]\n\n\ndata.table functions\nFor extracting integer date parts : for FAST sorting , aggregating\nand joining purposes\nyear(x) : 4 digit yearquarter(x)month(x)isoweek(x)week(x)yday(x) : day of the year (1-366)wday(x) : day of the weekmday(x) : day of the month (1~31)hour(x) : hour (1~24)minute(x)second(x)\nbase R\n\n\n# Get the current date: today\ntoday = Sys.Date() # Date\n# Get the current time: now\nnow = Sys.time() # POSIXct\n# See what now looks like under the hood\ntoday;now\nunclass(today);unclass(now)\n\n\nnumeric to date\nIt’s easier to convert to character\nnumeric to character\n\n\nmicrobenchmark::microbenchmark(\n  formatC(880904),\n  as.character(880904)\n)\n# Unit: nanoseconds\n#                  expr   min      lq     mean median      uq   max neval\n#       formatC(880904) 16553 22516.0 29228.28  24649 28396.0 95061   100\n#  as.character(880904)   985  1066.5  1993.59   1671  1924.5 15206   100\n\n?as.ITime\n\n\nas.POSIXct(as.character(880904), format = \"%y%m%d\")\n\n\nas.Date\nas.Date()\n\n\n# Definition of character strings representing dates\nstr1 <- \"May 23, '96\"\nstr2 <- \"2012-03-15\"\nstr3 <- \"30/January/2006\"\n\n# Convert the strings to dates: date1, date2, date3\ndate1 <- as.Date(str1, format = \"%b %d, '%y\")\ndate2 = as.Date(str2, format = \"%Y-%m-%d\")\ndate3 = as.Date(str3, format = '%d/%B/%Y')\n\n\n# Convert dates to formatted strings\nformat(date1, \"%A\") # take out weekday (full)\nformat(date2, \"%m\") # take out month 2 digit\nformat(date3, \"%b %Y\") # take out abb. month 4 digit year\n\n\n\n\n# Definition of character strings representing times\nstr1 <- \"May 23, '96 hours:23 minutes:01 seconds:45\"\nstr2 <- \"2012-3-12 14:23:08\"\n\n# Convert the strings to POSIXct objects: time1, time2\ntime1 <- as.POSIXct(str1, format = \"%B %d, '%y hours:%H minutes:%M seconds:%S\")\ntime2 = as.POSIXct(str2, format = '%Y-%m-%d %H:%M:%S')\n\n# Convert times to formatted strings\nformat(time1, format= '%M')\nformat(time2,format='%I:%M %p')\n\n\ntryFormats : converts to date with different noticeable formats\n\n\ndata[,grep('Date', names(data), value=T) := lapply(.SD, function(x) tryCatch(as.Date(x, tryFormats = c('%Y-%m-%d','%Y-%m-%d %H:%M:%S')), error= function(e) NULL)) , .SDcols = grep('Date', names(data), value=T)]\n\n\ntime difference\ntime_length() function\n\n\n# years to maturity\nfisd[, YEAR_TO_MAT := time_length(MATURITY-DATED_DATE, unit = 'year')]\n\n\nanytime\nAutomatid date and time parser, fast!\nlubridate\nAutomatic date and time parser, easy to work with!\n\n\nlibrary('lubridate')\nx = '2020, Apr 12'\nymd(x)\n\n# Parse as date and time (with no seconds!)\nmdy_hm(\"July 15, 2012 12:56\")\n\n\n\n\n# Add a line to create whole months on air variable\nbaker_time <- baker_time  %>% \n  mutate(time_on_air = interval(first_date_appeared_uk, last_date_appeared_uk), # interval object\n         weeks_on_air = time_on_air / weeks(1), # in weeks\n         months_on_air = time_on_air %/% months(1)) # in month (modulo - round down version)\n\n\nPOSIXct : base package\nPOSIXlt : list format time\nPOSIXct : calander time - integer\nConverting integer posix to\nposixct\n\n\n# Create POSIXct dates from a hypothetical Excel dataset\n#  excelDT: integer \"days since January 1, 1900\"\n\nexcelDT[, posix := as.POSIXct(as.Date(timecol, origin = \"1900-01-01\"), tz = \"UTC\")]\n\n# Convert strings to POSIXct\n# stringDT: string dates like \"2017-01-01\"\n\nstringDT[, posix := as.POSIXct(timecol, tz = \"UTC\")]\n\n# Convert epoch seconds to POSIXct\n# epochSecondsDT: integer seconds since January 1, 1970 (\"epoch seconds\")\n\nepochSecondsDT[, posix := as.POSIXct(timecol, tz = \"UTC\", origin = \"1970-01-01\")]\n\n# Convert epoch milliseconds to POSIXct\n# epochMillisDT: integer milliseconds since January 1, 1970 (\"epoch millis\")\n\nepochMillisDT[, posix := as.POSIXct(timecol / 1000, tz = \"UTC\", origin = \"1970-01-01\")]\n\n\nGenerating posix\n\n\n# Generate a series of dates\nmarch_dates <- seq.POSIXt(from = as.POSIXct(\"2017-03-01\"), to = as.POSIXct(\"2017-03-31\"), length.out = 31)\n# Generate hourly data\nhourly_times <- seq.POSIXt(as.POSIXct(\"2017-05-01 00:00:00\"), as.POSIXct('2017-05-02 00:00:00'), length.out = 1 + 24) # to make hourly observations : 1 day 24 hours : 25 observations\n?as.POSIXct\n\n# Generate sample IoT data\niotDT <- data.table(\n    timestamp = seq.POSIXt(as.POSIXct(\"2016-04-19 00:00:00\"), as.POSIXct(\"2016-04-20 00:00:00\"), length.out = 25),\n    engine_temp = rnorm(n = 25),\n    ambient_temp = rnorm(n = 25)\n)\nhead(iotDT)\n\n\nxts package\n\n\nlibrary(xts)\n\n\n\n\n# Simulated data : 2017/6/15 from 00 to 01, 100 observations\nsome_data <- rnorm(100)\nsome_dates <- seq.POSIXt(\n  from = as.POSIXct(\"2017-06-15 00:00:00Z\", tz = \"UTC\"),\n  to = as.POSIXct(\"2017-06-15 01:00:00Z\", tz = \"UTC\"),\n  length.out = 100\n)\n\n# Make your own 'xts' object\nmyXTS <- xts(x = some_data, order.by = some_dates)\nhead(myXTS)\n# check time zone\ntzone(myXTS)\n\n\n\n# All observations after 2017-06-15 00:45:00\nfifteenXTS <- myXTS['2017-06-15 00:45:00/']\n\n# 10-minute aggregations\ntenMinuteXTS <- to.minutes10(myXTS)\nprint(tenMinuteXTS) # automatically makes Open, High, Low, Close on 10 min intervals\n# 1-minute aggregations\noneMinuteXTS <- to.minutes(myXTS)\n\n``\n\n\n# TIDYVERSE\n\n## Package : dplyr\n\nPackage `dplyr` offers intuitive verbs for data wrangling:\\\n`filter`\\\n`mutate`\\\n`arrange`\\\n`select`\\\n`summarize`\\\n\n### Filter\n\n`filter` to filter rows\n\n# filter by subset\ngapminder %>%\n  filter(State=='CA' | Yield_Amount ==100) \n\nSelect\nselect to select cols, rename cols, and reorder\ncols.\nThere are helper funcitons that can be used inside\nselect.starts_with, everything and\nmultiple renaming are those examples.\n\n\n# filter by subset\ngapminder %>%\n  filter(State=='CA' | Yield_Amount ==100) %>% \n  select(State, starts_with('Y'), ends_with('ity'),ms, everything()) %>%  # starts_with , ends_with, everything\n  select(-Yield_Amount) %>%  # drop a column\n  select(Newname_ms = ms, everything()) # rename column ms to Newname_ms\n\n\n\n\n# Multiple columns renaming \npqdata %>% \n  select(new_name_ = starts_with('Y'), everything()) # multiple renaming with prefix\n\n\nMutate\nRecode column\nTo Recode column\nFor basic recode, use recode ,\nrecode_factor\nFor multiple recode, use case_when\n\n\npqdata %>% \n  mutate(State = recode(State, 'PA' = 'PAPAPA', .default = 'Not_my_interest')) %>% # single recode\n  mutate(State2 = case_when( # multiple recoding\n    State == 'Not_my_interest' ~ 'My_interest',\n    State == 'PAPAPA' ~ 'HELLO',\n    TRUE ~ NA_character_\n    )) %>% \n  select(State,State2)\n\n\nSummarize : aggregation\nmethod\nWhen it comes to summarize (or, aggregate) the data into one row, not\nnecessarily boradcasting all the data, use summerize\nverb.\n\n\n# Calculate descriptive statistics\ncalcium %>%\n  # Group by visit and group\n  group_by(visit, group) %>%\n  # Calculate summary stats of bmd\n  summarize(mean_bmd = mean(bmd, na.rm = TRUE), # if na.rm not mentioned, then one NA will make it NA for all\n           median_bmd = median(bmd, na.rm = TRUE),\n           minimum_bmd = min(bmd, na.rm = TRUE),\n           maximum_bmd = max(bmd, na.rm = TRUE),\n           standev_bmd = sd(bmd, na.rm = TRUE),\n           num_miss = sum(is.na(bmd)),\n           n = n())\n\n\nJoining dataframes\ninner_join() for Inner join\n\n\nvotes_joined <- votes_processed %>%\n  inner_join(descriptions, by = c(\"rcid\", \"session\"))\n\n\nData concatenation (binding)\nrbindlist() for data.table solution(fastest)rbind() for data.frames, baseR solutionbind_rows() with tidyverse solution\n\nrbind(2015 = sales_2015, 2016 = sales_2016, idcol = 'year') # now 2015, 2016 will be put into idcol variable \n\nCleaning column names\nUsing janitor package\n\n\nlibrary(janitor)\npqdata %>% \n  clean_names(case = 'snake' ) %>% # All lowercase, no whitespace, snake (default)\n  clean_names(case = 'upper_camel') # upper camel style\n\n\nType (format) conversion\n5 types of format in R :\ninteger,numeric,character,\nlogical,factor\n\n\n# To numeric \npqdata$ms_yield = as.numeric(pqdata$ms_yield)\n# To factor (category)\npqdata$d_call = as.numeric(pqdata$d_call)\npqdata$d_go = as.numeric(pqdata$d_go)\n# to date\npqdata$Final_Maturity = as.Date(pqdata$Final_Maturity)\npqdata[,c(\"ms_yield\",\"d_call\",\"d_go\",\"Final_Maturity\")]\n\n\nInstead, when only number parts are needed to be parsed,\nreadr::parse_number function can be used within.\n\n\nratings3 <- ratings2  %>% \n  # Unite and change the separator\n  unite(viewers_7day, viewers_millions, viewers_decimal, sep = \"\") %>%\n  # Adapt to cast viewers as a number\n  mutate(viewers_7day = parse_number(viewers_7day))\n\n\nGGPLOT2\nggplot2()\nGrammar of graphics (Leland Wilkinson, 1999)\nData : data being plotted\n- 1. Aesthetics : the scales onto which we map our data (x, y, col, fill, size, alpha, linetype, labels, shape) \n- 2. Geometrics : the visual elements used for our data (line, point, ...) \n- 3. Facets : plotting small multiples \n- 4. Statistics : representations of data to aid understanding (adding regression line to plot) \n- 5. Coordinates : the space on which the data will be plotted (xlim, ylim, ...) \n- 6. Themes : All non-data ink\nMy samples\nError bar\ngeom_ribbon : area errorbar with two standard\ndeviation\n\n\nmonthly[!is.na(peer_sim_rig_quintile),.(mean(ln_port_mtna,na.rm=T), se_mean(ln_port_mtna)), keyby = .(caldt, peer_sim_rig_quintile)] %>% \n  ggplot(aes(x=caldt, y=V1, color=peer_sim_rig_quintile)) + \n  geom_line() +\n  geom_ribbon(aes(ymin = V1 - 2*V2, ymax = V1 + 2*V2, fill=peer_sim_rig_quintile), linetype=0,  alpha=0.2) +\n  theme_bw()\n\n\ngeom_errorbar : dynamite like\n\n\nggplot(acf_avg, aes(x = I, y = mean)) + \n  geom_col() + \n  geom_errorbar(aes(ymin = mean - 2*se, ymax = mean + 2*se), color = 'blue') +\n  theme_bw()\n\n\nLegend\nLegend position, title change\n\n\nggplot(tot, aes( x= class_name, y=N, fill=ID)) +\n  geom_bar(stat='identity', position='fill') +\n  theme_bw() +\n  ylab('Proportion') +\n  xlab('Lipper-Class') +\n  theme(legend.position = 'bottom'\n        )+\n  guides(fill=guide_legend(title=\"PeerStrength\"))\n\n\nScaling axis\nDate ticks\nScale yearly ticks to monthly ticks\n\n\nggplot(optvol[year(Year) >=2019], aes(x= Year, y= sum_volume,color = cp_flag)) + geom_line() + geom_point() +\n  scale_x_date(breaks = '1 month', date_labels = '%b-%y')\n\n\nScientific notation\n\n\nintraday_sum[TIME != '09:30'] %>% \n  ggplot(aes(x=datetime, y=sum_vols_f)) + \n  geom_line() + \n  ggtitle(\"Intraday Sum Option Trading Volume, Firms\")+\n  scale_x_datetime(date_breaks = '30 min', date_labels = \"%H:%M\")+\n  scale_y_continuous(labels = scales::scientific)\n\n\nMillion, K notation\n\n\nscale_y_continuous(labels = scales::label_number(suffix = \" M\", scale = 1e-6)) \n\n\n1 Aesthetics\nAbout aesthetics (aes)\nAesthetics and attributes are a bit confusing terms.\nAesthetics : calls variables, and are essential part\nAttributes : calls character vectors, and are non-essential.\nnote :: attributes are always called in Geom layer.\nAesthetics : elements(:= variables) that are being represented x y\nfill color size alpha : transparency level linetype labels : Text on the\nplot, or on the axis shape\nAdjusting Aesthetics\nxlim() ylim() labs() scale_*_* position(within the geom function)\nposition\nAdjusting overlapping position. How should ggplot handle\nthose?\nidentity : just do overlap if plotting on the same\ncoordination\njitter : spread dots\ndodge : plot bar next to each other\njitterdodge : spread dots and place next to each other\nstack : plot bar on top of each\nfill : normalize to 1 and fill color by proportion of each\nnudge\nFor more detailed adjustments on jitter:\nposition_jitter() function\n\n\nps = position_jitter(width=0.1)\nggplot(iris) +\n  geom_point( aes(x=Sepal.Length, y=Sepal.Width, col=Species, position = ps)) # position ='identity' is the default\n\n\nscale\nUsually have scale_*_*() naming conventions.\nscale_x_ scale_y_ scale_color_ scale_fill_\nscale_shape_ scale_linetype_ scale_size_*\nAttributes: name : name of the scale - or, only for axis lable\nchange, use labs() function. limits : breaks : expand : gap\nbetween the plot and axis labels : category name change\nScale axis and labels\nscale_x_continuous()scale_color_discrete()\n\n\nggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species)) +\n  geom_point()+\n  scale_x_continuous('X label', limits = c(2,8), breaks =seq(2, 8, 3), expand = c(0,0)) + # scale x axis and modify x label\n  scale_color_discrete('Label on Species', labels = c('SeToSa','VerSiColor','VirGiniCa')) # legend \n\n\n2 Geometrics\nAttributes : change how it looks - ONLY CALLED IN GEOM LAYER!\ncolor = ‘red’ size = 10 shape = 4 fill = ‘black’ storke = 2 alpha -\n0.3\nDot plots\ngeom_point()\ngeom_point and geom_jitter for describing\nrelationship between two variablaes.\n\n\n# overplotting problem : => make it transparent, or use hollow dots instead, or use geom_jitter\nggplot(diamonds, aes(x = carat, y = price, color= clarity)) +\n  geom_point(alpha=0.4)\n\n\ngeom_jitter() :: equivalent to\ngeom_point(position='jitter') For overlapping dots, use\ngeom_jitter() to see the density more clearly\n\n\na1 = ggplot(iris, aes(x= Sepal.Length, y= Sepal.Width, col=Species)) +\n  geom_point() + labs(title='geom_point plot')\na2 = ggplot(iris, aes(x= Sepal.Length, y= Sepal.Width, col=Species)) +\n  geom_jitter() + ggtitle('geom_jitter plot')\na1;a2\n\n\none variable scatterplot\nDummy (meaningless) y axis for scatter plot set 0 in y\n\n\nggplot(mtcars, aes(x = mpg, y = 0)) +\n  geom_jitter() +\n  ylim(c(-2,2))\n\n\ngeom_dotpoint()\ngeom_dotpoint() for clear points\n\n\nggplot(mtcars, aes(as.factor(cyl), wt, fill = as.factor(am))) +\n  geom_dotplot(stackdir = \"center\", binaxis = \"y\")\n\n\ngeom_violin()\n: For describing distribution of several categories, with more\ngraphical advantage\n\n\nggplot(iris, aes(x = Species, y = Sepal.Length)) + \n  geom_violin() + # aes(fill = group) : fill color group by variable 'group'\n  theme_bw(base_size = 16)\n\n\nBar plots\ngeom_histogram()\nIt is closely related to stat function : stat_bin()\ngeom_histogram() : For describinag distribution of one\ndimensional numeric CONTINUOUS variable\n\n\nggplot(iris, aes(Sepal.Width)) +\n  geom_histogram(binwidth = 0.1) #same as above for position = identity is the default\n\n\nDensity on y instead of count :\nUse aes(y=..density..) inside\ngeom_histogram\n\n\nggplot(iris, aes(Sepal.Width, ..density..)) +\n  geom_histogram(binwidth=0.1)\n\n\nWhen fill= aesthetic was used, default position is\n‘stack’\n\n\nggplot(iris, aes(Sepal.Width, fill=Species)) +\n  geom_histogram(aes(y=..density..), binwidth=0.1, position='stack') +\n  geom_histogram(aes(y=..density..), binwidth=0.1, position='dodge') +\n  geom_histogram(aes(y=..density..), binwidth=0.1, position='fill') +\n  geom_histogram(aes(y=..density..), binwidth=0.1, position='identity', alpha=0.7) # overlapped, it needs alpha to show overlapped regions\n\n\ngeom_bar(), geom_col()\ngeom_bar() : count of y (or density of y) in each factor x =>\nstat_count() geom_col() : actual value of y in each factor x =>\nstat_identity()\n\n\n# Change the position argument to stack\nsetDT(mtcars)\nmtcars[,`:=`(am = as.factor(am), cyl=as.factor(cyl))]\nmtcars\nggplot(mtcars, aes(x = cyl , fill=am)) +\n  geom_bar() + # by default position ='stack'\n  scale_fill_brewer(palette = \"Set1\") # color palette\n\n\n\n\n# Change the position argument to fill - proportion comparisions\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position='fill') \n\n\n\n\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position='dodge')\n\n\nFor more details on dodge position\n\n\nposn_d <- position_dodge(width = 0.2)\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = posn_d, alpha=0.6)\n\n\nCustom range of colors\n\n\nmtcars[,gear:=as.factor(gear)]\nggplot(mtcars, aes(x = gear, fill = cyl)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_brewer()\n\n\nLine Plots\nUseful for time series(continuous or discrete) plots\ngeom_line()\ngeom_line : For comparing time trend of a (several)\nvariables.\n\n\nggplot(economics, aes(date,unemploy/pop)) +\n  geom_line()\n\n\nLineplot with different groups\n\n\n# EX 1\nggplot(gdi[GeoFIPS=='00000'], aes(x= Year, y = value, color=tencode, group=tencode)) +\n  geom_line()\n\n\nFilling area\ngeom_area() :: same as geom_line(position=‘fill’)\n(?)\n\n\n# use fill argument in aes\nggplot(by_year_continent, aes(x = year, y = medianGdpPercap, fill = continent)) +\n  geom_area()\n\n\ngeom_rect()\nAdding rectangular shades on the lineplot\ngeom_rect : rectangular shape\n\n\n# recession time data\nrecess = structure(list(begin = structure(c(-31, 1400, 3652, 4199, 7486, \n11382), class = \"Date\"), end = structure(c(304, 1885, 3834, 4687, \n7729, 11627), class = \"Date\")), .Names = c(\"begin\", \"end\"), class = \"data.frame\", row.names = c(NA, \n-6L))\n\nprint(recess)\n\nggplot(economics, aes(x = date, y = unemploy/pop)) +\n  geom_rect(data = recess,\n         aes(ymax = Inf , xmin = begin, xmax = end, ymin = -Inf),\n         inherit.aes = FALSE, fill = \"red\", alpha = 0.2) +\n  geom_line()\n\n\nFrequency polygraph : similar to histogram\n\n\nggplot(mtcars, aes(mpg, color = cyl)) +\n  geom_freqpoly(binwidth = 1)\n\n\ngeom_smooth()\ngeom_smooth() : with method ‘lm’, adds regression\nline\n\n\nmtcars$cyl = as.factor(mtcars$cyl) # factor variable to be colored\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point() + \n  geom_smooth(method='lm', se=F) +\n  geom_smooth(aes(group=1), method='lm', se=F, linetype=2)\n\n\ngeom_density_ridges()\n\n\nlibrary(ggridges)\nggplot(iris, aes(x = Sepal.Length, y = Species))+\n  geom_density_ridges(bandwidth = 3.5, alpha=0.7)\n\n\nBox plots\ngeom_col : For describing distribution of several\ncategories\n\n\ngapminder_1952 <- gapminder %>%\n  filter(year == 1952)\n\n# Add a title to this graph: \"Comparing GDP per capita across continents\"\nggplot(gapminder_1952, aes(x = continent, y = gdpPercap)) +\n  geom_boxplot() +\n  scale_y_log10() +\n  ggtitle(\"Comparing GDP per capita across continents\")\n\n\nfct_rev : converts to factor vector and reverses the\norder.\n\n\n# Edit to reverse x-axis order\n\nggplot(bakers, aes(x = fct_rev(skill), fill = series_winner)) +\n  geom_bar()\n\n\nCrosshairs\ngeom_vline()\ngeom_vline() and geom_hline()\nExample : let’s draw mean of values on the axis.\n\n\niris = as.data.table(iris)\niris.summary = iris[, lapply(.SD, mean), by=Species]\n\nggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species)) +\n  geom_point()+\n  geom_vline(data=iris.summary, aes(xintercept=Sepal.Length, col=Species), linetype=2)+\n  geom_hline(data=iris.summary, aes(yintercept=Sepal.Width, col =Species), linetype=1)\n\n\ngeom_hline()\n3 Facets\nfacet_grid()\n\n\nggplot(mtcars, aes(wt, mpg)) + \n  geom_point() +\n  # Facet rows by am and columns by cyl\n  facet_grid(rows = vars(am), cols = vars(cyl))\n\n\nlabel facets\nLabeling facets : labeller argument\nThe default value is\nlabel_value: Default, displays only the value\nCommon alternatives are:\nlabel_both: Displays both the value and the variable name\nlabel_context: Displays only the values or both the values and\nvariables depending on whether multiple factors are faceted\n\n\nggplot(mtcars, aes(wt, mpg)) + \n  geom_point() +\n  # Facet rows by am and columns by cyl\n  facet_grid(cols = vars(vs, cyl) , labeller = 'label_context')\n\n\nadjusting scale/space\ncoored_fixed() fixes the coordinates along the facets.\nThis conflicts with scales ='free_x' and\n'free_y' or both, 'free'.\n\n\nggplot(msleep, aes(log(bodywt), log(brainwt))) +\n  geom_point( alpha=0.6, shape = 16) +\n  coord_fixed() + \n  facet_grid(rows = vars(vore),\n             cols = vars(conservation)\n             )\n\n\nadjusting plot sizes\n\n\n# without free space\nggplot(msleep, aes(log(bodywt), name)) +\n  geom_point() +\n  facet_grid(rows = vars(vore),\n             scales = 'free_y',\n  ) +\n  ggtitle(\"without free space\")\n# with free space (y axis)\nggplot(msleep2, aes( bodywt_log, name)) +\n  geom_point() +\n  facet_grid(rows = vars(vore),\n             scales = 'free_y',\n             space = 'free_y') +\n  ggtitle(\"with free space\")\n\n\nmargin plot\n\n\nggplot(msleep, aes(log(bodywt), log(brainwt))) +\n  geom_point( alpha=0.6, shape = 16) +\n  coord_fixed() + \n  facet_grid(rows = vars(vore),\n             cols = vars(conservation),\n             margin=TRUE, # or 'vore' / 'vonservation' for only one side of margin\n             )\n\n\nfacet_wrap()\nWhen scale of BOTH x and y axis need to be different for\nEACH plots. All else are similar to facet_grid()\n\n\nggplot(msleep, aes(log(bodywt), log(brainwt))) +\n  geom_point( alpha=0.6, shape = 16) +\n  facet_wrap(facet = vars(vore), nrow =2, scales='free' # or 'fixed'\n             )\n\n\nMultiple plots\ngridExtra package\ngrid.arrange() function\n\n\n# Example\nplot1 = qplot(mtcars$mpg)\nplot2 = qplot(mtcars$disp)\ngrid.arrange(plot1, plot2, ncol=2)\n\n\n4 Statistics\nTwo types of statistics :\ncalled within geom : geom are the wrapper\nfunction of stat.\ncalled independently\n5 Coordinates\nstarts with coord_\nxlim, ylim\nUnlike coord_cartesian, it drops observations (reduces\ndata)\ncoord_cartesian()\nZOOMing in (instead of cutting values)\nIf coord_cartesian were used\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_jitter() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  coord_cartesian(xlim=c(5,6))\n\n\nif corrd was not used : cut offs off the sample with scale\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_jitter() +\n  geom_smooth(method = \"lm\", se = FALSE) + # different fit, for it uses reduced data\n  scale_x_continuous(limits = c(5, 6))\n\n\ncoord_fixed()\nCoordination ratio\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_jitter() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  # Fix the coordinate ratio\n  coord_fixed() # 1:1 ratio as default\n\n\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_jitter() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  # Fix the coordinate ratio\n  coord_fixed(ratio=0.1) # 10:1 ratio\n\n\ncoord_expand()\nexpand sets a buffer margin around the plot, so data and axes don’t\noverlap.\nSetting expand to F draws the axes to the limits of the data.\n\n\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point(size = 2) +\n  theme_classic() +\n  # Add Cartesian coordinates with zero expansion\n  coord_cartesian(expand = F)\n\n\nclip decides whether plot elements that would lie outside the plot\npanel are displayed or ignored (“clipped”).\n\n\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point(size = 2) +\n  theme_classic() +\n  # Add Cartesian coordinates with zero expansion\n  coord_cartesian(expand = F,clip='off') # dots pop up over axis\n\n\ncoordinates vs scales\ncoordinates does not transform the data, while scales transforms the\ndata.\n\n\nmsleep = as.data.table(msleep)\n# Produce a scatter plot of brainwt vs. bodywt\nggplot(msleep, aes(bodywt, brainwt)) +\n  geom_point() +\n  ggtitle(\"Raw Values\")\n\n\n\n\n# Plot with a scale_*_*() function:\nggplot(msleep, aes(bodywt, brainwt)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  # Add a log10 x scale\n  scale_x_log10() +\n  # Add a log10 y scale\n  scale_y_log10() +\n  ggtitle(\"Scale_ functions\")\n\n\n\n\n# coords does not change the variable scale!\n# Plot with transformed coordinates\nggplot(msleep, aes(bodywt, brainwt)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  # Add a log10 coordinate transformation for x and y axes\n  coord_trans(x = \"log10\", y = \"log10\")\n\n\ncoord_flip()\nFlip x and y axis\n\n\n# Plot fcyl bars, filled by fam\nggplot(mtcars, aes(as.factor(cyl), fill = as.factor(am))) +\n  # Place bars side by side\n  geom_bar(position = \"dodge\") + \n  coord_flip()\n\n\nsec_axis()\n\n\nairquality = as.data.table(airquality)\nairquality[,`:=`(Month = str_pad(Month,2,'left',0), Day = str_pad(Day,2,'left',0))]\nairquality[,Date := as.Date(str_c(1973,Month, Day), format='%Y%m%d')]\n\n# From previous step\ny_breaks <- c(59, 68, 77, 86, 95, 104)\ny_labels <- (y_breaks - 32) * 5 / 9\nsecondary_y_axis <- sec_axis(\n  trans = identity,\n  name = \"Celsius\",\n  breaks = y_breaks,\n  labels = y_labels\n)\n\n# Update the plot\nggplot(airquality, aes(x = Date, y = Temp)) +\n  geom_line() +\n  # Add the secondary y-axis \n  scale_y_continuous(sec.axis = secondary_y_axis) +\n  labs(x = \"Date (1973)\", y = \"Fahrenheit\")\n\n\npolar coordinates\npie charts\n\n\n# Run the code, view the plot, then update it\nggplot(mtcars, aes(x = 1, fill = factor(cyl))) +\n  geom_bar() +\n  # Add a polar coordinate system\n  coord_polar(theta = \"y\") +\n  theme_void() \n\n\n\n\nggplot(mtcars, aes(x = 1, fill = as.factor(cyl))) +\n  # Reduce the bar width to 0.1\n  geom_bar(width = 0.1) +\n  coord_polar(theta = \"y\") +\n  # Add a continuous x scale from 0.5 to 1.5\n  scale_x_continuous(limits = c(0.5, 1.5))\n\n\n6 Themes\nBase plot\n\n\nbaseplot = ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) + \n  geom_violin() + # aes(fill = group) : fill color group by variable 'group'\n  theme_bw(base_size = 16)\n\n\ntheme() function\naxis panel legend plot\nline : all lines (axis) rect : all rectangular shaped ones (legend,\npanel) text : all labels and texts (x,y axis labels)\nelement_blank() : remove the item\n\n\nggplot(iris, aes(x= Sepal.Length, y= Sepal.Width, color= Species)) + \n  geom_jitter(alpha=0.6) +\n  theme(line = element_blank(),\n        text = element_blank(),\n        rect = element_blank())\n\n\naxis\naxis.text\naxis.line\n\n\nbaseplot + \n  theme(axis.line = element_line(color = \"red\", linetype = \"dashed\"))\n\n\naxis.ticks\naxis.ticks.length : length of ticks\n\n\nbaseplot +\n  theme(\n    # Set the axis tick length to 2 lines\n    axis.ticks.length = unit(2, \"cm\")\n  )\n\n\nplot\nplot.margin\n\n\nbaseplot +\n  theme(\n    # Set the plot margin to (10, 30, 50, 70) millimeters\n    plot.margin = margin(10, 30, 50, 70, \"mm\") \n  )\n\niris[,.N,Species][,.(N=sum(N)),Species]\n\n\nplot.title\nTitle position\n\n\nbaseplot + \n  ggtitle('This is the title')+\n  theme(\n  plot.title = element_text(hjust = 0.5),\n  plot.subtitle = element_text(hjust = 0.5)\n)\n\n\npanel\npanel.grid\nRemoving veritcal grid\n\n\nbaseplot +\n  theme(\n    panel.grid.major.x = element_blank()\n  )\n\n\nline\ntext\nrect\n\n\nggplot(iris, aes(x = Sepal.Length, fill = Species)) + \n  geom_histogram() + \n  theme(rect = element_rect(fill = 'grey92'))\n\n\nlegend\nlegend.position\nHere, the new value can be\n“top”, “bottom”, “left”, or “right’”: place it at that side of the\nplot. “none”: don’t draw it. c(x, y): c(0, 0) means the bottom-left and\nc(1, 1) means the top-right.\n\n\nggplot(iris, aes(x = Sepal.Length, fill = Species)) + \n  geom_histogram() + \n  theme(legend.position = 'bottom')\n\n\nlegend.key\nlegend.key.size\n\n\nbaseplot +\n  theme(legend.key.size = unit(3,'cm'))\n\n\nlegend.margin\nmargins on legend\n\n\nbaseplot + \n  theme(\n  # Set the legend margin to (20, 30, 40, 50) points\n  legend.margin = margin(20, 30, 40, 50, \"pt\")\n)\n\n\nBar plot with two variables, x on the categorical, y on the numeric\nvalue\n\n\niris = as.data.table(iris)\niris_summ = iris[, lapply(.SD, sum), by=Species]\niris_summ\n\n\nTo draw bar plot with above table : X on Species, y on\nSpeal.Legnth\n\n\nggplot(iris_summ, aes(x=Species, y=Sepal.Length)) +\n  # geom_bar() throws an error for stat='count' is the default\n  geom_bar(stat='identity') # throws an error\n\n\nBuilt-in themes\nFor more pre-built themes, ggthemes package.\n\n\ntheme_classic()\ntheme_bw()\ntheme_dark()\n\n\nupdate theme\ntheme_update() function\n\n\ntheme_recession <- theme(\n  rect = element_rect(fill = \"grey92\"),\n  legend.key = element_rect(color = NA),\n  axis.ticks = element_blank(),\n  panel.grid = element_blank(),\n  panel.grid.major.y = element_line(color = \"white\", size = 0.5, linetype = \"dotted\"),\n  axis.text = element_text(color = \"grey25\"),\n  plot.title = element_text(face = \"italic\", size = 16),\n  legend.position = c(0.6, 0.1)\n)\nbaseplot + theme_recession\ntheme_recession = theme_update(axis.line = element_line(color='red'))\nbaseplot + theme_recession\n\n\nset default theme\ntheme_set() function.\n\n\ntheme_set(theme_recession)\nbaseplot\n\n\ncolor arguments\nhttp://sape.inf.usi.ch/quick-reference/ggplot2/colour\nHelper functions\nreorder\nPlot with re-ordered variable\n\n\n# set y axis as country ordered with respect to logFoldChange\nggplot(aes(x = logFoldChange, y = reorder(country, logFoldChange))) +\n  geom_point() +\n  # add a visual anchor at x = 0\n  geom_vline(xintercept = 0)\n\n\nEXAMPLES\n\n\nrequire(gapminder)\ngapminder %>% setDT\ngm2007 = gapminder[year ==2007, .(country, continent, lifeExp)][lifeExp <= sort(lifeExp)[10] | lifeExp >= sort(lifeExp, decreasing = T)[10]]\n\n\n\n\nplt_country_vs_lifeExp = ggplot(gm2007, aes(x = lifeExp, y = country, color = lifeExp)) +\n  geom_point(size = 4) +\n  geom_segment(aes(xend = 30, yend = country), size = 2) +\n  geom_text(aes(label = round(lifeExp,1)), color = \"white\", size = 1.5) +\n  scale_x_continuous(\"\", expand = c(0,0), limits = c(30,90), position = \"top\") +\n  labs(title=\"Highest and lowest life expectancies, 2007\", caption=\"Source: gapminder\")\n  \nplt_country_vs_lifeExp\n\n\n\n\nplot2 = \n  plt_country_vs_lifeExp +\n  theme_classic() +\n  theme(\n    axis.line.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text = element_text(color = \"black\"),\n    axis.title = element_blank(),\n    legend.position = \"none\"\n    )\nplot2\n\n\n\n\nglobal_mean = 67.00742\nx_start = global_mean +4\ny_start = 5.5\n\nplot3 = plot2 + geom_vline(xintercept = global_mean, color = \"grey40\", linetype = 3) +\n  annotate(\n    \"text\",\n    x = x_start, y = y_start,\n    label = \"The\\nglobal\\naverage\",\n    vjust = 1, size = 3, color = \"grey40\"\n  )\nplot3\n\n\n\n\nx_end = global_mean\ny_end = 7.5\n# add curve\nplot4 = plot3 +\n  annotate(\n    \"curve\",\n    x = x_start, y = y_start,\n    xend = x_end, yend = y_end,\n    arrow = arrow(length = unit(0.2, \"cm\"), type = \"closed\"),\n    color = \"grey40\"\n  )\nplot4\n\n\ndynamite plot\n\n\nmtcars[, fcyl := as.factor(cyl)]\n# Plot wt vs. fcyl\nggplot(mtcars, aes(x = fcyl, y = wt)) +\n  # Add a bar summary stat of means, colored skyblue\n  stat_summary(fun = mean, geom = \"bar\", fill = \"skyblue\") +\n  # Add an errorbar summary stat std deviation limits\n  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = \"errorbar\", width = 0.1)\n\n\nREGRESSION\nBasic regression comparison between Stata & R\nIn Stata, i.var indicates factor (categorical) and\nc.var indicates continuous\n# : interaction only <=> :## : multiplication combination <=>\n*\nStata\nR\ny x1 x2\ny ~ x1 + x2\ny x1,nocons\ny ~ 0 + x1\ngen ylog = log(y) ; gen x2log = log(x2)\n; ylog x2log\nlog(y) ~ log(x2)\ngen x3 = x1 + x2 ; y x3\ny ~ I(x1 + x2)\ny i.x1\ny ~ as.factor(x1)\ny c.x1#c.x2\ny ~ x1:x2\ny c.x1##c.x2\ny ~ x1*x2\ny c.x1##i.x2\ny ~ x1*as.factor(x2)\nareg y x1 [w=x3], a(id1) cl(id1)\nfelm(y ~ x1 | id1 | 0 | id1 , data=df,\nweight = x3 )\nreghdfe y x3 (x2 = x1), a(id1) cl(id1\nid2)\nfelm(y ~ x3 | id1 | (x2 ~ x1) | id1 +\nid2, data = df)\nreghdfe y x2, a(c.x3#i.id1 id1) cl(id1\nid2)\nfelm(y ~ x2 | x3:id1 + id1, df\nRegression code example\n\n\nlmOut = glm(count ~ time, data = dat, family = 'gaussian') # OLS regression\nlr_1 = glm(goal ~ player, data = scores, family = 'poisson') # GLM (poisson) regression\nlr_2 = glm( cbind(success,fail)~x, dataWide, family='binomial') # Logistic regression\nbusLogit <- glm(Bus ~ CommuteDays, data = bus, family = binomial(link = \"logit\")) # Logit regression\nbusProbit <- glm(Bus ~ CommuteDays, data = bus, family = binomial(link = \"probit\")) # Probit regression\n\n\nLinear regression\nR’s base function lm covers pretty much.\n\n\npqdata$d_call = as.integer(pqdata$d_call) # somehow the type is not perfectly converted, so make it inteager again\npqdata$d_go = as.integer(pqdata$d_go)\nreg1 = lm(ms_yield ~ d_go+d_call+d_go:d_call, data=pqdata)\nreg2 = lm(ms_yield ~ d_go*d_call, pqdata)\nreg3 = lm(ms_yield ~ as.factor(d_go)*as.factor(d_call), pqdata)\nreg4 = lm(ms_yield ~ as.factor(d_go):as.factor(d_call), pqdata)\n\ntexreg::screenreg(list(reg1,reg2,reg3,reg4))\n\n\nFixed Effects regression\nPackage lfe gives fast estimation results with\nhigh-dimiensional fixed effects.\nGeneral expression form :\n\\[\ny  \\sim  x1 + x2 \\ \\  | \\ f1 + f2 \\ | \\ \\ (Q \\ | \\ W  \\sim  z1 + z2) \\ |\n\\ clu1 + clu2\n\\]\ny : regressand\nx1, x2 : regressor\nf1, f2 : fixed effect variables\nQ,W : instrumented variables\nz1, z2 : instrument variables\nclu1, clu2 : clustered standard error\nParts that are not used : put 0\nexample:\n\\[\ny \\sim x  \\ | \\ f1 \\ | \\ 0  \\ | \\ 0\n\\]\n\n\nmodel = felm(ms_yield ~ d_go + d_call | toxic_state_name + year | 0 | toxic_state_name + toxic_county_name, data = pqdata, cmethod=\"reghdfe\")\nsummary(model)\n\n\nLogistic Regression\n\n\nglm(y~x, data, family='binomial') # family ='binomial' uses logit function as link function\n\n\nprogramatic formula\n\n\n# our modeling effort, \n# fully parameterized!\nf <- as.formula(\n  paste(outcome, \n        paste(variables, collapse = \" + \"), \n        sep = \" ~ \"))\nprint(f)\n# mpg ~ cyl + disp + hp + carb\n\n\nExample from Munibond project\n\n\nY = 'ms_w_y_amount'\ncontrols = c('competitive_d','taxable_d','go_d','credit_enhance_d','callable_d','s_f_m_rating','unrated_d','asset_backed_d', 'ln_ms_w_yrs_to_mat','ln_ms_w_size','Insured_Amount') %>% paste(collapse = ' + ')\ndiff = c('post2001_d * HighIPW5_d')\nfixed = c('Dated_Year','statecode') %>% paste(collapse = ' + ') # year and state fixed effect\nIV = '0'\nclustered = c('Dated_Year','statecode') %>% paste(collapse = ' + ') # year and state clustered \n\nf = as.formula(\n  Y %>% paste(diff, sep=' ~ ') %>% # diff\n    paste(controls, sep=' + ') %>% # controls\n    paste(fixed, sep = ' | ') %>% # fixed\n    paste(IV, sep = ' | ') %>%  # IV reg\n    paste(clustered, sep = ' | ') # se clustering\n)\nprint(f)\n\n\nRMSE\nRoot Mean Squared Error (Residual Standard Error in R)\n\\[RMSE \\equiv\n\\sqrt{\\frac{\\Sigma_ie_i^2}{df}}\\]\n\n\nmod = lm(y~x, data) \nsqrt(sum(residuals(mod)^2) /df.residual(mod)) # is the RMSE, average difference between actual vs fit\n\n\nLeverage and Influence\nWhat about outliers?\n.hat : Leverage in the augment() tells how\nmuch the explanatory variable is far from its own average..cooksd : Cook’s distance, tells how much that point\ninfluences the overall regression model.\nmodel.matrix()\nR automatically generate matrices from formula. To see how it works\nin action, use model.matrix\n\n\n# verify how the formula works in R\nmy_size = c(1.1,1.2,1.3)\nmodel.matrix(~my_size) # interpret it as numerical variable\n\n\n\n\ncolor = c('red','blue','black')\nclass(color) # character vector : automatically generate dummies with this variable\nmodel.matrix(~color) # by alphabetical order : 'black' is the first one, and is subsumed to intercept\nmodel.matrix(~color -1) # without intercept : explicit dummies\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-09-03T05:17:11-05:00",
    "input_file": {}
  }
]
